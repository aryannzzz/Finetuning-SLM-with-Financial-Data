{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:15:45.606655Z","iopub.execute_input":"2025-11-30T04:15:45.607261Z","iopub.status.idle":"2025-11-30T04:15:45.614475Z","shell.execute_reply.started":"2025-11-30T04:15:45.607235Z","shell.execute_reply":"2025-11-30T04:15:45.613877Z"}},"outputs":[{"name":"stdout","text":"/kaggle/lib/kaggle/gcp.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## V1","metadata":{}},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Complete Kaggle Notebook\n=================================================\n\nCopy this entire file into a Kaggle notebook and run!\n\nSetup:\n1. Create new Kaggle notebook\n2. Enable GPU (T4 x2 or P100)\n3. Enable Internet\n4. Copy this entire file into a cell\n5. Run!\n\nModel will be saved to: /kaggle/working/bond_classifier_v3/\n\"\"\"\n\n# ==================== INSTALL DEPENDENCIES ====================\nimport sys\nimport subprocess\n\nprint(\"=\" * 60)\nprint(\"INSTALLING DEPENDENCIES\")\nprint(\"=\" * 60)\n\n# Install required packages\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n                      \"transformers\", \"accelerate\", \"scikit-learn\"])\n\nprint(\"✓ Dependencies installed!\\n\")\n\n\n# ==================== IMPORTS ====================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    get_cosine_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport random\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom collections import deque\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport json\nimport re\n\n\n# ==================== CHECK GPU ====================\nprint(\"=\" * 60)\nprint(\"GPU CHECK\")\nprint(\"=\" * 60)\n\nif torch.cuda.is_available():\n    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"✓ CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"⚠ WARNING: No GPU detected! Training will be very slow.\")\n    print(\"   Go to Settings → Accelerator → Select GPU\")\n\nprint()\n\n\n# ==================== CONFIGURATION ====================\nCONFIG = {\n    'base_model': 'microsoft/deberta-v3-small',\n    'num_samples_per_intent': 1000,  # 13k total\n    'augmentation_factor': 0.5,  # 50% more data\n    'batch_size': 32,\n    'num_epochs': 10,\n    'learning_rate': 2e-5,\n    'warmup_ratio': 0.1,\n    'max_length': 128,\n    'output_dir': '/kaggle/working/bond_classifier_v3',\n    'seed': 42\n}\n\n# Set seeds\nrandom.seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\ntorch.manual_seed(CONFIG['seed'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CONFIG['seed'])\n\n\n# ==================== ENUMS ====================\n\nclass QueryIntent(str, Enum):\n    BUY_RECOMMENDATION = \"buy_recommendation\"\n    SELL_RECOMMENDATION = \"sell_recommendation\"\n    PORTFOLIO_ANALYSIS = \"portfolio_analysis\"\n    REDUCE_DURATION = \"reduce_duration\"\n    INCREASE_YIELD = \"increase_yield\"\n    HEDGE_VOLATILITY = \"hedge_volatility\"\n    SECTOR_REBALANCE = \"sector_rebalance\"\n    BARBELL_STRATEGY = \"barbell_strategy\"\n    SWITCH_BONDS = \"switch_bonds\"\n    EXPLAIN_RECOMMENDATION = \"explain_recommendation\"\n    MARKET_OUTLOOK = \"market_outlook\"\n    CREDIT_ANALYSIS = \"credit_analysis\"\n    FORECAST_PRICES = \"forecast_prices\"\n\n\n# ==================== DATA GENERATION ====================\n\nclass SyntheticDataGenerator:\n    \"\"\"Generate synthetic training data\"\"\"\n    \n    def __init__(self, num_samples_per_intent: int = 1000):\n        self.num_samples_per_intent = num_samples_per_intent\n        self.templates = self._create_templates()\n        self.slot_values = self._create_slot_values()\n    \n    def _create_templates(self) -> Dict[str, List[str]]:\n        return {\n            'buy_recommendation': [\n                \"Find {rating} rated {sector} bonds with {duration} duration\",\n                \"Suggest {bond_type} bonds in {sector} sector\",\n                \"Looking for {sector} bonds with {rating} rating\",\n                \"Show me {duration} {sector} bonds rated {rating}\",\n                \"Need {investor_type} investments in {sector}\",\n                \"Want to buy {rating} {sector} bonds\",\n                \"Recommend bonds for {investment_purpose}\",\n                \"Find liquid {sector} bonds rated {rating}\",\n            ],\n            'reduce_duration': [\n                \"Reduce portfolio duration from {old_duration} to {new_duration} years\",\n                \"Lower interest rate sensitivity\",\n                \"Decrease duration while {constraint}\",\n                \"Shorten bond maturities due to {reason}\",\n                \"Need to reduce duration risk\",\n                \"Switch to shorter duration bonds\",\n            ],\n            'increase_yield': [\n                \"Improve portfolio yield from {current_yield}% to {target_yield}%\",\n                \"Find higher yielding alternatives\",\n                \"Boost returns while maintaining {constraint}\",\n                \"Need better yield than {current_yield}%\",\n                \"Increase yield without sacrificing {factor}\",\n                \"Look for yield enhancement opportunities\",\n            ],\n            'hedge_volatility': [\n                \"Hedge against {risk_type} volatility\",\n                \"Protect portfolio from rate risk\",\n                \"Build defensive position\",\n                \"Reduce sensitivity to interest rates\",\n                \"Immunize portfolio against volatility\",\n            ],\n            'sector_rebalance': [\n                \"Reduce {sector1} from {current_pct}% to {target_pct}%\",\n                \"Too much concentration in {sector}\",\n                \"Diversify away from {sector}\",\n                \"Rebalance sector exposure\",\n                \"Shift from {sector1} to {sector2}\",\n            ],\n            'portfolio_analysis': [\n                \"Analyze my bond portfolio\",\n                \"Review my holdings\",\n                \"What are the risks in my portfolio?\",\n                \"Check duration and sector exposures\",\n                \"Evaluate credit quality distribution\",\n                \"Is my portfolio well diversified?\",\n            ],\n            'switch_bonds': [\n                \"Replace {entity} with better alternative\",\n                \"Switch from {bond1} to {bond2}\",\n                \"Find substitute for {entity}\",\n                \"Swap {entity} for higher quality bond\",\n            ],\n            'sell_recommendation': [\n                \"Should I sell {entity} given {reason}?\",\n                \"Exit {entity} position\",\n                \"Which bonds should I liquidate?\",\n                \"Recommend bonds to sell\",\n            ],\n            'market_outlook': [\n                \"What's your view on bond markets?\",\n                \"How will {event} impact bonds?\",\n                \"Is this good time to invest in {sector}?\",\n                \"What's the outlook for {market_segment}?\",\n            ],\n            'credit_analysis': [\n                \"Analyze credit quality of {entity}\",\n                \"What's the default risk for {entity}?\",\n                \"Compare credit profiles of {entity1} vs {entity2}\",\n                \"Is {entity} likely to be downgraded?\",\n            ],\n            'barbell_strategy': [\n                \"Create barbell with {short_duration} and {long_duration} bonds\",\n                \"Build short-long strategy in {sector}\",\n                \"Implement barbell approach\",\n            ],\n            'forecast_prices': [\n                \"Forecast {entity} price for next {period}\",\n                \"What will {entity} trade at in {timeframe}?\",\n                \"Predict returns for {sector} bonds\",\n            ],\n            'explain_recommendation': [\n                \"Why did you suggest {action}?\",\n                \"Explain rationale behind recommendation\",\n                \"What factors led to this suggestion?\",\n            ]\n        }\n    \n    def _create_slot_values(self) -> Dict[str, List[str]]:\n        return {\n            'rating': ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB'],\n            'sector': ['Sovereign', 'PSU Energy', 'Financial', 'Corporate', \n                      'Infrastructure', 'NBFC', 'Banking'],\n            'bond_type': ['G-Sec', 'Corporate', 'PSU', 'SDL'],\n            'duration': ['short', 'medium', 'long', '1-3 year', '5-7 year'],\n            'investor_type': ['conservative', 'moderate', 'aggressive'],\n            'investment_purpose': ['wealth preservation', 'income generation'],\n            'constraint': ['maintaining yield', 'keeping liquidity'],\n            'reason': ['rate risk', 'credit concerns', 'rebalancing'],\n            'risk_type': ['interest rate', 'credit', 'duration'],\n            'entity': ['HDFC Bank', 'ICICI Bank', 'NTPC', 'PFC'],\n            'factor': ['credit quality', 'liquidity'],\n            'event': ['RBI policy', 'Budget', 'inflation data'],\n            'market_segment': ['government bonds', 'corporate bonds'],\n            'period': ['quarter', '6 months', 'year'],\n            'timeframe': ['3 months', '6 months'],\n            'action': ['buying', 'selling', 'switching']\n        }\n    \n    def generate_dataset(self) -> List[Dict]:\n        \"\"\"Generate complete dataset\"\"\"\n        dataset = []\n        \n        print(\"=\" * 60)\n        print(\"GENERATING SYNTHETIC DATA\")\n        print(\"=\" * 60)\n        \n        for intent, templates in tqdm(self.templates.items(), desc=\"Generating\"):\n            for _ in range(self.num_samples_per_intent):\n                template = random.choice(templates)\n                query = self._fill_template(template)\n                \n                sample = {\n                    'text': query,\n                    'intent': intent,\n                    'sectors': self._extract_sectors(query),\n                    'rating': self._extract_rating(query),\n                    'duration': self._extract_duration(query),\n                    'constraints': self._extract_constraints(query)\n                }\n                dataset.append(sample)\n        \n        print(f\"✓ Generated {len(dataset)} samples\\n\")\n        return dataset\n    \n    def _fill_template(self, template: str) -> str:\n        \"\"\"Fill template with random values\"\"\"\n        query = template\n        \n        for slot_type, values in self.slot_values.items():\n            placeholder = f\"{{{slot_type}}}\"\n            if placeholder in query:\n                query = query.replace(placeholder, random.choice(values))\n        \n        # Fill numeric placeholders\n        query = re.sub(r'\\{old_duration\\}', str(random.randint(5, 10)), query)\n        query = re.sub(r'\\{new_duration\\}', str(random.randint(2, 5)), query)\n        query = re.sub(r'\\{current_yield\\}', f\"{random.uniform(6.0, 7.5):.1f}\", query)\n        query = re.sub(r'\\{target_yield\\}', f\"{random.uniform(7.5, 9.0):.1f}\", query)\n        query = re.sub(r'\\{current_pct\\}', str(random.randint(30, 50)), query)\n        query = re.sub(r'\\{target_pct\\}', str(random.randint(15, 25)), query)\n        \n        # Fill remaining\n        query = re.sub(r'\\{[^}]+\\}', 'bonds', query)\n        return query\n    \n    def _extract_sectors(self, query: str) -> List[str]:\n        sectors = []\n        for sector in ['Sovereign', 'PSU Energy', 'Financial', 'Corporate']:\n            if sector.lower() in query.lower():\n                sectors.append(sector)\n        return sectors\n    \n    def _extract_rating(self, query: str) -> Optional[str]:\n        for rating in ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB']:\n            if rating in query:\n                return rating\n        return None\n    \n    def _extract_duration(self, query: str) -> str:\n        query_lower = query.lower()\n        if any(kw in query_lower for kw in ['short', '1-3']):\n            return 'short'\n        elif any(kw in query_lower for kw in ['long', '7-10']):\n            return 'long'\n        return 'medium'\n    \n    def _extract_constraints(self, query: str) -> Dict[str, bool]:\n        query_lower = query.lower()\n        return {\n            'preserve_yield': 'maintain' in query_lower and 'yield' in query_lower,\n            'maintain_liquidity': 'liquid' in query_lower,\n            'avoid_downgrades': 'credit' in query_lower,\n            'sector_diversity': 'divers' in query_lower,\n            'rating_above_aa': 'AAA' in query or 'AA' in query\n        }\n\n\nclass DataAugmenter:\n    \"\"\"Augment data with variations\"\"\"\n    \n    @staticmethod\n    def augment_dataset(dataset: List[Dict], factor: float = 0.5) -> List[Dict]:\n        \"\"\"Augment dataset\"\"\"\n        num_to_augment = int(len(dataset) * factor)\n        samples = random.sample(dataset, min(num_to_augment, len(dataset)))\n        \n        print(\"=\" * 60)\n        print(\"AUGMENTING DATA\")\n        print(\"=\" * 60)\n        \n        augmented = []\n        for sample in tqdm(samples, desc=\"Augmenting\"):\n            aug_type = random.choice(['synonym', 'insertion', 'deletion'])\n            \n            if aug_type == 'synonym':\n                augmented.append(DataAugmenter._synonym_replacement(sample))\n            elif aug_type == 'insertion':\n                augmented.append(DataAugmenter._random_insertion(sample))\n            else:\n                augmented.append(DataAugmenter._random_deletion(sample))\n        \n        print(f\"✓ Added {len(augmented)} augmented samples\\n\")\n        return dataset + augmented\n    \n    @staticmethod\n    def _synonym_replacement(sample: Dict) -> Dict:\n        synonyms = {\n            'find': ['locate', 'search for', 'look for'],\n            'bonds': ['securities', 'instruments'],\n            'high': ['elevated', 'strong'],\n            'yield': ['return', 'interest'],\n        }\n        \n        words = sample['text'].split()\n        for i, word in enumerate(words):\n            if word.lower() in synonyms and random.random() < 0.3:\n                words[i] = random.choice(synonyms[word.lower()])\n        \n        new_sample = sample.copy()\n        new_sample['text'] = ' '.join(words)\n        return new_sample\n    \n    @staticmethod\n    def _random_insertion(sample: Dict) -> Dict:\n        words = sample['text'].split()\n        if len(words) > 3:\n            pos = random.randint(0, len(words))\n            words.insert(pos, random.choice(['please', 'kindly', 'also']))\n        \n        new_sample = sample.copy()\n        new_sample['text'] = ' '.join(words)\n        return new_sample\n    \n    @staticmethod\n    def _random_deletion(sample: Dict) -> Dict:\n        words = sample['text'].split()\n        words = [w for w in words if w.lower() not in ['please', 'kindly'] or random.random() > 0.5]\n        \n        new_sample = sample.copy()\n        new_sample['text'] = ' '.join(words) if words else sample['text']\n        return new_sample\n\n\n# ==================== PYTORCH DATASET ====================\n\nclass BondQueryDataset(Dataset):\n    \"\"\"PyTorch Dataset\"\"\"\n    \n    def __init__(self, data: List[Dict], tokenizer, max_length: int = 128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        self.intent_to_id = {\n            'buy_recommendation': 0, 'sell_recommendation': 1,\n            'portfolio_analysis': 2, 'reduce_duration': 3,\n            'increase_yield': 4, 'hedge_volatility': 5,\n            'sector_rebalance': 6, 'barbell_strategy': 7,\n            'switch_bonds': 8, 'explain_recommendation': 9,\n            'market_outlook': 10, 'credit_analysis': 11,\n            'forecast_prices': 12\n        }\n        \n        self.sector_to_id = {\n            'Sovereign': 0, 'PSU Energy': 1, 'Financial': 2,\n            'Corporate': 3, 'Infrastructure': 4, 'NBFC': 5, 'Banking': 6\n        }\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        encoding = self.tokenizer(\n            sample['text'],\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        intent_label = self.intent_to_id[sample['intent']]\n        \n        sector_labels = torch.zeros(len(self.sector_to_id))\n        for sector in sample.get('sectors', []):\n            if sector in self.sector_to_id:\n                sector_labels[self.sector_to_id[sector]] = 1\n        \n        rating = sample.get('rating')\n        rating_map = {'AAA': 0, 'AA+': 1, 'AA': 2, 'A+': 3, 'A': 4, 'BBB': 5}\n        rating_label = rating_map.get(rating, 6)\n        \n        duration_map = {'short': 0, 'medium': 1, 'long': 2}\n        duration_label = duration_map.get(sample.get('duration', 'medium'), 1)\n        \n        constraints = sample.get('constraints', {})\n        constraint_labels = torch.tensor([\n            float(constraints.get('preserve_yield', False)),\n            float(constraints.get('maintain_liquidity', False)),\n            float(constraints.get('avoid_downgrades', False)),\n            float(constraints.get('sector_diversity', False)),\n            float(constraints.get('rating_above_aa', False))\n        ])\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'intent_label': torch.tensor(intent_label),\n            'sector_labels': sector_labels,\n            'rating_label': torch.tensor(rating_label),\n            'duration_label': torch.tensor(duration_label),\n            'constraint_labels': constraint_labels\n        }\n\n\n# ==================== MODEL ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Multi-task classifier\"\"\"\n    \n    def __init__(self, base_model: str = 'distilbert-base-uncased', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== LOSS ====================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n\n\nclass MultiTaskLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intent_loss_fn = FocalLoss(gamma=2.0)\n        self.sector_loss_fn = nn.BCEWithLogitsLoss()\n        self.rating_loss_fn = nn.CrossEntropyLoss()\n        self.duration_loss_fn = nn.CrossEntropyLoss()\n        self.constraint_loss_fn = nn.BCEWithLogitsLoss()\n    \n    def forward(self, outputs, labels):\n        intent_loss = self.intent_loss_fn(outputs['intent_logits'], labels['intent_label'])\n        sector_loss = self.sector_loss_fn(outputs['sector_logits'], labels['sector_labels'])\n        rating_loss = self.rating_loss_fn(outputs['rating_logits'], labels['rating_label'])\n        duration_loss = self.duration_loss_fn(outputs['duration_logits'], labels['duration_label'])\n        constraint_loss = self.constraint_loss_fn(outputs['constraint_logits'], labels['constraint_labels'])\n        \n        total = intent_loss + 0.5*sector_loss + 0.3*rating_loss + 0.3*duration_loss + 0.4*constraint_loss\n        \n        return {'total': total, 'intent': intent_loss}\n\n\n# ==================== TRAINER ====================\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, device, output_dir):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.best_acc = 0.0\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            \n            labels = {k: v.to(self.device) for k, v in batch.items() \n                     if k not in ['input_ids', 'attention_mask']}\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(input_ids, attention_mask)\n            loss_dict = self.criterion(outputs, labels)\n            loss = loss_dict['total']\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            preds = outputs['intent_logits'].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels['intent_label'].cpu().numpy())\n            \n            pbar.set_postfix({'loss': f'{loss.item():.4f}', \n                            'acc': f'{accuracy_score(all_labels[-len(preds):], preds):.3f}'})\n        \n        return total_loss / len(self.train_loader)\n    \n    def evaluate(self):\n        self.model.eval()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc='Evaluating'):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = {k: v.to(self.device) for k, v in batch.items() \n                         if k not in ['input_ids', 'attention_mask']}\n                \n                outputs = self.model(input_ids, attention_mask)\n                loss_dict = self.criterion(outputs, labels)\n                total_loss += loss_dict['total'].item()\n                \n                preds = outputs['intent_logits'].argmax(dim=-1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels['intent_label'].cpu().numpy())\n        \n        return {\n            'loss': total_loss / len(self.val_loader),\n            'accuracy': accuracy_score(all_labels, all_preds),\n            'f1_macro': f1_score(all_labels, all_preds, average='macro')\n        }\n    \n    def save_checkpoint(self, epoch, metrics):\n        if metrics['accuracy'] > self.best_acc:\n            self.best_acc = metrics['accuracy']\n            torch.save(self.model.state_dict(), self.output_dir / 'pytorch_model.bin')\n            print(f\"✓ New best model saved (acc: {metrics['accuracy']:.4f})\")\n    \n    def train(self, num_epochs):\n        print(\"=\" * 60)\n        print(\"TRAINING\")\n        print(\"=\" * 60)\n        \n        for epoch in range(1, num_epochs + 1):\n            train_loss = self.train_epoch(epoch)\n            print(f\"\\nEpoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n            \n            val_metrics = self.evaluate()\n            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Val F1: {val_metrics['f1_macro']:.4f}\\n\")\n            \n            self.save_checkpoint(epoch, val_metrics)\n        \n        print(f\"Training complete! Best accuracy: {self.best_acc:.4f}\\n\")\n\n\n# ==================== MAIN TRAINING FUNCTION ====================\n\ndef train_model():\n    \"\"\"Main training function\"\"\"\n    \n    # Generate data\n    generator = SyntheticDataGenerator(CONFIG['num_samples_per_intent'])\n    dataset = generator.generate_dataset()\n    \n    # Augment\n    dataset = DataAugmenter.augment_dataset(dataset, CONFIG['augmentation_factor'])\n    print(f\"Total dataset size: {len(dataset)} samples\\n\")\n    \n    # Split\n    print(\"=\" * 60)\n    print(\"SPLITTING DATA\")\n    print(\"=\" * 60)\n    intents = [s['intent'] for s in dataset]\n    train_data, temp_data = train_test_split(dataset, test_size=0.3, stratify=intents, random_state=42)\n    temp_intents = [s['intent'] for s in temp_data]\n    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_intents, random_state=42)\n    \n    print(f\"Train: {len(train_data)}\")\n    print(f\"Val: {len(val_data)}\")\n    print(f\"Test: {len(test_data)}\\n\")\n    \n    # Create datasets\n    print(\"=\" * 60)\n    print(\"LOADING MODEL\")\n    print(\"=\" * 60)\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG['base_model'])\n    \n    train_dataset = BondQueryDataset(train_data, tokenizer, CONFIG['max_length'])\n    val_dataset = BondQueryDataset(val_data, tokenizer, CONFIG['max_length'])\n    test_dataset = BondQueryDataset(test_data, tokenizer, CONFIG['max_length'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'])\n    \n    # Model\n    model = ProductionBondClassifier(CONFIG['base_model'])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    print(f\"✓ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n    print(f\"✓ Device: {device}\\n\")\n    \n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n    num_training_steps = len(train_loader) * CONFIG['num_epochs']\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * CONFIG['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    criterion = MultiTaskLoss()\n    \n    # Train\n    trainer = Trainer(model, train_loader, val_loader, optimizer, scheduler, \n                     criterion, device, CONFIG['output_dir'])\n    trainer.train(CONFIG['num_epochs'])\n    \n    # Save tokenizer\n    tokenizer.save_pretrained(CONFIG['output_dir'])\n    \n    # Test\n    print(\"=\" * 60)\n    print(\"FINAL TEST\")\n    print(\"=\" * 60)\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc='Testing'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask)\n            preds = outputs['intent_logits'].argmax(dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['intent_label'].numpy())\n    \n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    print(f\"\\n✓ Test Accuracy: {test_acc:.4f}\")\n    print(f\"✓ Test F1 Macro: {test_f1:.4f}\\n\")\n    \n    print(\"=\" * 60)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"\\n✓ Model saved to: {CONFIG['output_dir']}\")\n    print(f\"✓ Files: pytorch_model.bin, tokenizer files\")\n    print(f\"\\nTo download:\")\n    print(f\"  1. Go to Output tab\")\n    print(f\"  2. Download 'bond_classifier_v3' folder\")\n    print(f\"  3. Use locally with production code\")\n\n\n# ==================== RUN ====================\n\nif __name__ == '__main__':\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:46:58.959922Z","iopub.execute_input":"2025-11-30T08:46:58.960371Z","iopub.status.idle":"2025-11-30T08:47:38.610478Z","shell.execute_reply.started":"2025-11-30T08:46:58.960345Z","shell.execute_reply":"2025-11-30T08:47:38.609531Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"============================================================\nINSTALLING DEPENDENCIES\n============================================================\n✓ Dependencies installed!\n\n============================================================\nGPU CHECK\n============================================================\n✓ GPU detected: Tesla T4\n✓ GPU memory: 15.83 GB\n✓ CUDA version: 12.4\n\n============================================================\nGENERATING SYNTHETIC DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf6aa01042d4a789ba55939ef0a873a"}},"metadata":{}},{"name":"stdout","text":"✓ Generated 13000 samples\n\n============================================================\nAUGMENTING DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmenting:   0%|          | 0/6500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4388790d5ead447c8a2dd24a415b72ba"}},"metadata":{}},{"name":"stdout","text":"✓ Added 6500 augmented samples\n\nTotal dataset size: 19500 samples\n\n============================================================\nSPLITTING DATA\n============================================================\nTrain: 13650\nVal: 2925\nTest: 2925\n\n============================================================\nLOADING MODEL\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Model loaded: 142,205,987 parameters\n✓ Device: cuda\n\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd2920e88d147e492d5c1f8e73634b0"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1610399588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_47/1610399588.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    694\u001b[0m     trainer = Trainer(model, train_loader, val_loader, optimizer, scheduler, \n\u001b[1;32m    695\u001b[0m                      criterion, device, CONFIG['output_dir'])\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# Save tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1610399588.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1610399588.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intent_logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Kaggle Inference Notebook\n==================================================\n\nUse this after training to test your model in Kaggle.\n\nSetup:\n1. Attach the output from training notebook as a dataset\n2. Copy this file into a new Kaggle notebook\n3. Run to test the classifier!\n\"\"\"\n\n# ==================== INSTALL & IMPORTS ====================\nimport sys\nimport subprocess\n\nprint(\"Installing dependencies...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\"])\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom collections import deque\nimport numpy as np\n\nprint(\"✓ Imports complete!\\n\")\n\n\n# ==================== MODEL ARCHITECTURE ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Same architecture as training\"\"\"\n    \n    def __init__(self, base_model: str = 'distilbert-base-uncased', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== INFERENCE CLASS ====================\n\nclass QueryIntent(str, Enum):\n    BUY_RECOMMENDATION = \"buy_recommendation\"\n    SELL_RECOMMENDATION = \"sell_recommendation\"\n    PORTFOLIO_ANALYSIS = \"portfolio_analysis\"\n    REDUCE_DURATION = \"reduce_duration\"\n    INCREASE_YIELD = \"increase_yield\"\n    HEDGE_VOLATILITY = \"hedge_volatility\"\n    SECTOR_REBALANCE = \"sector_rebalance\"\n    BARBELL_STRATEGY = \"barbell_strategy\"\n    SWITCH_BONDS = \"switch_bonds\"\n    EXPLAIN_RECOMMENDATION = \"explain_recommendation\"\n    MARKET_OUTLOOK = \"market_outlook\"\n    CREDIT_ANALYSIS = \"credit_analysis\"\n    FORECAST_PRICES = \"forecast_prices\"\n\n\n@dataclass\nclass ClassificationResult:\n    intent: str\n    confidence: float\n    filters: Dict[str, Any]\n    constraints: Dict[str, bool]\n\n\nclass BondClassifier:\n    \"\"\"Production classifier\"\"\"\n    \n    def __init__(self, model_path: str):\n        \"\"\"\n        Load model from Kaggle path\n        \n        Args:\n            model_path: Path like '/kaggle/input/bond-classifier-v3/bond_classifier_v3'\n        \"\"\"\n        print(f\"Loading model from: {model_path}\")\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        # Load model\n        self.model = ProductionBondClassifier()\n        state_dict = torch.load(f\"{model_path}/pytorch_model.bin\", map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n        \n        # Label mappings\n        self.intent_names = [\n            'buy_recommendation', 'sell_recommendation', 'portfolio_analysis',\n            'reduce_duration', 'increase_yield', 'hedge_volatility',\n            'sector_rebalance', 'barbell_strategy', 'switch_bonds',\n            'explain_recommendation', 'market_outlook', 'credit_analysis',\n            'forecast_prices'\n        ]\n        \n        self.sector_names = ['Sovereign', 'PSU Energy', 'Financial', 'Corporate', \n                            'Infrastructure', 'NBFC', 'Banking']\n        self.rating_names = ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB', 'Unrated']\n        self.duration_names = ['short', 'medium', 'long']\n        \n        print(\"✓ Model loaded successfully!\\n\")\n    \n    def classify(self, query: str, num_samples: int = 5) -> ClassificationResult:\n        \"\"\"\n        Classify query with uncertainty estimation\n        \n        Args:\n            query: User query string\n            num_samples: Number of MC dropout samples\n        \"\"\"\n        # Tokenize\n        inputs = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        ).to(self.device)\n        \n        # MC Dropout for uncertainty\n        self.model.train()  # Enable dropout\n        intent_predictions = []\n        \n        with torch.no_grad():\n            for _ in range(num_samples):\n                outputs = self.model(**inputs)\n                intent_probs = F.softmax(outputs['intent_logits'], dim=-1)\n                intent_predictions.append(intent_probs.cpu())\n        \n        # Calculate statistics\n        intent_mean = torch.stack(intent_predictions).mean(dim=0)[0]\n        confidence = intent_mean.max().item()\n        predicted_idx = intent_mean.argmax().item()\n        \n        # Get final outputs\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            \n            # Extract all predictions\n            sectors = self._extract_sectors(outputs['sector_logits'])\n            rating = self._extract_rating(outputs['rating_logits'])\n            duration = self._extract_duration(outputs['duration_logits'])\n            constraints = self._extract_constraints(outputs['constraint_logits'])\n        \n        return ClassificationResult(\n            intent=self.intent_names[predicted_idx],\n            confidence=confidence,\n            filters={\n                'sectors': sectors,\n                'min_rating': rating,\n                'duration_preference': duration\n            },\n            constraints=constraints\n        )\n    \n    def _extract_sectors(self, logits: torch.Tensor) -> List[str]:\n        probs = torch.sigmoid(logits[0])\n        indices = (probs > 0.5).nonzero(as_tuple=True)[0]\n        return [self.sector_names[i] for i in indices]\n    \n    def _extract_rating(self, logits: torch.Tensor) -> Optional[str]:\n        idx = logits[0].argmax().item()\n        return self.rating_names[idx] if idx < 6 else None\n    \n    def _extract_duration(self, logits: torch.Tensor) -> str:\n        idx = logits[0].argmax().item()\n        return self.duration_names[idx]\n    \n    def _extract_constraints(self, logits: torch.Tensor) -> Dict[str, bool]:\n        probs = torch.sigmoid(logits[0])\n        return {\n            'preserve_yield': probs[0].item() > 0.5,\n            'maintain_liquidity': probs[1].item() > 0.5,\n            'avoid_downgrades': probs[2].item() > 0.5,\n            'sector_diversity': probs[3].item() > 0.5,\n            'rating_above_aa': probs[4].item() > 0.5\n        }\n    \n    def batch_classify(self, queries: List[str]) -> List[ClassificationResult]:\n        \"\"\"Classify multiple queries\"\"\"\n        return [self.classify(q) for q in queries]\n\n\n# ==================== USAGE EXAMPLE ====================\n\ndef test_classifier():\n    \"\"\"Test the classifier with sample queries\"\"\"\n    \n    # TODO: Update this path to match your Kaggle dataset\n    # After training, add the output as a dataset, then use that path\n    MODEL_PATH = '/kaggle/working/bond_classifier_v3'\n    \n    # Or if testing in the same notebook after training:\n    # MODEL_PATH = '/kaggle/working/bond_classifier_v3'\n    \n    print(\"=\" * 60)\n    print(\"LOADING CLASSIFIER\")\n    print(\"=\" * 60)\n    \n    try:\n        classifier = BondClassifier(MODEL_PATH)\n    except Exception as e:\n        print(f\"❌ Error loading model: {e}\")\n        print(\"\\nMake sure to:\")\n        print(\"1. Add training output as a dataset\")\n        print(\"2. Update MODEL_PATH above\")\n        return\n    \n    print(\"=\" * 60)\n    print(\"TESTING QUERIES\")\n    print(\"=\" * 60)\n    \n    # Test queries\n    test_queries = [\n        # --- Category 1: Ambiguous Buy/Sell/Strategy ---\n        \"Should I shift my SDL holdings into shorter PSU bonds or just hold?\",\n        \"I want to reduce rate risk but I don’t want my yield to fall. What should I do?\",\n        \"Is it better to exit long-duration bonds and move into AA corporates?\",\n        \"Would switching from NTPC 2035 to REC 2030 improve my return?\",\n        \"Which reduces risk more: selling perpetuals or adding short-term G-Secs?\",\n    \n        # --- Category 2: Disguised intents ---\n        \"My yield looks weak lately… what should I change?\",\n        \"These long papers are stressing me out—what’s the safest move?\",\n        \"My portfolio feels too boring. Suggest something with more kick.\",\n        \"The curve is flattening; should I reposition?\",\n        \"My advisor said I’m too exposed. What adjustments should I consider?\",\n    \n        # --- Category 3: Multi-intent queries ---\n        \"Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\",\n        \"Reduce my duration and give alternatives with at least 7.5% yield.\",\n        \"Analyze my portfolio and tell me which low-yield bonds I should switch.\",\n        \"Before suggesting buys, what’s your outlook on corporate spreads?\",\n        \"If I shift to shorter bonds, how will my yield be impacted?\",\n    \n        # --- Category 4: Tricky phrasing for buy intent ---\n        \"Is NTPC 2033 attractive at current spreads?\",\n        \"Are AA PSU bonds offering a good entry point?\",\n        \"Is this a good time to accumulate SDLs?\",\n        \"Should I start adding exposure to long maturities?\",\n        \"Are there better alternatives to ICICI 2029 with similar risk?\",\n    \n        # --- Category 5: Risk management intents ---\n        \"The market seems jumpy; how do I protect my portfolio?\",\n        \"If RBI hikes unexpectedly, which holdings will get hit the most?\",\n        \"Rate volatility worries me—how do I reduce the impact?\",\n        \"How can I stabilize P&L swings in my portfolio?\",\n        \"Should I rebalance sectors before the credit cycle weakens?\",\n    \n        # --- Category 6: Very short hard queries ---\n        \"Duration too high?\",\n        \"Better yield ideas?\",\n        \"Switch or hold?\",\n        \"Cut risk?\",\n        \"Add PSU?\",\n    \n        # --- Category 7: Contradictory constraints ---\n        \"Cut duration but don’t let yield drop below 7.6%.\",\n        \"I want high yield but without taking credit risk.\",\n        \"Reduce risk but avoid selling anything.\",\n        \"Switch out of low-yield bonds but keep duration same.\",\n        \"Increase return without increasing duration or credit risk.\",\n    \n        # --- Category 8: Multi-sentence queries ---\n        \"My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\",\n        \"My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\",\n        \"I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\",\n        \"I expect inflation to cool. Should I increase duration a bit?\",\n        \"Markets feel stable. Should I rotate sectors or focus on yield first?\",\n    \n        # --- Category 9: Credit-analysis queries ---\n        \"How is the credit quality of PFC right now?\",\n        \"Is REC fundamentally strong enough for long-term holding?\",\n        \"What’s the default risk on NTPC?\",\n        \"Should I worry about credit spreads widening?\",\n        \"Are AA- names safe in this environment?\",\n    \n        # --- Category 10: Forecast / outlook queries ---\n        \"Where do you see G-Sec yields in six months?\",\n        \"If rates fall by 50 bps, what happens to long-duration PSU bonds?\",\n        \"Will corporate spreads tighten this year?\",\n        \"Predict the movement of the 10-year benchmark.\",\n        \"How will a Fed cut affect Indian bond yields?\",\n    ]\n\n    \n    for query in test_queries:\n        result = classifier.classify(query)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Query: {query}\")\n        print(f\"{'='*60}\")\n        print(f\"Intent: {result.intent}\")\n        print(f\"Confidence: {result.confidence:.3f}\")\n        print(f\"Sectors: {result.filters['sectors']}\")\n        print(f\"Rating: {result.filters['min_rating']}\")\n        print(f\"Duration: {result.filters['duration_preference']}\")\n        print(f\"Constraints: {result.constraints}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"TESTING COMPLETE!\")\n    print(\"=\" * 60)\n\n\n# ==================== INTERACTIVE TESTING ====================\n\ndef interactive_test():\n    \"\"\"Interactive testing - type your own queries\"\"\"\n    \n    MODEL_PATH = '/kaggle/working/bond_classifier_v3'\n    # Or: MODEL_PATH = '/kaggle/working/bond_classifier_v3'\n    \n    print(\"Loading classifier...\")\n    classifier = BondClassifier(MODEL_PATH)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"INTERACTIVE MODE\")\n    print(\"=\" * 60)\n    print(\"Type your bond queries below. Type 'quit' to exit.\\n\")\n    \n    while True:\n        query = input(\"Query: \").strip()\n        \n        if query.lower() in ['quit', 'exit', 'q']:\n            break\n        \n        if not query:\n            continue\n        \n        result = classifier.classify(query)\n        \n        print(f\"\\n  → Intent: {result.intent}\")\n        print(f\"  → Confidence: {result.confidence:.3f}\")\n        print(f\"  → Filters: {result.filters}\")\n        print()\n\n\n# ==================== BATCH TESTING ====================\n\ndef batch_test_from_list():\n    \"\"\"Test a large batch of queries\"\"\"\n    \n    MODEL_PATH = '/kaggle/input/bond-classifier-output/bond_classifier_v3'\n    \n    print(\"Loading classifier...\")\n    classifier = BondClassifier(MODEL_PATH)\n    \n    # Large test set\n    # queries = [\n    #     \"Find government bonds\",\n    #     \"High yield corporate bonds\",\n    #     \"Reduce duration risk\",\n    #     \"Increase portfolio yield\",\n    #     \"Diversify sector exposure\",\n    #     \"Analyze my portfolio\",\n    #     \"Switch from ICICI to HDFC bonds\",\n    #     \"Should I sell in this market?\",\n    #     \"What's the credit risk of NTPC?\",\n    #     \"Forecast bond prices\",\n    #     \"Explain your recommendation\",\n    #     \"Create barbell strategy\",\n    #     \"Hedge against inflation\",\n    # ]\n    queries = [\n        # --- Category 1: Ambiguous Buy/Sell/Strategy ---\n        \"Should I shift my SDL holdings into shorter PSU bonds or just hold?\",\n        \"I want to reduce rate risk but I don’t want my yield to fall. What should I do?\",\n        \"Is it better to exit long-duration bonds and move into AA corporates?\",\n        \"Would switching from NTPC 2035 to REC 2030 improve my return?\",\n        \"Which reduces risk more: selling perpetuals or adding short-term G-Secs?\",\n    \n        # --- Category 2: Disguised intents ---\n        \"My yield looks weak lately… what should I change?\",\n        \"These long papers are stressing me out—what’s the safest move?\",\n        \"My portfolio feels too boring. Suggest something with more kick.\",\n        \"The curve is flattening; should I reposition?\",\n        \"My advisor said I’m too exposed. What adjustments should I consider?\",\n    \n        # --- Category 3: Multi-intent queries ---\n        \"Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\",\n        \"Reduce my duration and give alternatives with at least 7.5% yield.\",\n        \"Analyze my portfolio and tell me which low-yield bonds I should switch.\",\n        \"Before suggesting buys, what’s your outlook on corporate spreads?\",\n        \"If I shift to shorter bonds, how will my yield be impacted?\",\n    \n        # --- Category 4: Tricky phrasing for buy intent ---\n        \"Is NTPC 2033 attractive at current spreads?\",\n        \"Are AA PSU bonds offering a good entry point?\",\n        \"Is this a good time to accumulate SDLs?\",\n        \"Should I start adding exposure to long maturities?\",\n        \"Are there better alternatives to ICICI 2029 with similar risk?\",\n    \n        # --- Category 5: Risk management intents ---\n        \"The market seems jumpy; how do I protect my portfolio?\",\n        \"If RBI hikes unexpectedly, which holdings will get hit the most?\",\n        \"Rate volatility worries me—how do I reduce the impact?\",\n        \"How can I stabilize P&L swings in my portfolio?\",\n        \"Should I rebalance sectors before the credit cycle weakens?\",\n    \n        # --- Category 6: Very short hard queries ---\n        \"Duration too high?\",\n        \"Better yield ideas?\",\n        \"Switch or hold?\",\n        \"Cut risk?\",\n        \"Add PSU?\",\n    \n        # --- Category 7: Contradictory constraints ---\n        \"Cut duration but don’t let yield drop below 7.6%.\",\n        \"I want high yield but without taking credit risk.\",\n        \"Reduce risk but avoid selling anything.\",\n        \"Switch out of low-yield bonds but keep duration same.\",\n        \"Increase return without increasing duration or credit risk.\",\n    \n        # --- Category 8: Multi-sentence queries ---\n        \"My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\",\n        \"My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\",\n        \"I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\",\n        \"I expect inflation to cool. Should I increase duration a bit?\",\n        \"Markets feel stable. Should I rotate sectors or focus on yield first?\",\n    \n        # --- Category 9: Credit-analysis queries ---\n        \"How is the credit quality of PFC right now?\",\n        \"Is REC fundamentally strong enough for long-term holding?\",\n        \"What’s the default risk on NTPC?\",\n        \"Should I worry about credit spreads widening?\",\n        \"Are AA- names safe in this environment?\",\n    \n        # --- Category 10: Forecast / outlook queries ---\n        \"Where do you see G-Sec yields in six months?\",\n        \"If rates fall by 50 bps, what happens to long-duration PSU bonds?\",\n        \"Will corporate spreads tighten this year?\",\n        \"Predict the movement of the 10-year benchmark.\",\n        \"How will a Fed cut affect Indian bond yields?\",\n    ]\n\n\n    \n    print(f\"\\nTesting {len(queries)} queries...\\n\")\n    \n    results = classifier.batch_classify(queries)\n    \n    # Show summary\n    intent_counts = {}\n    for result in results:\n        intent_counts[result.intent] = intent_counts.get(result.intent, 0) + 1\n    \n    print(\"=\" * 60)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\" * 60)\n    \n    for intent, count in sorted(intent_counts.items(), key=lambda x: -x[1]):\n        print(f\"{intent:30s}: {count:2d} queries\")\n    \n    avg_confidence = sum(r.confidence for r in results) / len(results)\n    print(f\"\\nAverage confidence: {avg_confidence:.3f}\")\n\n\n# ==================== RUN ====================\n\nif __name__ == '__main__':\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BOND QUERY CLASSIFIER - INFERENCE\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    # Choose test mode:\n    \n    # Option 1: Automated test with sample queries\n    test_classifier()\n    \n    # Option 2: Interactive mode (uncomment to use)\n    # interactive_test()\n    \n    # Option 3: Batch testing (uncomment to use)\n    # batch_test_from_list()","metadata":{"trusted":true,"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Installing dependencies...\n","output_type":"stream"},{"name":"stderr","text":"ERROR: Operation cancelled by user\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1857581459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Installing dependencies...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \"\"\"\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"## V2","metadata":{}},{"cell_type":"code","source":"# ===== GEMINI SETUP CELL =====\n!pip install -q google-genai\n\nimport os\n\n# Paste your key here (or use Kaggle secrets and set env there)\nos.environ[\"GEMINI_API_KEY\"] = \"AIzaSyD-9r0N-YGJfL_hZR2elzwpc6m4f6PPb2Y\"\n\nprint(\"GEMINI_API_KEY set:\", \"GEMINI_API_KEY\" in os.environ)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:00:12.163923Z","iopub.execute_input":"2025-11-30T10:00:12.164326Z","iopub.status.idle":"2025-11-30T10:00:15.487977Z","shell.execute_reply.started":"2025-11-30T10:00:12.164298Z","shell.execute_reply":"2025-11-30T10:00:15.487215Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"GEMINI_API_KEY set: True\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Complete Kaggle Notebook (with LLM data generation)\n===========================================================================\n\nSetup:\n1. Create new Kaggle notebook\n2. Enable GPU (T4 x2 or P100)\n3. Enable Internet\n4. Set OPENAI_API_KEY in Kaggle secrets / environment\n5. Copy this entire file into a cell\n6. Run!\n\nModel will be saved to: /kaggle/working/bond_classifier_v3/\n\"\"\"\n\n# ==================== INSTALL DEPENDENCIES ====================\nimport sys\nimport subprocess\n\nprint(\"=\" * 60)\nprint(\"INSTALLING DEPENDENCIES\")\nprint(\"=\" * 60)\n\n# Install required packages (added openai)\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"transformers\", \"accelerate\", \"scikit-learn\", \"openai\"\n])\n\nprint(\"✓ Dependencies installed!\\n\")\n\n\n# ==================== IMPORTS ====================\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    get_cosine_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport random\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom collections import deque\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport json\nimport re\n\nfrom google import genai  # Gemini Flash client\n\n# ==================== CHECK GPU ====================\nprint(\"=\" * 60)\nprint(\"GPU CHECK\")\nprint(\"=\" * 60)\n\nif torch.cuda.is_available():\n    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"✓ CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"⚠ WARNING: No GPU detected! Training will be very slow.\")\n    print(\"   Go to Settings → Accelerator → Select GPU\")\n\nprint()\n\n\n# ==================== CONFIGURATION ====================\nCONFIG = {\n    'base_model': 'microsoft/deberta-v3-small',\n    'num_samples_per_intent': 800,   # synthetic per intent (you can tune)\n    'augmentation_factor': 0.5,      # 50% more data\n\n    # --- NEW: LLM data generation config ---\n    'use_llm_data': True,            # toggle this to disable LLM data\n    'llm_samples_per_intent': 200,   # how many LLM examples per intent\n    'llm_model': 'gemini-2.0-flash',  # or 'gemini-1.5-flash' if you prefer\n\n    'llm_dataset_cache': '/kaggle/working/llm_intent_dataset.jsonl',\n    'reuse_cached_llm_data': True,   # if cache exists, reuse instead of regenerating\n\n    'batch_size': 32,\n    'num_epochs': 4,\n    'learning_rate': 2e-5,\n    'warmup_ratio': 0.1,\n    'max_length': 128,\n    'output_dir': '/kaggle/working/bond_classifier_v3',\n    'seed': 42\n}\n\n# Set seeds\nrandom.seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\ntorch.manual_seed(CONFIG['seed'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CONFIG['seed'])\n\n\n# ==================== ENUMS ====================\n\nclass QueryIntent(str, Enum):\n    BUY_RECOMMENDATION = \"buy_recommendation\"\n    SELL_RECOMMENDATION = \"sell_recommendation\"\n    PORTFOLIO_ANALYSIS = \"portfolio_analysis\"\n    REDUCE_DURATION = \"reduce_duration\"\n    INCREASE_YIELD = \"increase_yield\"\n    HEDGE_VOLATILITY = \"hedge_volatility\"\n    SECTOR_REBALANCE = \"sector_rebalance\"\n    BARBELL_STRATEGY = \"barbell_strategy\"\n    SWITCH_BONDS = \"switch_bonds\"\n    EXPLAIN_RECOMMENDATION = \"explain_recommendation\"\n    MARKET_OUTLOOK = \"market_outlook\"\n    CREDIT_ANALYSIS = \"credit_analysis\"\n    FORECAST_PRICES = \"forecast_prices\"\n\n\n# ==================== SYNTHETIC DATA GENERATION ====================\n\nclass SyntheticDataGenerator:\n    \"\"\"Generate synthetic training data from templates (unchanged)\"\"\"\n    \n    def __init__(self, num_samples_per_intent: int = 1000):\n        self.num_samples_per_intent = num_samples_per_intent\n        self.templates = self._create_templates()\n        self.slot_values = self._create_slot_values()\n    \n    def _create_templates(self) -> Dict[str, List[str]]:\n        return {\n            'buy_recommendation': [\n                \"Find {rating} rated {sector} bonds with {duration} duration\",\n                \"Suggest {bond_type} bonds in {sector} sector\",\n                \"Looking for {sector} bonds with {rating} rating\",\n                \"Show me {duration} {sector} bonds rated {rating}\",\n                \"Need {investor_type} investments in {sector}\",\n                \"Want to buy {rating} {sector} bonds\",\n                \"Recommend bonds for {investment_purpose}\",\n                \"Find liquid {sector} bonds rated {rating}\",\n            ],\n            'reduce_duration': [\n                \"Reduce portfolio duration from {old_duration} to {new_duration} years\",\n                \"Lower interest rate sensitivity\",\n                \"Decrease duration while {constraint}\",\n                \"Shorten bond maturities due to {reason}\",\n                \"Need to reduce duration risk\",\n                \"Switch to shorter duration bonds\",\n            ],\n            'increase_yield': [\n                \"Improve portfolio yield from {current_yield}% to {target_yield}%\",\n                \"Find higher yielding alternatives\",\n                \"Boost returns while maintaining {constraint}\",\n                \"Need better yield than {current_yield}%\",\n                \"Increase yield without sacrificing {factor}\",\n                \"Look for yield enhancement opportunities\",\n            ],\n            'hedge_volatility': [\n                \"Hedge against {risk_type} volatility\",\n                \"Protect portfolio from rate risk\",\n                \"Build defensive position\",\n                \"Reduce sensitivity to interest rates\",\n                \"Immunize portfolio against volatility\",\n            ],\n            'sector_rebalance': [\n                \"Reduce {sector1} from {current_pct}% to {target_pct}%\",\n                \"Too much concentration in {sector}\",\n                \"Diversify away from {sector}\",\n                \"Rebalance sector exposure\",\n                \"Shift from {sector1} to {sector2}\",\n            ],\n            'portfolio_analysis': [\n                \"Analyze my bond portfolio\",\n                \"Review my holdings\",\n                \"What are the risks in my portfolio?\",\n                \"Check duration and sector exposures\",\n                \"Evaluate credit quality distribution\",\n                \"Is my portfolio well diversified?\",\n            ],\n            'switch_bonds': [\n                \"Replace {entity} with better alternative\",\n                \"Switch from {bond1} to {bond2}\",\n                \"Find substitute for {entity}\",\n                \"Swap {entity} for higher quality bond\",\n            ],\n            'sell_recommendation': [\n                \"Should I sell {entity} given {reason}?\",\n                \"Exit {entity} position\",\n                \"Which bonds should I liquidate?\",\n                \"Recommend bonds to sell\",\n            ],\n            'market_outlook': [\n                \"What's your view on bond markets?\",\n                \"How will {event} impact bonds?\",\n                \"Is this good time to invest in {sector}?\",\n                \"What's the outlook for {market_segment}?\",\n            ],\n            'credit_analysis': [\n                \"Analyze credit quality of {entity}\",\n                \"What's the default risk for {entity}?\",\n                \"Compare credit profiles of {entity1} vs {entity2}\",\n                \"Is {entity} likely to be downgraded?\",\n            ],\n            'barbell_strategy': [\n                \"Create barbell with {short_duration} and {long_duration} bonds\",\n                \"Build short-long strategy in {sector}\",\n                \"Implement barbell approach\",\n            ],\n            'forecast_prices': [\n                \"Forecast {entity} price for next {period}\",\n                \"What will {entity} trade at in {timeframe}?\",\n                \"Predict returns for {sector} bonds\",\n            ],\n            'explain_recommendation': [\n                \"Why did you suggest {action}?\",\n                \"Explain rationale behind recommendation\",\n                \"What factors led to this suggestion?\",\n            ]\n        }\n    \n    def _create_slot_values(self) -> Dict[str, List[str]]:\n        return {\n            'rating': ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB'],\n            'sector': ['Sovereign', 'PSU Energy', 'Financial', 'Corporate', \n                      'Infrastructure', 'NBFC', 'Banking'],\n            'bond_type': ['G-Sec', 'Corporate', 'PSU', 'SDL'],\n            'duration': ['short', 'medium', 'long', '1-3 year', '5-7 year'],\n            'investor_type': ['conservative', 'moderate', 'aggressive'],\n            'investment_purpose': ['wealth preservation', 'income generation'],\n            'constraint': ['maintaining yield', 'keeping liquidity'],\n            'reason': ['rate risk', 'credit concerns', 'rebalancing'],\n            'risk_type': ['interest rate', 'credit', 'duration'],\n            'entity': ['HDFC Bank', 'ICICI Bank', 'NTPC', 'PFC'],\n            'factor': ['credit quality', 'liquidity'],\n            'event': ['RBI policy', 'Budget', 'inflation data'],\n            'market_segment': ['government bonds', 'corporate bonds'],\n            'period': ['quarter', '6 months', 'year'],\n            'timeframe': ['3 months', '6 months'],\n            'action': ['buying', 'selling', 'switching']\n        }\n    \n    def generate_dataset(self) -> List[Dict]:\n        \"\"\"Generate complete dataset\"\"\"\n        dataset = []\n        \n        print(\"=\" * 60)\n        print(\"GENERATING SYNTHETIC DATA\")\n        print(\"=\" * 60)\n        \n        for intent, templates in tqdm(self.templates.items(), desc=\"Generating synthetic\"):\n            for _ in range(self.num_samples_per_intent):\n                template = random.choice(templates)\n                query = self._fill_template(template)\n                \n                sample = {\n                    'text': query,\n                    'intent': intent,\n                    'sectors': self._extract_sectors(query),\n                    'rating': self._extract_rating(query),\n                    'duration': self._extract_duration(query),\n                    'constraints': self._extract_constraints(query)\n                }\n                dataset.append(sample)\n        \n        print(f\"✓ Generated {len(dataset)} synthetic samples\\n\")\n        return dataset\n    \n    def _fill_template(self, template: str) -> str:\n        \"\"\"Fill template with random values\"\"\"\n        query = template\n        \n        for slot_type, values in self.slot_values.items():\n            placeholder = f\"{{{slot_type}}}\"\n            if placeholder in query:\n                query = query.replace(placeholder, random.choice(values))\n        \n        # Fill numeric placeholders\n        query = re.sub(r'\\{old_duration\\}', str(random.randint(5, 10)), query)\n        query = re.sub(r'\\{new_duration\\}', str(random.randint(2, 5)), query)\n        query = re.sub(r'\\{current_yield\\}', f\"{random.uniform(6.0, 7.5):.1f}\", query)\n        query = re.sub(r'\\{target_yield\\}', f\"{random.uniform(7.5, 9.0):.1f}\", query)\n        query = re.sub(r'\\{current_pct\\}', str(random.randint(30, 50)), query)\n        query = re.sub(r'\\{target_pct\\}', str(random.randint(15, 25)), query)\n        \n        # Fill remaining\n        query = re.sub(r'\\{[^}]+\\}', 'bonds', query)\n        return query\n    \n    def _extract_sectors(self, query: str) -> List[str]:\n        sectors = []\n        for sector in ['Sovereign', 'PSU Energy', 'Financial', 'Corporate']:\n            if sector.lower() in query.lower():\n                sectors.append(sector)\n        return sectors\n    \n    def _extract_rating(self, query: str) -> Optional[str]:\n        for rating in ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB']:\n            if rating in query:\n                return rating\n        return None\n    \n    def _extract_duration(self, query: str) -> str:\n        query_lower = query.lower()\n        if any(kw in query_lower for kw in ['short', '1-3']):\n            return 'short'\n        elif any(kw in query_lower for kw in ['long', '7-10']):\n            return 'long'\n        return 'medium'\n    \n    def _extract_constraints(self, query: str) -> Dict[str, bool]:\n        query_lower = query.lower()\n        return {\n            'preserve_yield': 'maintain' in query_lower and 'yield' in query_lower,\n            'maintain_liquidity': 'liquid' in query_lower,\n            'avoid_downgrades': 'credit' in query_lower,\n            'sector_diversity': 'divers' in query_lower,\n            'rating_above_aa': 'AAA' in query or 'AA' in query\n        }\n\n\n# ==================== DATA AUGMENTATION ====================\n\nclass DataAugmenter:\n    \"\"\"Augment data with cheap text variations\"\"\"\n    \n    @staticmethod\n    def augment_dataset(dataset: List[Dict], factor: float = 0.5) -> List[Dict]:\n        \"\"\"Augment dataset\"\"\"\n        num_to_augment = int(len(dataset) * factor)\n        samples = random.sample(dataset, min(num_to_augment, len(dataset)))\n        \n        print(\"=\" * 60)\n        print(\"AUGMENTING DATA\")\n        print(\"=\" * 60)\n        \n        augmented = []\n        for sample in tqdm(samples, desc=\"Augmenting\"):\n            aug_type = random.choice(['synonym', 'insertion', 'deletion'])\n            \n            if aug_type == 'synonym':\n                augmented.append(DataAugmenter._synonym_replacement(sample))\n            elif aug_type == 'insertion':\n                augmented.append(DataAugmenter._random_insertion(sample))\n            else:\n                augmented.append(DataAugmenter._random_deletion(sample))\n        \n        print(f\"✓ Added {len(augmented)} augmented samples\\n\")\n        return dataset + augmented\n    \n    @staticmethod\n    def _synonym_replacement(sample: Dict) -> Dict:\n        synonyms = {\n            'find': ['locate', 'search for', 'look for'],\n            'bonds': ['securities', 'instruments'],\n            'high': ['elevated', 'strong'],\n            'yield': ['return', 'interest'],\n        }\n        \n        words = sample['text'].split()\n        for i, word in enumerate(words):\n            if word.lower() in synonyms and random.random() < 0.3:\n                words[i] = random.choice(synonyms[word.lower()])\n        \n        new_sample = sample.copy()\n        new_sample['text'] = ' '.join(words)\n        return new_sample\n    \n    @staticmethod\n    def _random_insertion(sample: Dict) -> Dict:\n        words = sample['text'].split()\n        if len(words) > 3:\n            pos = random.randint(0, len(words))\n            words.insert(pos, random.choice(['please', 'kindly', 'also']))\n        \n        new_sample = sample.copy()\n        new_sample['text'] = ' '.join(words)\n        return new_sample\n    \n    @staticmethod\n    def _random_deletion(sample: Dict) -> Dict:\n        words = sample['text'].split()\n        words = [w for w in words if w.lower() not in ['please', 'kindly'] or random.random() > 0.5]\n        \n        new_sample = sample.copy()\n        new_sample['text'] = ' '.join(words) if words else sample['text']\n        return new_sample\n\n\n# ==================== NEW: LLM DATA GENERATOR ====================\n\nclass LLMIntentDataGenerator:\n    \"\"\"\n    Use a Gemini Flash model to generate realistic labelled queries per intent.\n    Requires GEMINI_API_KEY to be set in environment.\n    \"\"\"\n    def __init__(\n        self,\n        model_name: str,\n        samples_per_intent: int = 200,\n        max_per_call: int = 20\n    ):\n        api_key = os.environ.get(\"GEMINI_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\n                \"GEMINI_API_KEY not set. \"\n                \"Set it in an earlier cell with os.environ['GEMINI_API_KEY'] = '...'\"\n            )\n\n        # Gemini client\n        self.client = genai.Client(api_key=api_key)\n        self.model_name = model_name\n        self.samples_per_intent = samples_per_intent\n        self.max_per_call = max_per_call\n\n        # For metadata validation\n        self.allowed_sectors = [\n            'Sovereign', 'PSU Energy', 'Financial',\n            'Corporate', 'Infrastructure', 'NBFC', 'Banking'\n        ]\n        self.allowed_ratings = ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB']\n        self.allowed_durations = ['short', 'medium', 'long']\n\n        # Human description per intent to guide the LLM\n        self.intent_descriptions = {\n            \"buy_recommendation\": \"User wants recommendations of which bonds to buy, often with preferences about sector, rating, duration or yield.\",\n            \"sell_recommendation\": \"User wants to know what to sell or whether to exit certain bonds or positions.\",\n            \"portfolio_analysis\": \"User wants analysis or diagnosis of their current portfolio, exposures, risks and diversification.\",\n            \"reduce_duration\": \"User wants to reduce interest rate risk or duration of their bond portfolio.\",\n            \"increase_yield\": \"User wants to increase portfolio yield or returns, often by moving to higher-yielding bonds.\",\n            \"hedge_volatility\": \"User wants to hedge or reduce the impact of rate or price volatility.\",\n            \"sector_rebalance\": \"User wants to rebalance sector allocation or reduce concentration in particular sectors.\",\n            \"barbell_strategy\": \"User wants a barbell strategy (mix of short and long duration bonds).\",\n            \"switch_bonds\": \"User wants to switch from one bond or issuer to another similar bond.\",\n            \"explain_recommendation\": \"User wants explanation or rationale for a previous recommendation or trade idea.\",\n            \"market_outlook\": \"User wants outlook on bond markets, yields, interest rates or spreads.\",\n            \"credit_analysis\": \"User wants analysis of credit quality, default risk or rating outlook.\",\n            \"forecast_prices\": \"User wants explicit forecasts of future bond prices or yields.\"\n        }\n\n    def generate_dataset(self) -> List[Dict]:\n        \"\"\"Generate LLM-labelled dataset across all intents.\"\"\"\n        print(\"=\" * 60)\n        print(\"GENERATING LLM DATA (Gemini)\")\n        print(\"=\" * 60)\n\n        all_samples: List[Dict] = []\n\n        for intent in QueryIntent:\n            intent_name = intent.value\n            needed = self.samples_per_intent\n            print(f\"\\n→ Generating LLM data for intent: {intent_name} ({needed} samples)\")\n            while needed > 0:\n                batch_size = min(self.max_per_call, needed)\n                batch = self._generate_batch_for_intent(intent_name, batch_size)\n                all_samples.extend(batch)\n                needed -= len(batch)\n                print(\n                    f\"  Collected {self.samples_per_intent - needed}/\"\n                    f\"{self.samples_per_intent} for {intent_name}\"\n                )\n\n        print(f\"\\n✓ Generated {len(all_samples)} LLM-labelled samples\\n\")\n        return all_samples\n\n    def _generate_batch_for_intent(self, intent_name: str, batch_size: int) -> List[Dict]:\n        \"\"\"Ask Gemini for a small batch of JSONL examples for a single intent.\"\"\"\n        description = self.intent_descriptions[intent_name]\n\n        system_msg = (\n            \"You are an expert bond investment assistant generating synthetic \"\n            \"training data for supervised learning. You must produce realistic \"\n            \"user queries for a bond assistant.\"\n        )\n\n        user_prompt = f\"\"\"\nGenerate {batch_size} diverse user queries whose PRIMARY intent is: \"{intent_name}\".\n\nIntent description:\n{description}\n\nFor EACH example, output ONE JSON object on its own line (JSON Lines format).\nDo NOT wrap the objects in an array. Do NOT add any commentary or code fences.\n\nEach JSON object MUST have exactly these keys:\n- \"text\": (string) realistic user query in natural language\n- \"intent\": (string) MUST be exactly \"{intent_name}\"\n- \"sectors\": (array of strings) zero or more from this list ONLY:\n  [\"Sovereign\", \"PSU Energy\", \"Financial\", \"Corporate\", \"Infrastructure\", \"NBFC\", \"Banking\"]\n- \"rating\": (string or null) one of [\"AAA\",\"AA+\",\"AA\",\"A+\",\"A\",\"BBB\"] or null\n- \"duration\": (string) one of [\"short\",\"medium\",\"long\"]\n- \"constraints\": (object) with boolean fields:\n  {{\n    \"preserve_yield\": <true/false>,\n    \"maintain_liquidity\": <true/false>,\n    \"avoid_downgrades\": <true/false>,\n    \"sector_diversity\": <true/false>,\n    \"rating_above_aa\": <true/false>\n  }}\n\nGuidelines:\n- Mix short queries and longer multi-sentence queries.\n- Vary tone: retail investor, professional PM, brief, descriptive, etc.\n- Do NOT leak these guidelines into the \"text\" field.\n- Keep everything realistic and specific to bonds (NOT equities or crypto).\n\"\"\"\n\n        # Simple single-text prompt is enough for Gemini\n        full_prompt = system_msg + \"\\n\\n\" + user_prompt\n\n        resp = self.client.models.generate_content(\n            model=self.model_name,\n            contents=full_prompt,\n        )\n\n        content = resp.text or \"\"\n        return self._parse_jsonl(content, intent_name)\n\n    def _parse_jsonl(self, raw: str, intent_name: str) -> List[Dict]:\n        \"\"\"Parse JSON-lines text returned by the LLM.\"\"\"\n        samples: List[Dict] = []\n        for line in raw.splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            # Strip bullets if Gemini gets chatty\n            if line.startswith(\"```\"):\n                continue\n            if line.startswith(\"-\"):\n                line = line.lstrip(\"-\").strip()\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue  # skip bad lines\n\n            sample = self._normalize_record(obj, intent_name)\n            if sample is not None:\n                samples.append(sample)\n        return samples\n\n    def _normalize_record(self, obj: Dict[str, Any], intent_name: str) -> Optional[Dict]:\n        \"\"\"Clean up and validate a single sample from the LLM.\"\"\"\n        text = str(obj.get(\"text\", \"\")).strip()\n        if not text:\n            return None\n\n        # Force our canonical intent name\n        intent = obj.get(\"intent\", intent_name)\n        if intent != intent_name:\n            intent = intent_name\n\n        # Sectors\n        raw_sectors = obj.get(\"sectors\") or []\n        sectors: List[str] = []\n        if isinstance(raw_sectors, list):\n            for s in raw_sectors:\n                s_str = str(s).strip()\n                if s_str in self.allowed_sectors and s_str not in sectors:\n                    sectors.append(s_str)\n\n        # Rating\n        rating = obj.get(\"rating\")\n        if rating is None:\n            rating_norm = None\n        else:\n            rating_str = str(rating).strip()\n            rating_norm = rating_str if rating_str in self.allowed_ratings else None\n\n        # Duration\n        duration = str(obj.get(\"duration\", \"medium\")).strip().lower()\n        if duration not in self.allowed_durations:\n            duration = \"medium\"\n\n        # Constraints\n        raw_constraints = obj.get(\"constraints\") or {}\n        constraints_defaults = {\n            \"preserve_yield\": False,\n            \"maintain_liquidity\": False,\n            \"avoid_downgrades\": False,\n            \"sector_diversity\": False,\n            \"rating_above_aa\": False,\n        }\n        if isinstance(raw_constraints, dict):\n            for k in list(constraints_defaults.keys()):\n                if k in raw_constraints:\n                    constraints_defaults[k] = bool(raw_constraints[k])\n\n        # If rating is high, we can set rating_above_aa = True\n        if rating_norm in (\"AAA\", \"AA+\", \"AA\"):\n            constraints_defaults[\"rating_above_aa\"] = True\n\n        return {\n            \"text\": text,\n            \"intent\": intent,\n            \"sectors\": sectors,\n            \"rating\": rating_norm,\n            \"duration\": duration,\n            \"constraints\": constraints_defaults,\n        }\n\n\n# ==================== PYTORCH DATASET ====================\n\nclass BondQueryDataset(Dataset):\n    \"\"\"PyTorch Dataset\"\"\"\n    \n    def __init__(self, data: List[Dict], tokenizer, max_length: int = 128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        self.intent_to_id = {\n            'buy_recommendation': 0, 'sell_recommendation': 1,\n            'portfolio_analysis': 2, 'reduce_duration': 3,\n            'increase_yield': 4, 'hedge_volatility': 5,\n            'sector_rebalance': 6, 'barbell_strategy': 7,\n            'switch_bonds': 8, 'explain_recommendation': 9,\n            'market_outlook': 10, 'credit_analysis': 11,\n            'forecast_prices': 12\n        }\n        \n        self.sector_to_id = {\n            'Sovereign': 0, 'PSU Energy': 1, 'Financial': 2,\n            'Corporate': 3, 'Infrastructure': 4, 'NBFC': 5, 'Banking': 6\n        }\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        encoding = self.tokenizer(\n            sample['text'],\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        intent_label = self.intent_to_id[sample['intent']]\n        \n        sector_labels = torch.zeros(len(self.sector_to_id))\n        for sector in sample.get('sectors', []):\n            if sector in self.sector_to_id:\n                sector_labels[self.sector_to_id[sector]] = 1\n        \n        rating = sample.get('rating')\n        rating_map = {'AAA': 0, 'AA+': 1, 'AA': 2, 'A+': 3, 'A': 4, 'BBB': 5}\n        rating_label = rating_map.get(rating, 6)\n        \n        duration_map = {'short': 0, 'medium': 1, 'long': 2}\n        duration_label = duration_map.get(sample.get('duration', 'medium'), 1)\n        \n        constraints = sample.get('constraints', {})\n        constraint_labels = torch.tensor([\n            float(constraints.get('preserve_yield', False)),\n            float(constraints.get('maintain_liquidity', False)),\n            float(constraints.get('avoid_downgrades', False)),\n            float(constraints.get('sector_diversity', False)),\n            float(constraints.get('rating_above_aa', False))\n        ])\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'intent_label': torch.tensor(intent_label),\n            'sector_labels': sector_labels,\n            'rating_label': torch.tensor(rating_label),\n            'duration_label': torch.tensor(duration_label),\n            'constraint_labels': constraint_labels\n        }\n\n\n# ==================== MODEL ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Multi-task classifier\"\"\"\n    \n    def __init__(self, base_model: str = 'distilbert-base-uncased', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== LOSS ====================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n\n\nclass MultiTaskLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intent_loss_fn = FocalLoss(gamma=2.0)\n        self.sector_loss_fn = nn.BCEWithLogitsLoss()\n        self.rating_loss_fn = nn.CrossEntropyLoss()\n        self.duration_loss_fn = nn.CrossEntropyLoss()\n        self.constraint_loss_fn = nn.BCEWithLogitsLoss()\n    \n    def forward(self, outputs, labels):\n        intent_loss = self.intent_loss_fn(outputs['intent_logits'], labels['intent_label'])\n        sector_loss = self.sector_loss_fn(outputs['sector_logits'], labels['sector_labels'])\n        rating_loss = self.rating_loss_fn(outputs['rating_logits'], labels['rating_label'])\n        duration_loss = self.duration_loss_fn(outputs['duration_logits'], labels['duration_label'])\n        constraint_loss = self.constraint_loss_fn(outputs['constraint_logits'], labels['constraint_labels'])\n        \n        total = intent_loss + 0.5*sector_loss + 0.3*rating_loss + 0.3*duration_loss + 0.4*constraint_loss\n        \n        return {'total': total, 'intent': intent_loss}\n\n\n# ==================== TRAINER ====================\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, device, output_dir):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.best_acc = 0.0\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            \n            labels = {k: v.to(self.device) for k, v in batch.items() \n                     if k not in ['input_ids', 'attention_mask']}\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(input_ids, attention_mask)\n            loss_dict = self.criterion(outputs, labels)\n            loss = loss_dict['total']\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            preds = outputs['intent_logits'].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels['intent_label'].cpu().numpy())\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{accuracy_score(all_labels[-len(preds):], preds):.3f}'\n            })\n        \n        return total_loss / len(self.train_loader)\n    \n    def evaluate(self):\n        self.model.eval()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc='Evaluating'):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = {k: v.to(self.device) for k, v in batch.items() \n                         if k not in ['input_ids', 'attention_mask']}\n                \n                outputs = self.model(input_ids, attention_mask)\n                loss_dict = self.criterion(outputs, labels)\n                total_loss += loss_dict['total'].item()\n                \n                preds = outputs['intent_logits'].argmax(dim=-1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels['intent_label'].cpu().numpy())\n        \n        return {\n            'loss': total_loss / len(self.val_loader),\n            'accuracy': accuracy_score(all_labels, all_preds),\n            'f1_macro': f1_score(all_labels, all_preds, average='macro')\n        }\n    \n    def save_checkpoint(self, epoch, metrics):\n        if metrics['accuracy'] > self.best_acc:\n            self.best_acc = metrics['accuracy']\n            torch.save(self.model.state_dict(), self.output_dir / 'pytorch_model.bin')\n            print(f\"✓ New best model saved (acc: {metrics['accuracy']:.4f})\")\n    \n    def train(self, num_epochs):\n        print(\"=\" * 60)\n        print(\"TRAINING\")\n        print(\"=\" * 60)\n        \n        for epoch in range(1, num_epochs + 1):\n            train_loss = self.train_epoch(epoch)\n            print(f\"\\nEpoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n            \n            val_metrics = self.evaluate()\n            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Val F1: {val_metrics['f1_macro']:.4f}\\n\")\n            \n            self.save_checkpoint(epoch, val_metrics)\n        \n        print(f\"Training complete! Best accuracy: {self.best_acc:.4f}\\n\")\n\n\n# ==================== MAIN TRAINING FUNCTION ====================\n\ndef train_model():\n    \"\"\"Main training function\"\"\"\n    \n    # ---- 1) Synthetic data ----\n    synth_gen = SyntheticDataGenerator(CONFIG['num_samples_per_intent'])\n    synthetic_dataset = synth_gen.generate_dataset()\n    \n    # ---- 2) LLM data (optional) ----\n    llm_dataset: List[Dict] = []\n    if CONFIG.get('use_llm_data', False):\n        cache_path = Path(CONFIG['llm_dataset_cache'])\n        if CONFIG.get('reuse_cached_llm_data', True) and cache_path.exists():\n            print(\"=\" * 60)\n            print(\"LOADING CACHED LLM DATA\")\n            print(\"=\" * 60)\n            with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        llm_dataset.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        continue\n            print(f\"✓ Loaded {len(llm_dataset)} LLM samples from cache\\n\")\n        else:\n            llm_gen = LLMIntentDataGenerator(\n                model_name=CONFIG['llm_model'],\n                samples_per_intent=CONFIG['llm_samples_per_intent']\n            )\n            llm_dataset = llm_gen.generate_dataset()\n            cache_path.parent.mkdir(parents=True, exist_ok=True)\n            print(f\"Saving LLM dataset to: {cache_path}\")\n            with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n                for s in llm_dataset:\n                    f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n            print(\"✓ LLM dataset cached\\n\")\n    \n    # ---- 3) Merge datasets ----\n    dataset = synthetic_dataset + llm_dataset\n    print(f\"Total base dataset size (synthetic + LLM): {len(dataset)} samples\\n\")\n    \n    # ---- 4) Augment ----\n    dataset = DataAugmenter.augment_dataset(dataset, CONFIG['augmentation_factor'])\n    print(f\"Total dataset size after augmentation: {len(dataset)} samples\\n\")\n    \n    # ---- 5) Split ----\n    print(\"=\" * 60)\n    print(\"SPLITTING DATA\")\n    print(\"=\" * 60)\n    intents = [s['intent'] for s in dataset]\n    train_data, temp_data = train_test_split(dataset, test_size=0.3, stratify=intents, random_state=42)\n    temp_intents = [s['intent'] for s in temp_data]\n    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_intents, random_state=42)\n    \n    print(f\"Train: {len(train_data)}\")\n    print(f\"Val:   {len(val_data)}\")\n    print(f\"Test:  {len(test_data)}\\n\")\n    \n    # ---- 6) Tokenizer & Datasets ----\n    print(\"=\" * 60)\n    print(\"LOADING MODEL & TOKENIZER\")\n    print(\"=\" * 60)\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG['base_model'])\n    \n    train_dataset = BondQueryDataset(train_data, tokenizer, CONFIG['max_length'])\n    val_dataset = BondQueryDataset(val_data, tokenizer, CONFIG['max_length'])\n    test_dataset = BondQueryDataset(test_data, tokenizer, CONFIG['max_length'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'])\n    \n    # ---- 7) Model ----\n    model = ProductionBondClassifier(CONFIG['base_model'])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    print(f\"✓ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n    print(f\"✓ Device: {device}\\n\")\n    \n    # ---- 8) Optimizer & Scheduler ----\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n    num_training_steps = len(train_loader) * CONFIG['num_epochs']\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * CONFIG['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    criterion = MultiTaskLoss()\n    \n    # ---- 9) Train ----\n    trainer = Trainer(model, train_loader, val_loader, optimizer, scheduler, \n                     criterion, device, CONFIG['output_dir'])\n    trainer.train(CONFIG['num_epochs'])\n    \n    # ---- 10) Save tokenizer ----\n    tokenizer.save_pretrained(CONFIG['output_dir'])\n    \n    # ---- 11) Final test ----\n    print(\"=\" * 60)\n    print(\"FINAL TEST\")\n    print(\"=\" * 60)\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc='Testing'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask)\n            preds = outputs['intent_logits'].argmax(dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['intent_label'].numpy())\n    \n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    print(f\"\\n✓ Test Accuracy: {test_acc:.4f}\")\n    print(f\"✓ Test F1 Macro: {test_f1:.4f}\\n\")\n    \n    print(\"=\" * 60)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"\\n✓ Model saved to: {CONFIG['output_dir']}\")\n    print(f\"✓ Files: pytorch_model.bin, tokenizer files\")\n    print(\"\\nTo download:\")\n    print(\"  1. Go to Output tab\")\n    print(\"  2. Download 'bond_classifier_v3' folder\")\n    print(\"  3. Use locally with your inference notebook\")\n\n\n# ==================== RUN ====================\n\nif __name__ == '__main__':\n    train_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:09:15.772181Z","iopub.execute_input":"2025-11-30T07:09:15.772399Z","iopub.status.idle":"2025-11-30T07:45:46.771417Z","shell.execute_reply.started":"2025-11-30T07:09:15.772377Z","shell.execute_reply":"2025-11-30T07:45:46.770708Z"}},"outputs":[{"name":"stdout","text":"============================================================\nINSTALLING DEPENDENCIES\n============================================================\n✓ Dependencies installed!\n\n============================================================\nGPU CHECK\n============================================================\n✓ GPU detected: Tesla T4\n✓ GPU memory: 15.83 GB\n✓ CUDA version: 12.4\n\n============================================================\nGENERATING SYNTHETIC DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating synthetic:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e3f0aed2ef4dd9a94d4c0324bd4d6f"}},"metadata":{}},{"name":"stdout","text":"✓ Generated 10400 synthetic samples\n\n============================================================\nGENERATING LLM DATA (Gemini)\n============================================================\n\n→ Generating LLM data for intent: buy_recommendation (200 samples)\n  Collected 20/200 for buy_recommendation\n  Collected 40/200 for buy_recommendation\n  Collected 60/200 for buy_recommendation\n  Collected 80/200 for buy_recommendation\n  Collected 100/200 for buy_recommendation\n  Collected 120/200 for buy_recommendation\n  Collected 140/200 for buy_recommendation\n  Collected 160/200 for buy_recommendation\n  Collected 180/200 for buy_recommendation\n  Collected 200/200 for buy_recommendation\n\n→ Generating LLM data for intent: sell_recommendation (200 samples)\n  Collected 20/200 for sell_recommendation\n  Collected 40/200 for sell_recommendation\n  Collected 60/200 for sell_recommendation\n  Collected 80/200 for sell_recommendation\n  Collected 100/200 for sell_recommendation\n  Collected 120/200 for sell_recommendation\n  Collected 140/200 for sell_recommendation\n  Collected 160/200 for sell_recommendation\n  Collected 180/200 for sell_recommendation\n  Collected 200/200 for sell_recommendation\n\n→ Generating LLM data for intent: portfolio_analysis (200 samples)\n  Collected 20/200 for portfolio_analysis\n  Collected 40/200 for portfolio_analysis\n  Collected 60/200 for portfolio_analysis\n  Collected 80/200 for portfolio_analysis\n  Collected 100/200 for portfolio_analysis\n  Collected 120/200 for portfolio_analysis\n  Collected 140/200 for portfolio_analysis\n  Collected 160/200 for portfolio_analysis\n  Collected 180/200 for portfolio_analysis\n  Collected 200/200 for portfolio_analysis\n\n→ Generating LLM data for intent: reduce_duration (200 samples)\n  Collected 20/200 for reduce_duration\n  Collected 40/200 for reduce_duration\n  Collected 60/200 for reduce_duration\n  Collected 80/200 for reduce_duration\n  Collected 100/200 for reduce_duration\n  Collected 120/200 for reduce_duration\n  Collected 140/200 for reduce_duration\n  Collected 160/200 for reduce_duration\n  Collected 180/200 for reduce_duration\n  Collected 200/200 for reduce_duration\n\n→ Generating LLM data for intent: increase_yield (200 samples)\n  Collected 20/200 for increase_yield\n  Collected 40/200 for increase_yield\n  Collected 60/200 for increase_yield\n  Collected 80/200 for increase_yield\n  Collected 100/200 for increase_yield\n  Collected 120/200 for increase_yield\n  Collected 140/200 for increase_yield\n  Collected 160/200 for increase_yield\n  Collected 180/200 for increase_yield\n  Collected 200/200 for increase_yield\n\n→ Generating LLM data for intent: hedge_volatility (200 samples)\n  Collected 20/200 for hedge_volatility\n  Collected 40/200 for hedge_volatility\n  Collected 60/200 for hedge_volatility\n  Collected 80/200 for hedge_volatility\n  Collected 100/200 for hedge_volatility\n  Collected 120/200 for hedge_volatility\n  Collected 140/200 for hedge_volatility\n  Collected 160/200 for hedge_volatility\n  Collected 180/200 for hedge_volatility\n  Collected 200/200 for hedge_volatility\n\n→ Generating LLM data for intent: sector_rebalance (200 samples)\n  Collected 20/200 for sector_rebalance\n  Collected 40/200 for sector_rebalance\n  Collected 60/200 for sector_rebalance\n  Collected 80/200 for sector_rebalance\n  Collected 100/200 for sector_rebalance\n  Collected 120/200 for sector_rebalance\n  Collected 140/200 for sector_rebalance\n  Collected 160/200 for sector_rebalance\n  Collected 180/200 for sector_rebalance\n  Collected 200/200 for sector_rebalance\n\n→ Generating LLM data for intent: barbell_strategy (200 samples)\n  Collected 20/200 for barbell_strategy\n  Collected 40/200 for barbell_strategy\n  Collected 60/200 for barbell_strategy\n  Collected 80/200 for barbell_strategy\n  Collected 100/200 for barbell_strategy\n  Collected 120/200 for barbell_strategy\n  Collected 140/200 for barbell_strategy\n  Collected 160/200 for barbell_strategy\n  Collected 180/200 for barbell_strategy\n  Collected 200/200 for barbell_strategy\n\n→ Generating LLM data for intent: switch_bonds (200 samples)\n  Collected 20/200 for switch_bonds\n  Collected 40/200 for switch_bonds\n  Collected 60/200 for switch_bonds\n  Collected 80/200 for switch_bonds\n  Collected 100/200 for switch_bonds\n  Collected 120/200 for switch_bonds\n  Collected 140/200 for switch_bonds\n  Collected 160/200 for switch_bonds\n  Collected 180/200 for switch_bonds\n  Collected 200/200 for switch_bonds\n\n→ Generating LLM data for intent: explain_recommendation (200 samples)\n  Collected 20/200 for explain_recommendation\n  Collected 40/200 for explain_recommendation\n  Collected 60/200 for explain_recommendation\n  Collected 80/200 for explain_recommendation\n  Collected 100/200 for explain_recommendation\n  Collected 120/200 for explain_recommendation\n  Collected 140/200 for explain_recommendation\n  Collected 160/200 for explain_recommendation\n  Collected 180/200 for explain_recommendation\n  Collected 200/200 for explain_recommendation\n\n→ Generating LLM data for intent: market_outlook (200 samples)\n  Collected 20/200 for market_outlook\n  Collected 40/200 for market_outlook\n  Collected 60/200 for market_outlook\n  Collected 80/200 for market_outlook\n  Collected 100/200 for market_outlook\n  Collected 120/200 for market_outlook\n  Collected 140/200 for market_outlook\n  Collected 160/200 for market_outlook\n  Collected 180/200 for market_outlook\n  Collected 200/200 for market_outlook\n\n→ Generating LLM data for intent: credit_analysis (200 samples)\n  Collected 20/200 for credit_analysis\n  Collected 40/200 for credit_analysis\n  Collected 60/200 for credit_analysis\n  Collected 80/200 for credit_analysis\n  Collected 100/200 for credit_analysis\n  Collected 120/200 for credit_analysis\n  Collected 140/200 for credit_analysis\n  Collected 160/200 for credit_analysis\n  Collected 180/200 for credit_analysis\n  Collected 200/200 for credit_analysis\n\n→ Generating LLM data for intent: forecast_prices (200 samples)\n  Collected 20/200 for forecast_prices\n  Collected 40/200 for forecast_prices\n  Collected 60/200 for forecast_prices\n  Collected 80/200 for forecast_prices\n  Collected 100/200 for forecast_prices\n  Collected 120/200 for forecast_prices\n  Collected 140/200 for forecast_prices\n  Collected 160/200 for forecast_prices\n  Collected 180/200 for forecast_prices\n  Collected 200/200 for forecast_prices\n\n✓ Generated 2600 LLM-labelled samples\n\nSaving LLM dataset to: /kaggle/working/llm_intent_dataset.jsonl\n✓ LLM dataset cached\n\nTotal base dataset size (synthetic + LLM): 13000 samples\n\n============================================================\nAUGMENTING DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmenting:   0%|          | 0/6500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4932afd1b44146c9a489c3a0cc46baf4"}},"metadata":{}},{"name":"stdout","text":"✓ Added 6500 augmented samples\n\nTotal dataset size after augmentation: 19500 samples\n\n============================================================\nSPLITTING DATA\n============================================================\nTrain: 13650\nVal:   2925\nTest:  2925\n\n============================================================\nLOADING MODEL & TOKENIZER\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aeaf0e15a53439995203f48b0f3a4f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e78293259954e47953e23206644dbf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8663c742fe94baa8d499ab05547f0d7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-30 07:31:20.989825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764487881.193815      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764487881.251439      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5fe4b718584178af7bcb09d8d59c74"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded: 142,205,987 parameters\n✓ Device: cuda\n\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01326e94ecf14b2d8c22dbe41284cb02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7eb1100b7ad44a1a371ad9188d95954"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/4 - Train Loss: 1.5893\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ba535e908c469bbcc3c6b9d333f62a"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.4190\nVal Accuracy: 0.9942\nVal F1: 0.9942\n\n✓ New best model saved (acc: 0.9942)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3505a7960b140af912b4267f65e12d2"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2/4 - Train Loss: 0.3604\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45d3b8a19a3348c486b2f1d8e3f1a340"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.2936\nVal Accuracy: 0.9925\nVal F1: 0.9925\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a498476a4847a8b6de94e8c7b7ab37"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3/4 - Train Loss: 0.2755\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5cba525c04846bb930ab5843c326754"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.2490\nVal Accuracy: 0.9959\nVal F1: 0.9959\n\n✓ New best model saved (acc: 0.9959)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f0014f08584975b787c7dde6fb7e52"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4/4 - Train Loss: 0.2502\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead222c2036e479aaa0acbbaf7a73ff1"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.2409\nVal Accuracy: 0.9945\nVal F1: 0.9945\n\nTraining complete! Best accuracy: 0.9959\n\n============================================================\nFINAL TEST\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664f08f5c9384251bd1df1b2c2b13ef8"}},"metadata":{}},{"name":"stdout","text":"\n✓ Test Accuracy: 0.9945\n✓ Test F1 Macro: 0.9945\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n\n✓ Model saved to: /kaggle/working/bond_classifier_v3\n✓ Files: pytorch_model.bin, tokenizer files\n\nTo download:\n  1. Go to Output tab\n  2. Download 'bond_classifier_v3' folder\n  3. Use locally with your inference notebook\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==================== CONFIGURATION ====================\nCONFIG = {\n    'base_model': 'microsoft/deberta-v3-small',\n    'num_samples_per_intent': 800,   # synthetic per intent (you can tune)\n    'augmentation_factor': 0.5,      # 50% more data\n\n    # --- NEW: LLM data generation config ---\n    'use_llm_data': True,            # toggle this to disable LLM data\n    'llm_samples_per_intent': 200,   # how many LLM examples per intent\n    'llm_model': 'gemini-2.0-flash',  # or 'gemini-1.5-flash' if you prefer\n\n    'llm_dataset_cache': '/kaggle/working/llm_intent_dataset.jsonl',\n    'reuse_cached_llm_data': True,   # if cache exists, reuse instead of regenerating\n\n    'batch_size': 32,\n    'num_epochs': 10,\n    'learning_rate': 2e-5,\n    'warmup_ratio': 0.1,\n    'max_length': 128,\n    'output_dir': '/kaggle/working/bond_classifier_v3',\n    'seed': 42\n}\n\n# ==================== MODEL ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Multi-task classifier\"\"\"\n    \n    def __init__(self, base_model: str = 'distilbert-base-uncased', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== LOSS ====================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n\n\nclass MultiTaskLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intent_loss_fn = FocalLoss(gamma=2.0)\n        self.sector_loss_fn = nn.BCEWithLogitsLoss()\n        self.rating_loss_fn = nn.CrossEntropyLoss()\n        self.duration_loss_fn = nn.CrossEntropyLoss()\n        self.constraint_loss_fn = nn.BCEWithLogitsLoss()\n    \n    def forward(self, outputs, labels):\n        intent_loss = self.intent_loss_fn(outputs['intent_logits'], labels['intent_label'])\n        sector_loss = self.sector_loss_fn(outputs['sector_logits'], labels['sector_labels'])\n        rating_loss = self.rating_loss_fn(outputs['rating_logits'], labels['rating_label'])\n        duration_loss = self.duration_loss_fn(outputs['duration_logits'], labels['duration_label'])\n        constraint_loss = self.constraint_loss_fn(outputs['constraint_logits'], labels['constraint_labels'])\n        \n        total = intent_loss + 0.5*sector_loss + 0.3*rating_loss + 0.3*duration_loss + 0.4*constraint_loss\n        \n        return {'total': total, 'intent': intent_loss}\n\n\n# ==================== TRAINER ====================\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, device, output_dir):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.best_acc = 0.0\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            \n            labels = {k: v.to(self.device) for k, v in batch.items() \n                     if k not in ['input_ids', 'attention_mask']}\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(input_ids, attention_mask)\n            loss_dict = self.criterion(outputs, labels)\n            loss = loss_dict['total']\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            preds = outputs['intent_logits'].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels['intent_label'].cpu().numpy())\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{accuracy_score(all_labels[-len(preds):], preds):.3f}'\n            })\n        \n        return total_loss / len(self.train_loader)\n    \n    def evaluate(self):\n        self.model.eval()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc='Evaluating'):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = {k: v.to(self.device) for k, v in batch.items() \n                         if k not in ['input_ids', 'attention_mask']}\n                \n                outputs = self.model(input_ids, attention_mask)\n                loss_dict = self.criterion(outputs, labels)\n                total_loss += loss_dict['total'].item()\n                \n                preds = outputs['intent_logits'].argmax(dim=-1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels['intent_label'].cpu().numpy())\n        \n        return {\n            'loss': total_loss / len(self.val_loader),\n            'accuracy': accuracy_score(all_labels, all_preds),\n            'f1_macro': f1_score(all_labels, all_preds, average='macro')\n        }\n    \n    def save_checkpoint(self, epoch, metrics):\n        if metrics['accuracy'] > self.best_acc:\n            self.best_acc = metrics['accuracy']\n            torch.save(self.model.state_dict(), self.output_dir / 'pytorch_model.bin')\n            print(f\"✓ New best model saved (acc: {metrics['accuracy']:.4f})\")\n    \n    def train(self, num_epochs):\n        print(\"=\" * 60)\n        print(\"TRAINING\")\n        print(\"=\" * 60)\n        \n        for epoch in range(1, num_epochs + 1):\n            train_loss = self.train_epoch(epoch)\n            print(f\"\\nEpoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n            \n            val_metrics = self.evaluate()\n            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Val F1: {val_metrics['f1_macro']:.4f}\\n\")\n            \n            self.save_checkpoint(epoch, val_metrics)\n        \n        print(f\"Training complete! Best accuracy: {self.best_acc:.4f}\\n\")\n\n\n# ==================== MAIN TRAINING FUNCTION ====================\n\ndef train_model():\n    \"\"\"Main training function\"\"\"\n    \n    # ---- 1) Synthetic data ----\n    synth_gen = SyntheticDataGenerator(CONFIG['num_samples_per_intent'])\n    synthetic_dataset = synth_gen.generate_dataset()\n    \n    # ---- 2) LLM data (optional) ----\n    llm_dataset: List[Dict] = []\n    if CONFIG.get('use_llm_data', False):\n        cache_path = Path(CONFIG['llm_dataset_cache'])\n        if CONFIG.get('reuse_cached_llm_data', True) and cache_path.exists():\n            print(\"=\" * 60)\n            print(\"LOADING CACHED LLM DATA\")\n            print(\"=\" * 60)\n            with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        llm_dataset.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        continue\n            print(f\"✓ Loaded {len(llm_dataset)} LLM samples from cache\\n\")\n        else:\n            llm_gen = LLMIntentDataGenerator(\n                model_name=CONFIG['llm_model'],\n                samples_per_intent=CONFIG['llm_samples_per_intent']\n            )\n            llm_dataset = llm_gen.generate_dataset()\n            cache_path.parent.mkdir(parents=True, exist_ok=True)\n            print(f\"Saving LLM dataset to: {cache_path}\")\n            with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n                for s in llm_dataset:\n                    f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n            print(\"✓ LLM dataset cached\\n\")\n    \n    # ---- 3) Merge datasets ----\n    dataset = synthetic_dataset + llm_dataset\n    print(f\"Total base dataset size (synthetic + LLM): {len(dataset)} samples\\n\")\n    \n    # ---- 4) Augment ----\n    dataset = DataAugmenter.augment_dataset(dataset, CONFIG['augmentation_factor'])\n    print(f\"Total dataset size after augmentation: {len(dataset)} samples\\n\")\n    \n    # ---- 5) Split ----\n    print(\"=\" * 60)\n    print(\"SPLITTING DATA\")\n    print(\"=\" * 60)\n    intents = [s['intent'] for s in dataset]\n    train_data, temp_data = train_test_split(dataset, test_size=0.3, stratify=intents, random_state=42)\n    temp_intents = [s['intent'] for s in temp_data]\n    val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_intents, random_state=42)\n    \n    print(f\"Train: {len(train_data)}\")\n    print(f\"Val:   {len(val_data)}\")\n    print(f\"Test:  {len(test_data)}\\n\")\n    \n    # ---- 6) Tokenizer & Datasets ----\n    print(\"=\" * 60)\n    print(\"LOADING MODEL & TOKENIZER\")\n    print(\"=\" * 60)\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG['base_model'])\n    \n    train_dataset = BondQueryDataset(train_data, tokenizer, CONFIG['max_length'])\n    val_dataset = BondQueryDataset(val_data, tokenizer, CONFIG['max_length'])\n    test_dataset = BondQueryDataset(test_data, tokenizer, CONFIG['max_length'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'])\n    \n    # ---- 7) Model ----\n    model = ProductionBondClassifier(CONFIG['base_model'])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    print(f\"✓ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n    print(f\"✓ Device: {device}\\n\")\n    \n    # ---- 8) Optimizer & Scheduler ----\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n    num_training_steps = len(train_loader) * CONFIG['num_epochs']\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * CONFIG['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    criterion = MultiTaskLoss()\n    \n    # ---- 9) Train ----\n    trainer = Trainer(model, train_loader, val_loader, optimizer, scheduler, \n                     criterion, device, CONFIG['output_dir'])\n    trainer.train(CONFIG['num_epochs'])\n    \n    # ---- 10) Save tokenizer ----\n    tokenizer.save_pretrained(CONFIG['output_dir'])\n    \n    # ---- 11) Final test ----\n    print(\"=\" * 60)\n    print(\"FINAL TEST\")\n    print(\"=\" * 60)\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc='Testing'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask)\n            preds = outputs['intent_logits'].argmax(dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['intent_label'].numpy())\n    \n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    print(f\"\\n✓ Test Accuracy: {test_acc:.4f}\")\n    print(f\"✓ Test F1 Macro: {test_f1:.4f}\\n\")\n    \n    print(\"=\" * 60)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"\\n✓ Model saved to: {CONFIG['output_dir']}\")\n    print(f\"✓ Files: pytorch_model.bin, tokenizer files\")\n    print(\"\\nTo download:\")\n    print(\"  1. Go to Output tab\")\n    print(\"  2. Download 'bond_classifier_v3' folder\")\n    print(\"  3. Use locally with your inference notebook\")\n\n\n# ==================== RUN ====================\n\nif __name__ == '__main__':\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T08:48:39.844124Z","iopub.execute_input":"2025-11-30T08:48:39.844910Z","iopub.status.idle":"2025-11-30T09:23:59.818021Z","shell.execute_reply.started":"2025-11-30T08:48:39.844884Z","shell.execute_reply":"2025-11-30T09:23:59.817180Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING SYNTHETIC DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ded4e230d2c44bf968b6aaba328eb12"}},"metadata":{}},{"name":"stdout","text":"✓ Generated 10400 samples\n\n============================================================\nLOADING CACHED LLM DATA\n============================================================\n✓ Loaded 2600 LLM samples from cache\n\nTotal base dataset size (synthetic + LLM): 13000 samples\n\n============================================================\nAUGMENTING DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmenting:   0%|          | 0/6500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb422e953064309b4ab420df547e0eb"}},"metadata":{}},{"name":"stdout","text":"✓ Added 6500 augmented samples\n\nTotal dataset size after augmentation: 19500 samples\n\n============================================================\nSPLITTING DATA\n============================================================\nTrain: 13650\nVal:   2925\nTest:  2925\n\n============================================================\nLOADING MODEL & TOKENIZER\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Model loaded: 142,205,987 parameters\n✓ Device: cuda\n\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"216e771af97c4dc490de4d2c8cf88eb9"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/10 - Train Loss: 2.1460\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d729d7a50a89480b819a6b2ec515fc5b"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5034\nVal Accuracy: 0.9908\nVal F1: 0.9907\n\n✓ New best model saved (acc: 0.9908)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba956dd6d584f00b1f0ac9e7f1638ec"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2/10 - Train Loss: 0.4018\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"919da95031414b8ab20715552d08b808"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.2853\nVal Accuracy: 0.9952\nVal F1: 0.9952\n\n✓ New best model saved (acc: 0.9952)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d599004d65dc4fa5aa668fda6a509ff4"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3/10 - Train Loss: 0.2671\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90d7a31548db417ba1398e0f4a636d9f"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.2153\nVal Accuracy: 0.9966\nVal F1: 0.9966\n\n✓ New best model saved (acc: 0.9966)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abeacd85901e4a4eadb7c5d518e6f98e"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4/10 - Train Loss: 0.2113\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d619d0585b8840bda44315189025dbd1"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1759\nVal Accuracy: 0.9983\nVal F1: 0.9983\n\n✓ New best model saved (acc: 0.9983)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb4e28c3f419446da8bfb17223d34dc3"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 5/10 - Train Loss: 0.1747\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def7aa6cf3334962878b595d0f4d3cef"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1531\nVal Accuracy: 0.9983\nVal F1: 0.9983\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395b9d26df994e0aa6d53f3b664cd5b5"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 6/10 - Train Loss: 0.1546\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071bfe048c4e4008981b0035364db648"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1390\nVal Accuracy: 0.9979\nVal F1: 0.9979\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b002c4ea80474e87b30906060bf9a9"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 7/10 - Train Loss: 0.1409\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f4c8ce860c4d3996c580a34597367f"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1315\nVal Accuracy: 0.9983\nVal F1: 0.9983\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1eaaf61831471494c1b77a62f94733"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 8/10 - Train Loss: 0.1332\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55fb5db981c04522a88b0979760d4131"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1276\nVal Accuracy: 0.9986\nVal F1: 0.9986\n\n✓ New best model saved (acc: 0.9986)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16d26c95a2546b490c7856d397f97cd"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 9/10 - Train Loss: 0.1293\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f639e3541461461a91bea35ecedaae2f"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1256\nVal Accuracy: 0.9986\nVal F1: 0.9986\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/427 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c714982baff04d289d5ce02ff18a6889"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 10/10 - Train Loss: 0.1272\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860a138ec1a94642a342ba4fb5eea14e"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.1255\nVal Accuracy: 0.9986\nVal F1: 0.9986\n\nTraining complete! Best accuracy: 0.9986\n\n============================================================\nFINAL TEST\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/92 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24085e2ca5504726a382f842349fd1a2"}},"metadata":{}},{"name":"stdout","text":"\n✓ Test Accuracy: 0.9979\n✓ Test F1 Macro: 0.9979\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n\n✓ Model saved to: /kaggle/working/bond_classifier_v3\n✓ Files: pytorch_model.bin, tokenizer files\n\nTo download:\n  1. Go to Output tab\n  2. Download 'bond_classifier_v3' folder\n  3. Use locally with your inference notebook\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ==================== EXPORT DATASET FOR DOWNLOAD ====================\nfrom pathlib import Path\nimport json\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\" * 60)\nprint(\"REBUILDING & SAVING DATASET (SYNTHETIC + GEMINI)\")\nprint(\"=\" * 60)\n\n# 1) Synthetic data (same as training)\nsynth_gen = SyntheticDataGenerator(CONFIG['num_samples_per_intent'])\nsynthetic_dataset = synth_gen.generate_dataset()\n\n# 2) LLM (Gemini) data – prefer cached file to avoid extra API calls\nllm_dataset: List[Dict] = []\nif CONFIG.get(\"use_llm_data\", False):\n    cache_path = Path(CONFIG[\"llm_dataset_cache\"])\n    if cache_path.exists():\n        print(\"Loading LLM data from cache:\", cache_path)\n        with cache_path.open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    llm_dataset.append(json.loads(line))\n                except json.JSONDecodeError:\n                    continue\n        print(f\"✓ Loaded {len(llm_dataset)} LLM samples from cache\\n\")\n    else:\n        print(\"⚠ LLM cache not found, generating a fresh LLM dataset...\")\n        llm_gen = LLMIntentDataGenerator(\n            model_name=CONFIG[\"llm_model\"],\n            samples_per_intent=CONFIG[\"llm_samples_per_intent\"],\n        )\n        llm_dataset = llm_gen.generate_dataset()\n        cache_path.parent.mkdir(parents=True, exist_ok=True)\n        with cache_path.open(\"w\", encoding=\"utf-8\") as f:\n            for s in llm_dataset:\n                f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n        print(f\"✓ LLM dataset generated & cached to {cache_path}\\n\")\n\n# 3) Merge datasets\ndataset = synthetic_dataset + llm_dataset\nprint(f\"Base dataset size (synthetic + LLM): {len(dataset)}\")\n\n# 4) Augment (same factor as training)\ndataset = DataAugmenter.augment_dataset(dataset, CONFIG[\"augmentation_factor\"])\nprint(f\"Dataset size after augmentation: {len(dataset)}\\n\")\n\n# 5) Train/val/test split (same logic as training)\nintents = [s[\"intent\"] for s in dataset]\ntrain_data, temp_data = train_test_split(\n    dataset, test_size=0.3, stratify=intents, random_state=42\n)\ntemp_intents = [s[\"intent\"] for s in temp_data]\nval_data, test_data = train_test_split(\n    temp_data, test_size=0.5, stratify=temp_intents, random_state=42\n)\n\nprint(f\"Train: {len(train_data)}\")\nprint(f\"Val:   {len(val_data)}\")\nprint(f\"Test:  {len(test_data)}\\n\")\n\n# 6) Save to /kaggle/working so you can download from Output tab\nout_dir = Path(\"/kaggle/working/bond_intent_data\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\ndef save_jsonl(path: Path, rows):\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nsave_jsonl(out_dir / \"train.jsonl\", train_data)\nsave_jsonl(out_dir / \"val.jsonl\", val_data)\nsave_jsonl(out_dir / \"test.jsonl\", test_data)\n\nprint(\"✓ Saved JSONL datasets to:\")\nprint(f\"  {out_dir / 'train.jsonl'}\")\nprint(f\"  {out_dir / 'val.jsonl'}\")\nprint(f\"  {out_dir / 'test.jsonl'}\")\nprint(\"\\nTo download:\")\nprint(\"  Go to the 'Output' tab in Kaggle and download the 'bond_intent_data' folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:49:20.660019Z","iopub.execute_input":"2025-11-30T07:49:20.660784Z","iopub.status.idle":"2025-11-30T07:49:21.072657Z","shell.execute_reply.started":"2025-11-30T07:49:20.660755Z","shell.execute_reply":"2025-11-30T07:49:21.071817Z"}},"outputs":[{"name":"stdout","text":"============================================================\nREBUILDING & SAVING DATASET (SYNTHETIC + GEMINI)\n============================================================\n============================================================\nGENERATING SYNTHETIC DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating synthetic:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"767d5c4cec12458287da15c341346211"}},"metadata":{}},{"name":"stdout","text":"✓ Generated 10400 synthetic samples\n\nLoading LLM data from cache: /kaggle/working/llm_intent_dataset.jsonl\n✓ Loaded 2600 LLM samples from cache\n\nBase dataset size (synthetic + LLM): 13000\n============================================================\nAUGMENTING DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmenting:   0%|          | 0/6500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf8608bede446be82122084a37e6d34"}},"metadata":{}},{"name":"stdout","text":"✓ Added 6500 augmented samples\n\nDataset size after augmentation: 19500\n\nTrain: 13650\nVal:   2925\nTest:  2925\n\n✓ Saved JSONL datasets to:\n  /kaggle/working/bond_intent_data/train.jsonl\n  /kaggle/working/bond_intent_data/val.jsonl\n  /kaggle/working/bond_intent_data/test.jsonl\n\nTo download:\n  Go to the 'Output' tab in Kaggle and download the 'bond_intent_data' folder.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Kaggle Inference Notebook (DeBERTa + Gemini data)\n=========================================================================\n\nSetup:\n1. Upload / attach the training output as a Kaggle dataset\n   (it should contain the folder `bond_classifier_v3` with pytorch_model.bin + tokenizer files)\n2. Update MODEL_PATH below to point to that folder\n3. Run this notebook to test the classifier\n\"\"\"\n\n# ==================== INSTALL & IMPORTS ====================\nimport sys\nimport subprocess\n\nprint(\"Installing dependencies...\")\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"transformers\", \"accelerate\", \"scikit-learn\"\n])\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport numpy as np\n\nprint(\"✓ Imports complete!\\n\")\n\n\n# ==================== MODEL ARCHITECTURE ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Same architecture as training: DeBERTa-v3-small + multi-task heads.\"\"\"\n    \n    def __init__(self, base_model: str = 'microsoft/deberta-v3-small', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # Heads: intent + sectors + rating + duration + constraints\n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== INFERENCE CLASS ====================\n\nclass QueryIntent(str, Enum):\n    BUY_RECOMMENDATION = \"buy_recommendation\"\n    SELL_RECOMMENDATION = \"sell_recommendation\"\n    PORTFOLIO_ANALYSIS = \"portfolio_analysis\"\n    REDUCE_DURATION = \"reduce_duration\"\n    INCREASE_YIELD = \"increase_yield\"\n    HEDGE_VOLATILITY = \"hedge_volatility\"\n    SECTOR_REBALANCE = \"sector_rebalance\"\n    BARBELL_STRATEGY = \"barbell_strategy\"\n    SWITCH_BONDS = \"switch_bonds\"\n    EXPLAIN_RECOMMENDATION = \"explain_recommendation\"\n    MARKET_OUTLOOK = \"market_outlook\"\n    CREDIT_ANALYSIS = \"credit_analysis\"\n    FORECAST_PRICES = \"forecast_prices\"\n\n\n@dataclass\nclass ClassificationResult:\n    intent: str\n    confidence: float\n    filters: Dict[str, Any]\n    constraints: Dict[str, bool]\n\n\nclass BondClassifier:\n    \"\"\"Production classifier wrapper with MC-Dropout + filter extraction.\"\"\"\n    \n    def __init__(self, model_path: str, base_model: str = \"microsoft/deberta-v3-small\"):\n        \"\"\"\n        Load model from Kaggle path.\n        \n        Args:\n            model_path: path to folder containing pytorch_model.bin + tokenizer files,\n                        e.g. '/kaggle/working/bond_classifier_v3'\n        \"\"\"\n        print(f\"Loading model from: {model_path}\")\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        \n        # Load tokenizer saved during training\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        # Load model with same architecture as training\n        self.model = ProductionBondClassifier(base_model=base_model)\n        state_dict = torch.load(f\"{model_path}/pytorch_model.bin\", map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n        \n        # Label mappings\n        self.intent_names = [\n            'buy_recommendation', 'sell_recommendation', 'portfolio_analysis',\n            'reduce_duration', 'increase_yield', 'hedge_volatility',\n            'sector_rebalance', 'barbell_strategy', 'switch_bonds',\n            'explain_recommendation', 'market_outlook', 'credit_analysis',\n            'forecast_prices'\n        ]\n        \n        self.sector_names = [\n            'Sovereign', 'PSU Energy', 'Financial', 'Corporate', \n            'Infrastructure', 'NBFC', 'Banking'\n        ]\n        # 7 rating classes in training (0..6)\n        self.rating_names = ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB', 'Unrated']\n        self.duration_names = ['short', 'medium', 'long']\n        \n        print(\"✓ Model loaded successfully!\\n\")\n    \n    def classify(self, query: str, num_samples: int = 5) -> ClassificationResult:\n        \"\"\"\n        Classify query with MC-Dropout uncertainty.\n        \n        Args:\n            query: User query string\n            num_samples: Number of stochastic forward passes\n        \"\"\"\n        # Tokenize\n        enc = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        input_ids = enc[\"input_ids\"].to(self.device)\n        attention_mask = enc[\"attention_mask\"].to(self.device)\n        \n        # --- MC Dropout on intent head ---\n        self.model.train()  # enable dropout\n        intent_predictions = []\n        \n        with torch.no_grad():\n            for _ in range(num_samples):\n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                intent_probs = F.softmax(outputs['intent_logits'], dim=-1)\n                intent_predictions.append(intent_probs.cpu())\n        \n        # Aggregate\n        intent_mean = torch.stack(intent_predictions).mean(dim=0)[0]\n        confidence = intent_mean.max().item()\n        predicted_idx = intent_mean.argmax().item()\n        \n        # Final pass (deterministic) for other heads\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            sectors = self._extract_sectors(outputs['sector_logits'])\n            rating = self._extract_rating(outputs['rating_logits'])\n            duration = self._extract_duration(outputs['duration_logits'])\n            constraints = self._extract_constraints(outputs['constraint_logits'])\n        \n        return ClassificationResult(\n            intent=self.intent_names[predicted_idx],\n            confidence=confidence,\n            filters={\n                'sectors': sectors,\n                'min_rating': rating,\n                'duration_preference': duration\n            },\n            constraints=constraints\n        )\n    \n    def _extract_sectors(self, logits: torch.Tensor) -> List[str]:\n        \"\"\"Multi-label sector head with simple sigmoid threshold.\"\"\"\n        probs = torch.sigmoid(logits[0])\n        # Slightly relaxed threshold to account for noisy training\n        indices = (probs > 0.4).nonzero(as_tuple=True)[0]\n        return [self.sector_names[i] for i in indices]\n    \n    def _extract_rating(self, logits: torch.Tensor) -> Optional[str]:\n        \"\"\"Map argmax of 7-way rating head to rating string.\"\"\"\n        idx = logits[0].argmax().item()\n        if 0 <= idx < len(self.rating_names):\n            return self.rating_names[idx]\n        return None\n    \n    def _extract_duration(self, logits: torch.Tensor) -> str:\n        \"\"\"Map argmax of 3-way duration head to short/medium/long.\"\"\"\n        idx = logits[0].argmax().item()\n        if 0 <= idx < len(self.duration_names):\n            return self.duration_names[idx]\n        return \"medium\"\n    \n    def _extract_constraints(self, logits: torch.Tensor) -> Dict[str, bool]:\n        \"\"\"Binary constraint flags from 5-way sigmoid head.\"\"\"\n        probs = torch.sigmoid(logits[0])\n        return {\n            'preserve_yield': probs[0].item() > 0.5,\n            'maintain_liquidity': probs[1].item() > 0.5,\n            'avoid_downgrades': probs[2].item() > 0.5,\n            'sector_diversity': probs[3].item() > 0.5,\n            'rating_above_aa': probs[4].item() > 0.5\n        }\n    \n    def batch_classify(self, queries: List[str]) -> List[ClassificationResult]:\n        \"\"\"Classify multiple queries.\"\"\"\n        return [self.classify(q) for q in queries]\n\n\n# ==================== USAGE EXAMPLE ====================\n\ndef test_classifier():\n    \"\"\"Test the classifier with sample queries.\"\"\"\n    \n    # If running in the SAME notebook right after training:\n    MODEL_PATH = '/kaggle/working/bond_classifier_v3'\n    # If as a separate notebook, this might be e.g.:\n    # MODEL_PATH = '/kaggle/input/bond-classifier-v3/bond_classifier_v3'\n    \n    print(\"=\" * 60)\n    print(\"LOADING CLASSIFIER\")\n    print(\"=\" * 60)\n    \n    try:\n        classifier = BondClassifier(MODEL_PATH, base_model=\"microsoft/deberta-v3-small\")\n    except Exception as e:\n        print(f\"❌ Error loading model: {e}\")\n        print(\"\\nMake sure to:\")\n        print(\"1. Attach the training output as a dataset (if in a new notebook)\")\n        print(\"2. Update MODEL_PATH above\")\n        return\n    \n    print(\"=\" * 60)\n    print(\"TESTING QUERIES\")\n    print(\"=\" * 60)\n    \n    test_queries = [\n        # --- Category 1: Ambiguous Buy/Sell/Strategy ---\n        \"Should I shift my SDL holdings into shorter PSU bonds or just hold?\",\n        \"I want to reduce rate risk but I don’t want my yield to fall. What should I do?\",\n        \"Is it better to exit long-duration bonds and move into AA corporates?\",\n        \"Would switching from NTPC 2035 to REC 2030 improve my return?\",\n        \"Which reduces risk more: selling perpetuals or adding short-term G-Secs?\",\n    \n        # --- Category 2: Disguised intents ---\n        \"My yield looks weak lately… what should I change?\",\n        \"These long papers are stressing me out—what’s the safest move?\",\n        \"My portfolio feels too boring. Suggest something with more kick.\",\n        \"The curve is flattening; should I reposition?\",\n        \"My advisor said I’m too exposed. What adjustments should I consider?\",\n    \n        # --- Category 3: Multi-intent queries ---\n        \"Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\",\n        \"Reduce my duration and give alternatives with at least 7.5% yield.\",\n        \"Analyze my portfolio and tell me which low-yield bonds I should switch.\",\n        \"Before suggesting buys, what’s your outlook on corporate spreads?\",\n        \"If I shift to shorter bonds, how will my yield be impacted?\",\n    \n        # --- Category 4: Tricky phrasing for buy intent ---\n        \"Is NTPC 2033 attractive at current spreads?\",\n        \"Are AA PSU bonds offering a good entry point?\",\n        \"Is this a good time to accumulate SDLs?\",\n        \"Should I start adding exposure to long maturities?\",\n        \"Are there better alternatives to ICICI 2029 with similar risk?\",\n    \n        # --- Category 5: Risk management intents ---\n        \"The market seems jumpy; how do I protect my portfolio?\",\n        \"If RBI hikes unexpectedly, which holdings will get hit the most?\",\n        \"Rate volatility worries me—how do I reduce the impact?\",\n        \"How can I stabilize P&L swings in my portfolio?\",\n        \"Should I rebalance sectors before the credit cycle weakens?\",\n    \n        # --- Category 6: Very short hard queries ---\n        \"Duration too high?\",\n        \"Better yield ideas?\",\n        \"Switch or hold?\",\n        \"Cut risk?\",\n        \"Add PSU?\",\n    \n        # --- Category 7: Contradictory constraints ---\n        \"Cut duration but don’t let yield drop below 7.6%.\",\n        \"I want high yield but without taking credit risk.\",\n        \"Reduce risk but avoid selling anything.\",\n        \"Switch out of low-yield bonds but keep duration same.\",\n        \"Increase return without increasing duration or credit risk.\",\n    \n        # --- Category 8: Multi-sentence queries ---\n        \"My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\",\n        \"My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\",\n        \"I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\",\n        \"I expect inflation to cool. Should I increase duration a bit?\",\n        \"Markets feel stable. Should I rotate sectors or focus on yield first?\",\n    \n        # --- Category 9: Credit-analysis queries ---\n        \"How is the credit quality of PFC right now?\",\n        \"Is REC fundamentally strong enough for long-term holding?\",\n        \"What’s the default risk on NTPC?\",\n        \"Should I worry about credit spreads widening?\",\n        \"Are AA- names safe in this environment?\",\n    \n        # --- Category 10: Forecast / outlook queries ---\n        \"Where do you see G-Sec yields in six months?\",\n        \"If rates fall by 50 bps, what happens to long-duration PSU bonds?\",\n        \"Will corporate spreads tighten this year?\",\n        \"Predict the movement of the 10-year benchmark.\",\n        \"How will a Fed cut affect Indian bond yields?\",\n    ]\n\n    for query in test_queries:\n        result = classifier.classify(query)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Query: {query}\")\n        print(f\"{'='*60}\")\n        print(f\"Intent: {result.intent}\")\n        print(f\"Confidence: {result.confidence:.3f}\")\n        # print(f\"Sectors: {result.filters['sectors']}\")\n        # print(f\"Rating: {result.filters['min_rating']}\")\n        # print(f\"Duration: {result.filters['duration_preference']}\")\n        # print(f\"Constraints: {result.constraints}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"TESTING COMPLETE!\")\n    print(\"=\" * 60)\n\n\n# ==================== RUN ====================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BOND QUERY CLASSIFIER - INFERENCE\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    # Option 1: Automated test with curated queries\n    test_classifier()\n    \n    # Option 2: Interactive mode (uncomment to use)\n    # interactive_test()\n    \n    # Option 3: Batch testing (uncomment to use)\n    # batch_test_from_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:25:02.322859Z","iopub.execute_input":"2025-11-30T09:25:02.323151Z","iopub.status.idle":"2025-11-30T09:25:11.778127Z","shell.execute_reply.started":"2025-11-30T09:25:02.323127Z","shell.execute_reply":"2025-11-30T09:25:11.777175Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\n✓ Imports complete!\n\n\n============================================================\nBOND QUERY CLASSIFIER - INFERENCE\n============================================================\n\n============================================================\nLOADING CLASSIFIER\n============================================================\nLoading model from: /kaggle/working/bond_classifier_v3\nUsing device: cuda\n✓ Model loaded successfully!\n\n============================================================\nTESTING QUERIES\n============================================================\n\n============================================================\nQuery: Should I shift my SDL holdings into shorter PSU bonds or just hold?\n============================================================\nIntent: switch_bonds\nConfidence: 0.548\n\n============================================================\nQuery: I want to reduce rate risk but I don’t want my yield to fall. What should I do?\n============================================================\nIntent: reduce_duration\nConfidence: 0.586\n\n============================================================\nQuery: Is it better to exit long-duration bonds and move into AA corporates?\n============================================================\nIntent: switch_bonds\nConfidence: 0.309\n\n============================================================\nQuery: Would switching from NTPC 2035 to REC 2030 improve my return?\n============================================================\nIntent: switch_bonds\nConfidence: 0.574\n\n============================================================\nQuery: Which reduces risk more: selling perpetuals or adding short-term G-Secs?\n============================================================\nIntent: reduce_duration\nConfidence: 0.258\n\n============================================================\nQuery: My yield looks weak lately… what should I change?\n============================================================\nIntent: increase_yield\nConfidence: 0.342\n\n============================================================\nQuery: These long papers are stressing me out—what’s the safest move?\n============================================================\nIntent: reduce_duration\nConfidence: 0.437\n\n============================================================\nQuery: My portfolio feels too boring. Suggest something with more kick.\n============================================================\nIntent: switch_bonds\nConfidence: 0.380\n\n============================================================\nQuery: The curve is flattening; should I reposition?\n============================================================\nIntent: reduce_duration\nConfidence: 0.490\n\n============================================================\nQuery: My advisor said I’m too exposed. What adjustments should I consider?\n============================================================\nIntent: portfolio_analysis\nConfidence: 0.678\n\n============================================================\nQuery: Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\n============================================================\nIntent: sell_recommendation\nConfidence: 0.855\n\n============================================================\nQuery: Reduce my duration and give alternatives with at least 7.5% yield.\n============================================================\nIntent: increase_yield\nConfidence: 0.622\n\n============================================================\nQuery: Analyze my portfolio and tell me which low-yield bonds I should switch.\n============================================================\nIntent: switch_bonds\nConfidence: 0.822\n\n============================================================\nQuery: Before suggesting buys, what’s your outlook on corporate spreads?\n============================================================\nIntent: market_outlook\nConfidence: 0.897\n\n============================================================\nQuery: If I shift to shorter bonds, how will my yield be impacted?\n============================================================\nIntent: increase_yield\nConfidence: 0.191\n\n============================================================\nQuery: Is NTPC 2033 attractive at current spreads?\n============================================================\nIntent: credit_analysis\nConfidence: 0.383\n\n============================================================\nQuery: Are AA PSU bonds offering a good entry point?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.554\n\n============================================================\nQuery: Is this a good time to accumulate SDLs?\n============================================================\nIntent: market_outlook\nConfidence: 0.749\n\n============================================================\nQuery: Should I start adding exposure to long maturities?\n============================================================\nIntent: sell_recommendation\nConfidence: 0.354\n\n============================================================\nQuery: Are there better alternatives to ICICI 2029 with similar risk?\n============================================================\nIntent: switch_bonds\nConfidence: 0.950\n\n============================================================\nQuery: The market seems jumpy; how do I protect my portfolio?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.704\n\n============================================================\nQuery: If RBI hikes unexpectedly, which holdings will get hit the most?\n============================================================\nIntent: increase_yield\nConfidence: 0.224\n\n============================================================\nQuery: Rate volatility worries me—how do I reduce the impact?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.912\n\n============================================================\nQuery: How can I stabilize P&L swings in my portfolio?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.799\n\n============================================================\nQuery: Should I rebalance sectors before the credit cycle weakens?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.679\n\n============================================================\nQuery: Duration too high?\n============================================================\nIntent: reduce_duration\nConfidence: 0.784\n\n============================================================\nQuery: Better yield ideas?\n============================================================\nIntent: increase_yield\nConfidence: 0.939\n\n============================================================\nQuery: Switch or hold?\n============================================================\nIntent: switch_bonds\nConfidence: 0.836\n\n============================================================\nQuery: Cut risk?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.857\n\n============================================================\nQuery: Add PSU?\n============================================================\nIntent: barbell_strategy\nConfidence: 0.539\n\n============================================================\nQuery: Cut duration but don’t let yield drop below 7.6%.\n============================================================\nIntent: reduce_duration\nConfidence: 0.562\n\n============================================================\nQuery: I want high yield but without taking credit risk.\n============================================================\nIntent: increase_yield\nConfidence: 0.490\n\n============================================================\nQuery: Reduce risk but avoid selling anything.\n============================================================\nIntent: sell_recommendation\nConfidence: 0.340\n\n============================================================\nQuery: Switch out of low-yield bonds but keep duration same.\n============================================================\nIntent: switch_bonds\nConfidence: 0.912\n\n============================================================\nQuery: Increase return without increasing duration or credit risk.\n============================================================\nIntent: increase_yield\nConfidence: 0.634\n\n============================================================\nQuery: My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\n============================================================\nIntent: reduce_duration\nConfidence: 0.828\n\n============================================================\nQuery: My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.894\n\n============================================================\nQuery: I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\n============================================================\nIntent: sell_recommendation\nConfidence: 0.269\n\n============================================================\nQuery: I expect inflation to cool. Should I increase duration a bit?\n============================================================\nIntent: reduce_duration\nConfidence: 0.849\n\n============================================================\nQuery: Markets feel stable. Should I rotate sectors or focus on yield first?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.578\n\n============================================================\nQuery: How is the credit quality of PFC right now?\n============================================================\nIntent: credit_analysis\nConfidence: 0.952\n\n============================================================\nQuery: Is REC fundamentally strong enough for long-term holding?\n============================================================\nIntent: credit_analysis\nConfidence: 0.369\n\n============================================================\nQuery: What’s the default risk on NTPC?\n============================================================\nIntent: credit_analysis\nConfidence: 0.886\n\n============================================================\nQuery: Should I worry about credit spreads widening?\n============================================================\nIntent: market_outlook\nConfidence: 0.621\n\n============================================================\nQuery: Are AA- names safe in this environment?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.264\n\n============================================================\nQuery: Where do you see G-Sec yields in six months?\n============================================================\nIntent: market_outlook\nConfidence: 0.649\n\n============================================================\nQuery: If rates fall by 50 bps, what happens to long-duration PSU bonds?\n============================================================\nIntent: reduce_duration\nConfidence: 0.259\n\n============================================================\nQuery: Will corporate spreads tighten this year?\n============================================================\nIntent: market_outlook\nConfidence: 0.775\n\n============================================================\nQuery: Predict the movement of the 10-year benchmark.\n============================================================\nIntent: forecast_prices\nConfidence: 0.775\n\n============================================================\nQuery: How will a Fed cut affect Indian bond yields?\n============================================================\nIntent: market_outlook\nConfidence: 0.948\n\n============================================================\nTESTING COMPLETE!\n============================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==================== EXPORT SYNTHETIC DATA ONLY ====================\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport json\n\nprint(\"=\" * 60)\nprint(\"GENERATING & SAVING SYNTHETIC DATA ONLY\")\nprint(\"=\" * 60)\n\n# 1. Generate base synthetic dataset\ngenerator = SyntheticDataGenerator(CONFIG['num_samples_per_intent'])\ndataset = generator.generate_dataset()\n\n# 2. Augment (optional – same as training script)\ndataset = DataAugmenter.augment_dataset(dataset, CONFIG['augmentation_factor'])\nprint(f\"Total dataset size after augmentation: {len(dataset)} samples\\n\")\n\n# 3. Train/val/test split (same logic as train_model)\nintents = [s['intent'] for s in dataset]\ntrain_data, temp_data = train_test_split(\n    dataset, test_size=0.3, stratify=intents, random_state=42\n)\ntemp_intents = [s['intent'] for s in temp_data]\nval_data, test_data = train_test_split(\n    temp_data, test_size=0.5, stratify=temp_intents, random_state=42\n)\n\nprint(f\"Train: {len(train_data)}\")\nprint(f\"Val:   {len(val_data)}\")\nprint(f\"Test:  {len(test_data)}\\n\")\n\n# 4. Save to /kaggle/working as JSONL\nout_dir = Path(\"/kaggle/working/bond_query_data\")\nout_dir.mkdir(parents=True, exist_ok=True)\n\ndef save_jsonl(path: Path, rows):\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nsave_jsonl(out_dir / \"train.jsonl\", train_data)\nsave_jsonl(out_dir / \"val.jsonl\", val_data)\nsave_jsonl(out_dir / \"test.jsonl\", test_data)\n\nprint(\"✓ Saved datasets to:\")\nprint(f\"  {out_dir / 'train.jsonl'}\")\nprint(f\"  {out_dir / 'val.jsonl'}\")\nprint(f\"  {out_dir / 'test.jsonl'}\")\nprint(\"\\nTo download:\")\nprint(\"  Go to the 'Output' tab in Kaggle → download the 'bond_query_data' folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:43:35.100509Z","iopub.execute_input":"2025-11-30T06:43:35.100872Z","iopub.status.idle":"2025-11-30T06:43:35.622662Z","shell.execute_reply.started":"2025-11-30T06:43:35.100835Z","shell.execute_reply":"2025-11-30T06:43:35.621733Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING & SAVING SYNTHETIC DATA ONLY\n============================================================\n============================================================\nGENERATING SYNTHETIC DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1da5b11e597a488faa1293b87204a486"}},"metadata":{}},{"name":"stdout","text":"✓ Generated 13000 samples\n\n============================================================\nAUGMENTING DATA\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Augmenting:   0%|          | 0/6500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b796e0acc8f4382bdc59a964422f944"}},"metadata":{}},{"name":"stdout","text":"✓ Added 6500 augmented samples\n\nTotal dataset size after augmentation: 19500 samples\n\nTrain: 13650\nVal:   2925\nTest:  2925\n\n✓ Saved datasets to:\n  /kaggle/working/bond_query_data/train.jsonl\n  /kaggle/working/bond_query_data/val.jsonl\n  /kaggle/working/bond_query_data/test.jsonl\n\nTo download:\n  Go to the 'Output' tab in Kaggle → download the 'bond_query_data' folder.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Improving model through additional training","metadata":{}},{"cell_type":"code","source":"# ==================== HARD-CASE DATASET (manually curated) ====================\n\nHARD_CASES = [\n    # ---- Cluster A: reduce_duration vs increase_yield (duration is primary) ----\n    {\n        \"text\": \"Reduce my portfolio duration but keep my overall yield roughly the same.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": True, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Shorten duration across my holdings without dropping below 7.5% portfolio yield.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": True, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Cut my exposure to long bonds and move into shorter ones, but I still want a decent yield.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": True, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Help me move out of 10-year papers into 3-year ones while keeping yield close to current levels.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": True, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"I’m okay with slightly lower yield if it really reduces my duration risk.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n\n    # Opposite: yield is primary, duration as constraint\n    {\n        \"text\": \"Increase my portfolio yield but don’t extend duration beyond 5 years.\",\n        \"intent\": \"increase_yield\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Find higher-yield bonds with similar duration and credit risk to my current holdings.\",\n        \"intent\": \"increase_yield\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": True, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Boost my yield by switching into slightly lower-rated bonds but keep duration unchanged.\",\n        \"intent\": \"increase_yield\",\n        \"sectors\": [],\n        \"rating\": \"A\",\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n\n    # ---- Cluster B: direction of duration change (increase vs reduce) ----\n    {\n        \"text\": \"I think rates will fall, so I want to increase the duration of my bond portfolio.\",\n        \"intent\": \"buy_recommendation\",  # or a separate \"increase_duration\" if you had it\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"long\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Add some longer-maturity bonds to take advantage of a potential rate cut.\",\n        \"intent\": \"buy_recommendation\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"long\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"I expect inflation to cool, so please extend my duration a bit.\",\n        \"intent\": \"buy_recommendation\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"long\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Raise the average maturity of my holdings while keeping credit quality investment grade.\",\n        \"intent\": \"buy_recommendation\",\n        \"sectors\": [],\n        \"rating\": \"A\",\n        \"duration\": \"long\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": True, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n\n    # Reduce duration (clear negative direction)\n    {\n        \"text\": \"Rates might spike, so I want to cut my duration sharply.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Move me from long-duration bonds into short ones ahead of potential RBI hikes.\",\n        \"intent\": \"reduce_duration\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"short\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n\n    # ---- Cluster C: reduce risk but avoid selling (no sell_recommendation) ----\n    {\n        \"text\": \"Reduce my risk but avoid selling anything; I just want to hedge my portfolio.\",\n        \"intent\": \"hedge_volatility\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": True,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"I don’t want to liquidate any bonds, just soften the impact of rate moves.\",\n        \"intent\": \"hedge_volatility\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": True,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Keep my existing holdings but add positions that offset interest-rate risk.\",\n        \"intent\": \"hedge_volatility\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Protect my portfolio from volatility using hedges rather than selling bonds.\",\n        \"intent\": \"hedge_volatility\",\n        \"sectors\": [],\n        \"rating\": None,\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": True,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n\n    # ---- Cluster D: credit safety vs buy (credit_analysis vs buy_recommendation) ----\n    {\n        \"text\": \"Are AA- PSU bonds fundamentally safe right now from a credit perspective?\",\n        \"intent\": \"credit_analysis\",\n        \"sectors\": [\"PSU Energy\"],\n        \"rating\": \"AA\",\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": True, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Is it still safe to hold AA- names if the credit cycle weakens?\",\n        \"intent\": \"credit_analysis\",\n        \"sectors\": [],\n        \"rating\": \"AA\",\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": True, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"How risky are BBB rated bonds compared to AA in the current environment?\",\n        \"intent\": \"credit_analysis\",\n        \"sectors\": [],\n        \"rating\": \"BBB\",\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": True, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    {\n        \"text\": \"Given recent downgrades, is it prudent to add more exposure to AA- corporate bonds?\",\n        \"intent\": \"credit_analysis\",\n        \"sectors\": [\"Corporate\"],\n        \"rating\": \"AA\",\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": True, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n    # Contrast: explicit buy intent\n    {\n        \"text\": \"Suggest specific AA- bonds to buy if you think they are still fundamentally safe.\",\n        \"intent\": \"buy_recommendation\",\n        \"sectors\": [],\n        \"rating\": \"AA\",\n        \"duration\": \"medium\",\n        \"constraints\": {\"preserve_yield\": False, \"maintain_liquidity\": False,\n                        \"avoid_downgrades\": False, \"sector_diversity\": False,\n                        \"rating_above_aa\": False},\n    },\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:43:19.919925Z","iopub.execute_input":"2025-11-30T09:43:19.920583Z","iopub.status.idle":"2025-11-30T09:43:19.937242Z","shell.execute_reply.started":"2025-11-30T09:43:19.920546Z","shell.execute_reply":"2025-11-30T09:43:19.936615Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ==================== CONTINUATION TRAINING ON HARD CASES ====================\n\ndef continue_training_on_hard_cases(\n    model_path: str = \"/kaggle/working/bond_classifier_v3\",\n    base_model: str = \"microsoft/deberta-v3-small\",\n    epochs: int = 3,\n    lr: float = 5e-6,\n    batch_size: int = 8,\n):\n    \"\"\"\n    Continue training the already trained model on the HARD_CASES dataset only.\n    Does NOT rebuild synthetic/LLM data. Small LR, a few epochs.\n    \"\"\"\n\n    print(\"=\" * 60)\n    print(\"CONTINUATION TRAINING ON HARD CASES\")\n    print(\"=\" * 60)\n    print(f\"Loading tokenizer & model from: {model_path}\")\n\n    # 1) Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # 2) Build small dataset from HARD_CASES\n    hard_dataset = BondQueryDataset(HARD_CASES, tokenizer, CONFIG[\"max_length\"])\n    hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n\n    # 3) Load existing model checkpoint\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ProductionBondClassifier(base_model)  # will use DeBERTa-v3-small\n    state_dict = torch.load(os.path.join(model_path, \"pytorch_model.bin\"), map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device)\n\n    print(f\"✓ Loaded model for continuation training on {len(hard_dataset)} hard examples\")\n    print(f\"Using device: {device}\")\n\n    # 4) Optimizer & loss (small LR)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)\n    criterion = MultiTaskLoss()\n\n    # 5) Simple training loop (no scheduler/val, dataset is tiny)\n    model.train()\n    for epoch in range(1, epochs + 1):\n        print(f\"\\n--- Hard-case Epoch {epoch}/{epochs} ---\")\n        epoch_loss = 0.0\n        all_preds, all_labels = [], []\n\n        pbar = tqdm(hard_loader, desc=f\"Hard-case Epoch {epoch}\")\n        for batch in pbar:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = {k: v.to(device) for k, v in batch.items()\n                      if k not in [\"input_ids\", \"attention_mask\"]}\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss_dict = criterion(outputs, labels)\n            loss = loss_dict[\"total\"]\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            preds = outputs[\"intent_logits\"].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels[\"intent_label\"].cpu().numpy())\n\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_loss = epoch_loss / len(hard_loader)\n        acc = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        print(f\"Hard-case Epoch {epoch} - Loss: {avg_loss:.4f} | Acc: {acc:.3f} | F1: {f1:.3f}\")\n\n    # 6) Save updated weights\n    out_path = os.path.join(model_path, \"pytorch_model_hard.bin\")\n    torch.save(model.state_dict(), out_path)\n    print(f\"\\n✓ Hard-case fine-tuned model saved to: {out_path}\")\n    print(\"You can point your inference notebook to this file if desired.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:43:28.745336Z","iopub.execute_input":"2025-11-30T09:43:28.745656Z","iopub.status.idle":"2025-11-30T09:43:28.756136Z","shell.execute_reply.started":"2025-11-30T09:43:28.745634Z","shell.execute_reply":"2025-11-30T09:43:28.755522Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"continue_training_on_hard_cases(\n    model_path=\"/kaggle/working/bond_classifier_v3\",\n    base_model=\"microsoft/deberta-v3-small\",\n    epochs=10,\n    lr=5e-6,\n    batch_size=8,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:44:14.029806Z","iopub.execute_input":"2025-11-30T09:44:14.030272Z","iopub.status.idle":"2025-11-30T09:44:22.576847Z","shell.execute_reply.started":"2025-11-30T09:44:14.030245Z","shell.execute_reply":"2025-11-30T09:44:22.575925Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCONTINUATION TRAINING ON HARD CASES\n============================================================\nLoading tokenizer & model from: /kaggle/working/bond_classifier_v3\n✓ Loaded model for continuation training on 23 hard examples\nUsing device: cuda\n\n--- Hard-case Epoch 1/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 1:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb5d080718924e8e856ad9abc5c24966"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 1 - Loss: 1.5951 | Acc: 0.696 | F1: 0.551\n\n--- Hard-case Epoch 2/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 2:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d434a53d7c3d46d2a3eee84ca9b81372"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 2 - Loss: 1.5565 | Acc: 0.565 | F1: 0.472\n\n--- Hard-case Epoch 3/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 3:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e43c288d054462795be3dbc994ac82b"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 3 - Loss: 1.3535 | Acc: 0.696 | F1: 0.457\n\n--- Hard-case Epoch 4/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 4:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bfc83a759f7493ab7eaea4ab185caa1"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 4 - Loss: 1.1646 | Acc: 0.826 | F1: 0.615\n\n--- Hard-case Epoch 5/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 5:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d30df3a085004ebabe3e327d275392da"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 5 - Loss: 0.9765 | Acc: 0.739 | F1: 0.641\n\n--- Hard-case Epoch 6/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 6:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a87194c453d42c181dfe5475f1dabca"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 6 - Loss: 0.7686 | Acc: 0.870 | F1: 0.870\n\n--- Hard-case Epoch 7/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 7:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ded21f5eb1949d6bc1b002d76cf4f8b"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 7 - Loss: 0.5777 | Acc: 0.870 | F1: 0.880\n\n--- Hard-case Epoch 8/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 8:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22c179f3368f47bfb728c557c107af9f"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 8 - Loss: 0.5945 | Acc: 0.870 | F1: 0.758\n\n--- Hard-case Epoch 9/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 9:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e873ed7345174c85b4615613f9532faa"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 9 - Loss: 0.4301 | Acc: 0.957 | F1: 0.953\n\n--- Hard-case Epoch 10/10 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Hard-case Epoch 10:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48317fd849e04643a8a4ffa094aee358"}},"metadata":{}},{"name":"stdout","text":"Hard-case Epoch 10 - Loss: 0.3909 | Acc: 1.000 | F1: 1.000\n\n✓ Hard-case fine-tuned model saved to: /kaggle/working/bond_classifier_v3/pytorch_model_hard.bin\nYou can point your inference notebook to this file if desired.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Hard-Finetuned Model Evaluation Notebook\n================================================================\n\nSetup:\n1. In the training notebook, run `continue_training_on_hard_cases(...)`\n   so that it saves: /kaggle/working/bond_classifier_v3/pytorch_model_hard.bin\n2. In this NEW Kaggle notebook:\n   - Attach the training output as a dataset (if needed), OR\n   - Run in the same environment where /kaggle/working/bond_classifier_v3 exists.\n3. Update MODEL_PATH below if necessary.\n4. Run!\n\"\"\"\n\n# ==================== INSTALL & IMPORTS ====================\nimport sys\nimport subprocess\n\nprint(\"Installing dependencies...\")\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"transformers\", \"accelerate\", \"scikit-learn\"\n])\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport numpy as np\nimport os\n\nprint(\"✓ Imports complete!\\n\")\n\n\n# ==================== MODEL ARCHITECTURE ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Same architecture as training: DeBERTa-v3-small + multi-task heads.\"\"\"\n    \n    def __init__(self, base_model: str = 'microsoft/deberta-v3-small', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # Heads: intent + sectors + rating + duration + constraints\n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== ENUMS & RESULT TYPE ====================\n\nclass QueryIntent(str, Enum):\n    BUY_RECOMMENDATION = \"buy_recommendation\"\n    SELL_RECOMMENDATION = \"sell_recommendation\"\n    PORTFOLIO_ANALYSIS = \"portfolio_analysis\"\n    REDUCE_DURATION = \"reduce_duration\"\n    INCREASE_YIELD = \"increase_yield\"\n    HEDGE_VOLATILITY = \"hedge_volatility\"\n    SECTOR_REBALANCE = \"sector_rebalance\"\n    BARBELL_STRATEGY = \"barbell_strategy\"\n    SWITCH_BONDS = \"switch_bonds\"\n    EXPLAIN_RECOMMENDATION = \"explain_recommendation\"\n    MARKET_OUTLOOK = \"market_outlook\"\n    CREDIT_ANALYSIS = \"credit_analysis\"\n    FORECAST_PRICES = \"forecast_prices\"\n\n\n@dataclass\nclass ClassificationResult:\n    intent: str\n    confidence: float\n    filters: Dict[str, Any]\n    constraints: Dict[str, bool]\n\n\n# ==================== CLASSIFIER WRAPPER ====================\n\nclass BondClassifier:\n    \"\"\"Production classifier wrapper with MC-Dropout + filter extraction.\"\"\"\n    \n    def __init__(\n        self,\n        model_path: str,\n        base_model: str = \"microsoft/deberta-v3-small\",\n        weight_file: str = \"pytorch_model_hard.bin\"  # <-- use hard-finetuned weights\n    ):\n        \"\"\"\n        Load model from Kaggle path.\n        \n        Args:\n            model_path: path to folder containing weights + tokenizer files,\n                        e.g. '/kaggle/working/bond_classifier_v3'\n            base_model: HF base model name used in training\n            weight_file: which checkpoint to load\n        \"\"\"\n        print(f\"Loading model from: {model_path}\")\n        print(f\"Using weights file: {weight_file}\")\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        \n        # Load tokenizer saved during training\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        # Load model with same architecture as training\n        self.model = ProductionBondClassifier(base_model=base_model)\n        weight_path = os.path.join(model_path, weight_file)\n        if not os.path.exists(weight_path):\n            raise FileNotFoundError(f\"Checkpoint not found at {weight_path}\")\n        \n        state_dict = torch.load(weight_path, map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n        \n        # Label mappings\n        self.intent_names = [\n            'buy_recommendation', 'sell_recommendation', 'portfolio_analysis',\n            'reduce_duration', 'increase_yield', 'hedge_volatility',\n            'sector_rebalance', 'barbell_strategy', 'switch_bonds',\n            'explain_recommendation', 'market_outlook', 'credit_analysis',\n            'forecast_prices'\n        ]\n        \n        self.sector_names = [\n            'Sovereign', 'PSU Energy', 'Financial', 'Corporate', \n            'Infrastructure', 'NBFC', 'Banking'\n        ]\n        self.rating_names = ['AAA', 'AA+', 'AA', 'A+', 'A', 'BBB', 'Unrated']\n        self.duration_names = ['short', 'medium', 'long']\n        \n        print(\"✓ Model loaded successfully!\\n\")\n    \n    def classify(self, query: str, num_samples: int = 5) -> ClassificationResult:\n        \"\"\"\n        Classify query with MC-Dropout uncertainty.\n        \n        Args:\n            query: User query string\n            num_samples: Number of stochastic forward passes\n        \"\"\"\n        # Tokenize\n        enc = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        input_ids = enc[\"input_ids\"].to(self.device)\n        attention_mask = enc[\"attention_mask\"].to(self.device)\n        \n        # --- MC Dropout on intent head ---\n        self.model.train()  # enable dropout\n        intent_predictions = []\n        \n        with torch.no_grad():\n            for _ in range(num_samples):\n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                intent_probs = F.softmax(outputs['intent_logits'], dim=-1)\n                intent_predictions.append(intent_probs.cpu())\n        \n        # Aggregate\n        intent_mean = torch.stack(intent_predictions).mean(dim=0)[0]\n        confidence = intent_mean.max().item()\n        predicted_idx = intent_mean.argmax().item()\n        \n        # Final pass (deterministic) for other heads\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            sectors = self._extract_sectors(outputs['sector_logits'])\n            rating = self._extract_rating(outputs['rating_logits'])\n            duration = self._extract_duration(outputs['duration_logits'])\n            constraints = self._extract_constraints(outputs['constraint_logits'])\n        \n        return ClassificationResult(\n            intent=self.intent_names[predicted_idx],\n            confidence=confidence,\n            filters={\n                'sectors': sectors,\n                'min_rating': rating,\n                'duration_preference': duration\n            },\n            constraints=constraints\n        )\n    \n    def _extract_sectors(self, logits: torch.Tensor) -> List[str]:\n        \"\"\"Multi-label sector head with simple sigmoid threshold.\"\"\"\n        probs = torch.sigmoid(logits[0])\n        indices = (probs > 0.4).nonzero(as_tuple=True)[0]\n        return [self.sector_names[i] for i in indices]\n    \n    def _extract_rating(self, logits: torch.Tensor) -> Optional[str]:\n        \"\"\"Map argmax of 7-way rating head to rating string.\"\"\"\n        idx = logits[0].argmax().item()\n        if 0 <= idx < len(self.rating_names):\n            return self.rating_names[idx]\n        return None\n    \n    def _extract_duration(self, logits: torch.Tensor) -> str:\n        \"\"\"Map argmax of 3-way duration head to short/medium/long.\"\"\"\n        idx = logits[0].argmax().item()\n        if 0 <= idx < len(self.duration_names):\n            return self.duration_names[idx]\n        return \"medium\"\n    \n    def _extract_constraints(self, logits: torch.Tensor) -> Dict[str, bool]:\n        \"\"\"Binary constraint flags from 5-way sigmoid head.\"\"\"\n        probs = torch.sigmoid(logits[0])\n        return {\n            'preserve_yield': probs[0].item() > 0.5,\n            'maintain_liquidity': probs[1].item() > 0.5,\n            'avoid_downgrades': probs[2].item() > 0.5,\n            'sector_diversity': probs[3].item() > 0.5,\n            'rating_above_aa': probs[4].item() > 0.5\n        }\n    \n    def batch_classify(self, queries: List[str]) -> List[ClassificationResult]:\n        \"\"\"Classify multiple queries.\"\"\"\n        return [self.classify(q) for q in queries]\n\n\n# ==================== EVAL SCRIPT ====================\n\nTEST_QUERIES = [\n    # --- Category 1: Ambiguous Buy/Sell/Strategy ---\n    \"Should I shift my SDL holdings into shorter PSU bonds or just hold?\",\n    \"I want to reduce rate risk but I don’t want my yield to fall. What should I do?\",\n    \"Is it better to exit long-duration bonds and move into AA corporates?\",\n    \"Would switching from NTPC 2035 to REC 2030 improve my return?\",\n    \"Which reduces risk more: selling perpetuals or adding short-term G-Secs?\",\n\n    # --- Category 2: Disguised intents ---\n    \"My yield looks weak lately… what should I change?\",\n    \"These long papers are stressing me out—what’s the safest move?\",\n    \"My portfolio feels too boring. Suggest something with more kick.\",\n    \"The curve is flattening; should I reposition?\",\n    \"My advisor said I’m too exposed. What adjustments should I consider?\",\n\n    # --- Category 3: Multi-intent queries ---\n    \"Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\",\n    \"Reduce my duration and give alternatives with at least 7.5% yield.\",\n    \"Analyze my portfolio and tell me which low-yield bonds I should switch.\",\n    \"Before suggesting buys, what’s your outlook on corporate spreads?\",\n    \"If I shift to shorter bonds, how will my yield be impacted?\",\n\n    # --- Category 4: Tricky phrasing for buy intent ---\n    \"Is NTPC 2033 attractive at current spreads?\",\n    \"Are AA PSU bonds offering a good entry point?\",\n    \"Is this a good time to accumulate SDLs?\",\n    \"Should I start adding exposure to long maturities?\",\n    \"Are there better alternatives to ICICI 2029 with similar risk?\",\n\n    # --- Category 5: Risk management intents ---\n    \"The market seems jumpy; how do I protect my portfolio?\",\n    \"If RBI hikes unexpectedly, which holdings will get hit the most?\",\n    \"Rate volatility worries me—how do I reduce the impact?\",\n    \"How can I stabilize P&L swings in my portfolio?\",\n    \"Should I rebalance sectors before the credit cycle weakens?\",\n\n    # --- Category 6: Very short hard queries ---\n    \"Duration too high?\",\n    \"Better yield ideas?\",\n    \"Switch or hold?\",\n    \"Cut risk?\",\n    \"Add PSU?\",\n\n    # --- Category 7: Contradictory constraints ---\n    \"Cut duration but don’t let yield drop below 7.6%.\",\n    \"I want high yield but without taking credit risk.\",\n    \"Reduce risk but avoid selling anything.\",\n    \"Switch out of low-yield bonds but keep duration same.\",\n    \"Increase return without increasing duration or credit risk.\",\n\n    # --- Category 8: Multi-sentence queries ---\n    \"My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\",\n    \"My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\",\n    \"I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\",\n    \"I expect inflation to cool. Should I increase duration a bit?\",\n    \"Markets feel stable. Should I rotate sectors or focus on yield first?\",\n\n    # --- Category 9: Credit-analysis queries ---\n    \"How is the credit quality of PFC right now?\",\n    \"Is REC fundamentally strong enough for long-term holding?\",\n    \"What’s the default risk on NTPC?\",\n    \"Should I worry about credit spreads widening?\",\n    \"Are AA- names safe in this environment?\",\n\n    # --- Category 10: Forecast / outlook queries ---\n    \"Where do you see G-Sec yields in six months?\",\n    \"If rates fall by 50 bps, what happens to long-duration PSU bonds?\",\n    \"Will corporate spreads tighten this year?\",\n    \"Predict the movement of the 10-year benchmark.\",\n    \"How will a Fed cut affect Indian bond yields?\",\n]\n\n\ndef evaluate_hard_model():\n    \"\"\"Evaluate the hard-case fine-tuned model on the tough query set.\"\"\"\n    \n    MODEL_PATH = '/kaggle/working/bond_classifier_v3'\n    # If using as a separate notebook with attached dataset, update to:\n    # MODEL_PATH = '/kaggle/input/bond-classifier-v3/bond_classifier_v3'\n    \n    print(\"=\" * 60)\n    print(\"LOADING HARD-FINETUNED CLASSIFIER\")\n    print(\"=\" * 60)\n    \n    try:\n        classifier = BondClassifier(\n            MODEL_PATH,\n            base_model=\"microsoft/deberta-v3-small\",\n            weight_file=\"pytorch_model_hard.bin\"\n        )\n    except Exception as e:\n        print(f\"❌ Error loading model: {e}\")\n        print(\"\\nMake sure:\")\n        print(\"1. Hard-case fine-tuning saved pytorch_model_hard.bin\")\n        print(\"2. MODEL_PATH is correct.\")\n        return\n    \n    print(\"=\" * 60)\n    print(\"TESTING QUERIES (HARD-FINETUNED MODEL)\")\n    print(\"=\" * 60)\n    \n    for query in TEST_QUERIES:\n        result = classifier.classify(query)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Query: {query}\")\n        print(f\"{'='*60}\")\n        print(f\"Intent: {result.intent}\")\n        print(f\"Confidence: {result.confidence:.3f}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"HARD-FINETUNED MODEL EVALUATION COMPLETE!\")\n    print(\"=\" * 60)\n\n\n# ==================== RUN ====================\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BOND QUERY CLASSIFIER - HARD MODEL EVAL\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    evaluate_hard_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:50:46.014908Z","iopub.execute_input":"2025-11-30T09:50:46.015751Z","iopub.status.idle":"2025-11-30T09:50:55.360567Z","shell.execute_reply.started":"2025-11-30T09:50:46.015723Z","shell.execute_reply":"2025-11-30T09:50:55.359955Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\n✓ Imports complete!\n\n\n============================================================\nBOND QUERY CLASSIFIER - HARD MODEL EVAL\n============================================================\n\n============================================================\nLOADING HARD-FINETUNED CLASSIFIER\n============================================================\nLoading model from: /kaggle/working/bond_classifier_v3\nUsing weights file: pytorch_model_hard.bin\nUsing device: cuda\n✓ Model loaded successfully!\n\n============================================================\nTESTING QUERIES (HARD-FINETUNED MODEL)\n============================================================\n\n============================================================\nQuery: Should I shift my SDL holdings into shorter PSU bonds or just hold?\n============================================================\nIntent: reduce_duration\nConfidence: 0.443\n\n============================================================\nQuery: I want to reduce rate risk but I don’t want my yield to fall. What should I do?\n============================================================\nIntent: reduce_duration\nConfidence: 0.671\n\n============================================================\nQuery: Is it better to exit long-duration bonds and move into AA corporates?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.181\n\n============================================================\nQuery: Would switching from NTPC 2035 to REC 2030 improve my return?\n============================================================\nIntent: switch_bonds\nConfidence: 0.409\n\n============================================================\nQuery: Which reduces risk more: selling perpetuals or adding short-term G-Secs?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.255\n\n============================================================\nQuery: My yield looks weak lately… what should I change?\n============================================================\nIntent: increase_yield\nConfidence: 0.226\n\n============================================================\nQuery: These long papers are stressing me out—what’s the safest move?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.405\n\n============================================================\nQuery: My portfolio feels too boring. Suggest something with more kick.\n============================================================\nIntent: switch_bonds\nConfidence: 0.457\n\n============================================================\nQuery: The curve is flattening; should I reposition?\n============================================================\nIntent: reduce_duration\nConfidence: 0.723\n\n============================================================\nQuery: My advisor said I’m too exposed. What adjustments should I consider?\n============================================================\nIntent: portfolio_analysis\nConfidence: 0.581\n\n============================================================\nQuery: Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\n============================================================\nIntent: sell_recommendation\nConfidence: 0.575\n\n============================================================\nQuery: Reduce my duration and give alternatives with at least 7.5% yield.\n============================================================\nIntent: increase_yield\nConfidence: 0.579\n\n============================================================\nQuery: Analyze my portfolio and tell me which low-yield bonds I should switch.\n============================================================\nIntent: switch_bonds\nConfidence: 0.557\n\n============================================================\nQuery: Before suggesting buys, what’s your outlook on corporate spreads?\n============================================================\nIntent: market_outlook\nConfidence: 0.879\n\n============================================================\nQuery: If I shift to shorter bonds, how will my yield be impacted?\n============================================================\nIntent: market_outlook\nConfidence: 0.245\n\n============================================================\nQuery: Is NTPC 2033 attractive at current spreads?\n============================================================\nIntent: credit_analysis\nConfidence: 0.518\n\n============================================================\nQuery: Are AA PSU bonds offering a good entry point?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.714\n\n============================================================\nQuery: Is this a good time to accumulate SDLs?\n============================================================\nIntent: market_outlook\nConfidence: 0.840\n\n============================================================\nQuery: Should I start adding exposure to long maturities?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.463\n\n============================================================\nQuery: Are there better alternatives to ICICI 2029 with similar risk?\n============================================================\nIntent: switch_bonds\nConfidence: 0.945\n\n============================================================\nQuery: The market seems jumpy; how do I protect my portfolio?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.900\n\n============================================================\nQuery: If RBI hikes unexpectedly, which holdings will get hit the most?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.405\n\n============================================================\nQuery: Rate volatility worries me—how do I reduce the impact?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.941\n\n============================================================\nQuery: How can I stabilize P&L swings in my portfolio?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.875\n\n============================================================\nQuery: Should I rebalance sectors before the credit cycle weakens?\n============================================================\nIntent: credit_analysis\nConfidence: 0.357\n\n============================================================\nQuery: Duration too high?\n============================================================\nIntent: reduce_duration\nConfidence: 0.500\n\n============================================================\nQuery: Better yield ideas?\n============================================================\nIntent: increase_yield\nConfidence: 0.928\n\n============================================================\nQuery: Switch or hold?\n============================================================\nIntent: switch_bonds\nConfidence: 0.646\n\n============================================================\nQuery: Cut risk?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.697\n\n============================================================\nQuery: Add PSU?\n============================================================\nIntent: barbell_strategy\nConfidence: 0.480\n\n============================================================\nQuery: Cut duration but don’t let yield drop below 7.6%.\n============================================================\nIntent: reduce_duration\nConfidence: 0.799\n\n============================================================\nQuery: I want high yield but without taking credit risk.\n============================================================\nIntent: increase_yield\nConfidence: 0.445\n\n============================================================\nQuery: Reduce risk but avoid selling anything.\n============================================================\nIntent: hedge_volatility\nConfidence: 0.424\n\n============================================================\nQuery: Switch out of low-yield bonds but keep duration same.\n============================================================\nIntent: switch_bonds\nConfidence: 0.773\n\n============================================================\nQuery: Increase return without increasing duration or credit risk.\n============================================================\nIntent: increase_yield\nConfidence: 0.452\n\n============================================================\nQuery: My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\n============================================================\nIntent: reduce_duration\nConfidence: 0.688\n\n============================================================\nQuery: My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.831\n\n============================================================\nQuery: I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.640\n\n============================================================\nQuery: I expect inflation to cool. Should I increase duration a bit?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.350\n\n============================================================\nQuery: Markets feel stable. Should I rotate sectors or focus on yield first?\n============================================================\nIntent: increase_yield\nConfidence: 0.183\n\n============================================================\nQuery: How is the credit quality of PFC right now?\n============================================================\nIntent: credit_analysis\nConfidence: 0.953\n\n============================================================\nQuery: Is REC fundamentally strong enough for long-term holding?\n============================================================\nIntent: credit_analysis\nConfidence: 0.386\n\n============================================================\nQuery: What’s the default risk on NTPC?\n============================================================\nIntent: credit_analysis\nConfidence: 0.932\n\n============================================================\nQuery: Should I worry about credit spreads widening?\n============================================================\nIntent: credit_analysis\nConfidence: 0.447\n\n============================================================\nQuery: Are AA- names safe in this environment?\n============================================================\nIntent: credit_analysis\nConfidence: 0.649\n\n============================================================\nQuery: Where do you see G-Sec yields in six months?\n============================================================\nIntent: market_outlook\nConfidence: 0.603\n\n============================================================\nQuery: If rates fall by 50 bps, what happens to long-duration PSU bonds?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.238\n\n============================================================\nQuery: Will corporate spreads tighten this year?\n============================================================\nIntent: market_outlook\nConfidence: 0.814\n\n============================================================\nQuery: Predict the movement of the 10-year benchmark.\n============================================================\nIntent: forecast_prices\nConfidence: 0.758\n\n============================================================\nQuery: How will a Fed cut affect Indian bond yields?\n============================================================\nIntent: market_outlook\nConfidence: 0.946\n\n============================================================\nHARD-FINETUNED MODEL EVALUATION COMPLETE!\n============================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ==================== CELL 1: GEMINI EDGE-CASE DATA GENERATION ====================\n\nfrom google import genai\nfrom pathlib import Path\nimport json\nimport os\n\nEDGE_CASE_OUTPUT = \"/kaggle/working/edge_cases.jsonl\"\n\nclass EdgeCaseDataGenerator:\n    \"\"\"\n    Generate 'hard' edge-case examples for the bond intent classifier using Gemini.\n    Focus areas:\n      - reduce_duration vs increase_yield (both mentioned)\n      - increase vs reduce duration\n      - reduce risk but avoid selling (no sell_recommendation)\n      - credit safety vs buy_recommendation\n      - what-if / analytics questions (not pure buy/sell)\n    \"\"\"\n\n    def __init__(self, model_name: str = \"gemini-2.0-flash\", total_examples: int = 80):\n        api_key = os.environ.get(\"GEMINI_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\n                \"GEMINI_API_KEY not set. \"\n                \"Set it earlier with os.environ['GEMINI_API_KEY'] = '...'\"\n            )\n        self.client = genai.Client(api_key=api_key)\n        self.model_name = model_name\n        self.total_examples = total_examples\n\n        # Valid intents\n        self.allowed_intents = [\n            \"buy_recommendation\",\n            \"sell_recommendation\",\n            \"portfolio_analysis\",\n            \"reduce_duration\",\n            \"increase_yield\",\n            \"hedge_volatility\",\n            \"sector_rebalance\",\n            \"barbell_strategy\",\n            \"switch_bonds\",\n            \"explain_recommendation\",\n            \"market_outlook\",\n            \"credit_analysis\",\n            \"forecast_prices\",\n        ]\n        self.allowed_sectors = [\n            \"Sovereign\", \"PSU Energy\", \"Financial\",\n            \"Corporate\", \"Infrastructure\", \"NBFC\", \"Banking\"\n        ]\n        self.allowed_ratings = [\"AAA\", \"AA+\", \"AA\", \"A+\", \"A\", \"BBB\"]\n        self.allowed_durations = [\"short\", \"medium\", \"long\"]\n\n    def generate_edge_cases(self) -> list[dict]:\n        \"\"\"\n        Ask Gemini for a single batch of diverse edge-case examples.\n        \"\"\"\n        print(\"=\" * 60)\n        print(\"GENERATING EDGE-CASE DATA WITH GEMINI\")\n        print(\"=\" * 60)\n\n        system_msg = (\n            \"You are an expert bond investment assistant generating edge-case \"\n            \"queries for intent classification training. The classifier must decide \"\n            \"between similar intents such as reduce_duration vs increase_yield, \"\n            \"hedge_volatility vs sell_recommendation vs portfolio_analysis, \"\n            \"credit_analysis vs buy_recommendation, and analytics/what-if questions \"\n            \"vs pure advisory.\"\n        )\n\n        user_prompt = f\"\"\"\nGenerate {self.total_examples} diverse, realistic user queries for a bond assistant that are\nINTENTIONALLY AMBIGUOUS or lie on the boundary between related intents.\n\nFor EACH example, output ONE JSON object per line (JSON Lines format). \nDO NOT wrap in an array. DO NOT add commentary or code fences.\n\nValid intents (intent MUST be exactly one of these):\n- buy_recommendation\n- sell_recommendation\n- portfolio_analysis\n- reduce_duration\n- increase_yield\n- hedge_volatility\n- sector_rebalance\n- barbell_strategy\n- switch_bonds\n- explain_recommendation\n- market_outlook\n- credit_analysis\n- forecast_prices\n\nFollow these patterns:\n\n1) Duration vs yield (reduce_duration vs increase_yield)\n   - Queries that mention BOTH duration and yield.\n   - Some where reduce_duration is primary (yield is a constraint).\n   - Some where increase_yield is primary (duration is a constraint).\n   - Label with reduce_duration when the main aim is cutting duration.\n   - Label with increase_yield when the main aim is boosting yield.\n\n2) Direction of duration (increase vs reduce)\n   - Queries that clearly want to INCREASE duration (longer maturities) because of rate cut expectations.\n   - Label those as buy_recommendation or market_outlook (not reduce_duration).\n   - Queries that clearly want to REDUCE duration ahead of rate hikes.\n   - Label those as reduce_duration.\n\n3) Reduce risk but avoid selling\n   - Queries that say \"reduce risk\" or \"protect portfolio\" but explicitly say \"avoid selling\" or \"do not sell\".\n   - Label these as hedge_volatility or reduce_duration, NOT sell_recommendation.\n\n4) Credit safety vs buying\n   - Queries about \"are AA- names safe\", \"is this issuer safe\", \"credit risk in this environment\".\n   - Label those as credit_analysis.\n   - Separate queries that explicitly ask for bonds to BUY if credit is safe -> label buy_recommendation.\n\n5) Analytics / what-if questions\n   - Queries like \"if RBI hikes, which holdings get hit most?\", \"if rates fall 50 bps, what happens to long bonds?\"\n   - These should test whether the classifier can distinguish analysis from pure buy/sell.\n   - Label as portfolio_analysis, market_outlook, or forecast_prices (not buy/sell).\n\nFor EACH example, output:\n\n{{\n  \"text\": \"... user query ...\",\n  \"intent\": \"<one of valid intents>\",\n  \"sectors\": [ ... zero or more from [\"Sovereign\",\"PSU Energy\",\"Financial\",\"Corporate\",\"Infrastructure\",\"NBFC\",\"Banking\"] ... ],\n  \"rating\": \"<AAA|AA+|AA|A+|A|BBB>\" or null,\n  \"duration\": \"<short|medium|long>\",\n  \"constraints\": {{\n      \"preserve_yield\": true/false,\n      \"maintain_liquidity\": true/false,\n      \"avoid_downgrades\": true/false,\n      \"sector_diversity\": true/false,\n      \"rating_above_aa\": true/false\n  }}\n}}\n\nGuidelines:\n- Mix short and multi-sentence queries.\n- Vary tone (retail, PM, brief, detailed).\n- Keep everything in the context of BONDS (not equities/crypto).\n\"\"\"\n\n        full_prompt = system_msg + \"\\n\\n\" + user_prompt\n\n        resp = self.client.models.generate_content(\n            model=self.model_name,\n            contents=full_prompt,\n        )\n\n        raw = resp.text or \"\"\n        samples = self._parse_and_normalize(raw)\n        print(f\"\\n✓ Parsed {len(samples)} valid edge-case examples\")\n        return samples\n\n    def _parse_and_normalize(self, raw: str) -> list[dict]:\n        samples = []\n        for line in raw.splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            if line.startswith(\"```\"):\n                continue\n            if line.startswith(\"-\"):\n                line = line.lstrip(\"-\").strip()\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n\n            sample = self._normalize_record(obj)\n            if sample is not None:\n                samples.append(sample)\n        return samples\n\n    def _normalize_record(self, obj: dict) -> Optional[dict]:\n        text = str(obj.get(\"text\", \"\")).strip()\n        if not text:\n            return None\n\n        intent = obj.get(\"intent\")\n        if intent not in self.allowed_intents:\n            return None\n\n        # sectors\n        raw_sectors = obj.get(\"sectors\") or []\n        sectors = []\n        if isinstance(raw_sectors, list):\n            for s in raw_sectors:\n                s = str(s).strip()\n                if s in self.allowed_sectors and s not in sectors:\n                    sectors.append(s)\n\n        # rating\n        rating = obj.get(\"rating\")\n        if rating is not None:\n            r = str(rating).strip()\n            rating_norm = r if r in self.allowed_ratings else None\n        else:\n            rating_norm = None\n\n        # duration\n        duration = str(obj.get(\"duration\", \"medium\")).strip().lower()\n        if duration not in self.allowed_durations:\n            duration = \"medium\"\n\n        # constraints\n        raw_constraints = obj.get(\"constraints\") or {}\n        constraints_defaults = {\n            \"preserve_yield\": False,\n            \"maintain_liquidity\": False,\n            \"avoid_downgrades\": False,\n            \"sector_diversity\": False,\n            \"rating_above_aa\": False,\n        }\n        if isinstance(raw_constraints, dict):\n            for k in list(constraints_defaults.keys()):\n                if k in raw_constraints:\n                    constraints_defaults[k] = bool(raw_constraints[k])\n\n        if rating_norm in (\"AAA\", \"AA+\", \"AA\"):\n            constraints_defaults[\"rating_above_aa\"] = True\n\n        return {\n            \"text\": text,\n            \"intent\": intent,\n            \"sectors\": sectors,\n            \"rating\": rating_norm,\n            \"duration\": duration,\n            \"constraints\": constraints_defaults,\n        }\n\n\n# ---- Run generation & save JSONL ----\nedge_gen = EdgeCaseDataGenerator(model_name=CONFIG[\"llm_model\"], total_examples=80)\nedge_cases = edge_gen.generate_edge_cases()\n\nedge_path = Path(EDGE_CASE_OUTPUT)\nedge_path.parent.mkdir(parents=True, exist_ok=True)\nwith edge_path.open(\"w\", encoding=\"utf-8\") as f:\n    for ex in edge_cases:\n        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n\nprint(f\"\\n✓ Edge-case dataset saved to: {edge_path}\")\nprint(\"You can inspect/download it from the 'Output' tab.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:02:11.847190Z","iopub.execute_input":"2025-11-30T10:02:11.848026Z","iopub.status.idle":"2025-11-30T10:02:52.595717Z","shell.execute_reply.started":"2025-11-30T10:02:11.847994Z","shell.execute_reply":"2025-11-30T10:02:52.595064Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING EDGE-CASE DATA WITH GEMINI\n============================================================\n\n✓ Parsed 90 valid edge-case examples\n\n✓ Edge-case dataset saved to: /kaggle/working/edge_cases.jsonl\nYou can inspect/download it from the 'Output' tab.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ==================== CELL 2: CONTINUE TRAINING ON EDGE CASES ====================\n\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\n\nEDGE_CASE_OUTPUT = \"/kaggle/working/edge_cases.jsonl\"\n\ndef load_edge_cases(path: str) -> List[Dict]:\n    data = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            data.append(obj)\n    print(f\"Loaded {len(data)} edge-case examples from {path}\")\n    return data\n\n\ndef continue_training_on_edge_cases(\n    model_path: str = \"/kaggle/working/bond_classifier_v3\",\n    base_model: str = \"microsoft/deberta-v3-small\",\n    edge_path: str = EDGE_CASE_OUTPUT,\n    epochs: int = 3,\n    lr: float = 5e-6,\n    batch_size: int = 8,\n    out_weight_file: str = \"pytorch_model_edge.bin\",\n):\n    \"\"\"\n    Continue training the already trained model on the Gemini edge-case dataset only.\n    \"\"\"\n\n    print(\"=\" * 60)\n    print(\"CONTINUATION TRAINING ON GEMINI EDGE CASES\")\n    print(\"=\" * 60)\n    print(f\"Loading tokenizer & base model from: {model_path}\")\n\n    # 1) Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # 2) Load edge-case data\n    edge_data = load_edge_cases(edge_path)\n    if not edge_data:\n        print(\"❌ No edge-case data found. Run the generation cell first.\")\n        return\n\n    # 3) Build dataset & dataloader\n    edge_dataset = BondQueryDataset(edge_data, tokenizer, CONFIG[\"max_length\"])\n    edge_loader = DataLoader(edge_dataset, batch_size=batch_size, shuffle=True)\n\n    # 4) Load existing model checkpoint\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ProductionBondClassifier(base_model)  # uses DeBERTa-v3-small\n    ckpt_path = os.path.join(model_path, \"pytorch_model_hard.bin\")\n    state_dict = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device)\n\n    print(f\"✓ Loaded base model from {ckpt_path}\")\n    print(f\"Fine-tuning on {len(edge_dataset)} edge-case examples\")\n    print(f\"Using device: {device}\")\n\n    # 5) Optimizer & loss\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)\n    criterion = MultiTaskLoss()\n\n    # 6) Simple training loop\n    for epoch in range(1, epochs + 1):\n        print(f\"\\n--- Edge-case Epoch {epoch}/{epochs} ---\")\n        model.train()\n        epoch_loss = 0.0\n        all_preds, all_labels = [], []\n\n        pbar = tqdm(edge_loader, desc=f\"Edge-case Epoch {epoch}\")\n        for batch in pbar:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = {k: v.to(device) for k, v in batch.items()\n                      if k not in [\"input_ids\", \"attention_mask\"]}\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss_dict = criterion(outputs, labels)\n            loss = loss_dict[\"total\"]\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            preds = outputs[\"intent_logits\"].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels[\"intent_label\"].cpu().numpy())\n\n            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n        avg_loss = epoch_loss / len(edge_loader)\n        acc = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n        print(f\"Edge-case Epoch {epoch} - Loss: {avg_loss:.4f} | Acc: {acc:.3f} | F1: {f1:.3f}\")\n\n    # 7) Save updated weights\n    out_path = os.path.join(model_path, out_weight_file)\n    torch.save(model.state_dict(), out_path)\n    print(f\"\\n✓ Edge-case fine-tuned model saved to: {out_path}\")\n    print(\"Point your inference notebook at this weight file to use the updated model.\")\n\n\n# ---- Run continuation training ----\ncontinue_training_on_edge_cases(\n    model_path=\"/kaggle/working/bond_classifier_v3\",\n    base_model=\"microsoft/deberta-v3-small\",\n    edge_path=EDGE_CASE_OUTPUT,\n    epochs=8,\n    lr=5e-6,\n    batch_size=8,\n    out_weight_file=\"pytorch_model_edge.bin\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:03:29.310478Z","iopub.execute_input":"2025-11-30T10:03:29.310780Z","iopub.status.idle":"2025-11-30T10:03:46.968748Z","shell.execute_reply.started":"2025-11-30T10:03:29.310756Z","shell.execute_reply":"2025-11-30T10:03:46.967866Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCONTINUATION TRAINING ON GEMINI EDGE CASES\n============================================================\nLoading tokenizer & base model from: /kaggle/working/bond_classifier_v3\nLoaded 90 edge-case examples from /kaggle/working/edge_cases.jsonl\n✓ Loaded base model from /kaggle/working/bond_classifier_v3/pytorch_model_hard.bin\nFine-tuning on 90 edge-case examples\nUsing device: cuda\n\n--- Edge-case Epoch 1/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 1:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4836b016dcba4b719b5674fb081f33f3"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 1 - Loss: 1.0356 | Acc: 0.733 | F1: 0.712\n\n--- Edge-case Epoch 2/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 2:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88f800638954735a5a8251da50133b3"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 2 - Loss: 0.8891 | Acc: 0.778 | F1: 0.773\n\n--- Edge-case Epoch 3/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 3:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4eba559855e4fa4809a6523af16c9e2"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 3 - Loss: 0.6680 | Acc: 0.856 | F1: 0.852\n\n--- Edge-case Epoch 4/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 4:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a78f242a132424c8700039395e3a27a"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 4 - Loss: 0.4729 | Acc: 0.867 | F1: 0.858\n\n--- Edge-case Epoch 5/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 5:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7554d612e14bed94e4ea1a37b7f3ef"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 5 - Loss: 0.4416 | Acc: 0.889 | F1: 0.898\n\n--- Edge-case Epoch 6/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 6:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d832e56b7e40ab8ff3815a7afdb199"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 6 - Loss: 0.3496 | Acc: 0.956 | F1: 0.960\n\n--- Edge-case Epoch 7/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 7:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4893af398dba4aa1bf3ee7cce624f1cc"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 7 - Loss: 0.3155 | Acc: 0.967 | F1: 0.970\n\n--- Edge-case Epoch 8/8 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Edge-case Epoch 8:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c6ab956b374130b390f62ec94da38d"}},"metadata":{}},{"name":"stdout","text":"Edge-case Epoch 8 - Loss: 0.3010 | Acc: 0.967 | F1: 0.973\n\n✓ Edge-case fine-tuned model saved to: /kaggle/working/bond_classifier_v3/pytorch_model_edge.bin\nPoint your inference notebook at this weight file to use the updated model.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ==================== CELL 3: EVALUATE EDGE-TUNED MODEL ====================\n\n# Reuse ProductionBondClassifier, ClassificationResult, etc. from above\n\nTEST_QUERIES = [\n    # --- Category 1: Ambiguous Buy/Sell/Strategy ---\n    \"Should I shift my SDL holdings into shorter PSU bonds or just hold?\",\n    \"I want to reduce rate risk but I don’t want my yield to fall. What should I do?\",\n    \"Is it better to exit long-duration bonds and move into AA corporates?\",\n    \"Would switching from NTPC 2035 to REC 2030 improve my return?\",\n    \"Which reduces risk more: selling perpetuals or adding short-term G-Secs?\",\n\n    # --- Category 2: Disguised intents ---\n    \"My yield looks weak lately… what should I change?\",\n    \"These long papers are stressing me out—what’s the safest move?\",\n    \"My portfolio feels too boring. Suggest something with more kick.\",\n    \"The curve is flattening; should I reposition?\",\n    \"My advisor said I’m too exposed. What adjustments should I consider?\",\n\n    # --- Category 3: Multi-intent queries ---\n    \"Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\",\n    \"Reduce my duration and give alternatives with at least 7.5% yield.\",\n    \"Analyze my portfolio and tell me which low-yield bonds I should switch.\",\n    \"Before suggesting buys, what’s your outlook on corporate spreads?\",\n    \"If I shift to shorter bonds, how will my yield be impacted?\",\n\n    # --- Category 4: Tricky phrasing for buy intent ---\n    \"Is NTPC 2033 attractive at current spreads?\",\n    \"Are AA PSU bonds offering a good entry point?\",\n    \"Is this a good time to accumulate SDLs?\",\n    \"Should I start adding exposure to long maturities?\",\n    \"Are there better alternatives to ICICI 2029 with similar risk?\",\n\n    # --- Category 5: Risk management intents ---\n    \"The market seems jumpy; how do I protect my portfolio?\",\n    \"If RBI hikes unexpectedly, which holdings will get hit the most?\",\n    \"Rate volatility worries me—how do I reduce the impact?\",\n    \"How can I stabilize P&L swings in my portfolio?\",\n    \"Should I rebalance sectors before the credit cycle weakens?\",\n\n    # --- Category 6: Very short hard queries ---\n    \"Duration too high?\",\n    \"Better yield ideas?\",\n    \"Switch or hold?\",\n    \"Cut risk?\",\n    \"Add PSU?\",\n\n    # --- Category 7: Contradictory constraints ---\n    \"Cut duration but don’t let yield drop below 7.6%.\",\n    \"I want high yield but without taking credit risk.\",\n    \"Reduce risk but avoid selling anything.\",\n    \"Switch out of low-yield bonds but keep duration same.\",\n    \"Increase return without increasing duration or credit risk.\",\n\n    # --- Category 8: Multi-sentence queries ---\n    \"My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\",\n    \"My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\",\n    \"I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\",\n    \"I expect inflation to cool. Should I increase duration a bit?\",\n    \"Markets feel stable. Should I rotate sectors or focus on yield first?\",\n\n    # --- Category 9: Credit-analysis queries ---\n    \"How is the credit quality of PFC right now?\",\n    \"Is REC fundamentally strong enough for long-term holding?\",\n    \"What’s the default risk on NTPC?\",\n    \"Should I worry about credit spreads widening?\",\n    \"Are AA- names safe in this environment?\",\n\n    # --- Category 10: Forecast / outlook queries ---\n    \"Where do you see G-Sec yields in six months?\",\n    \"If rates fall by 50 bps, what happens to long-duration PSU bonds?\",\n    \"Will corporate spreads tighten this year?\",\n    \"Predict the movement of the 10-year benchmark.\",\n    \"How will a Fed cut affect Indian bond yields?\",\n]\n\n\ndef evaluate_edge_tuned_model():\n    \"\"\"Evaluate the edge-case fine-tuned model on the tough query set.\"\"\"\n    \n    MODEL_PATH = \"/kaggle/working/bond_classifier_v3\"\n    WEIGHT_FILE = \"pytorch_model_edge.bin\"\n    \n    print(\"=\" * 60)\n    print(\"LOADING EDGE-TUNED CLASSIFIER\")\n    print(\"=\" * 60)\n    \n    try:\n        classifier = BondClassifier(\n            MODEL_PATH,\n            base_model=\"microsoft/deberta-v3-small\",\n            weight_file=WEIGHT_FILE\n        )\n    except Exception as e:\n        print(f\"❌ Error loading model: {e}\")\n        print(\"\\nMake sure:\")\n        print(\"1. Edge-case fine-tuning saved\", WEIGHT_FILE)\n        print(\"2. MODEL_PATH is correct.\")\n        return\n    \n    print(\"=\" * 60)\n    print(\"TESTING QUERIES (EDGE-TUNED MODEL)\")\n    print(\"=\" * 60)\n    \n    for q in TEST_QUERIES:\n        result = classifier.classify(q)\n        print(f\"\\n{'='*60}\")\n        print(f\"Query: {q}\")\n        print(f\"{'='*60}\")\n        print(f\"Intent: {result.intent}\")\n        print(f\"Confidence: {result.confidence:.3f}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"EDGE-TUNED MODEL EVALUATION COMPLETE!\")\n    print(\"=\" * 60)\n\n\n# ---- Run evaluation ----\nevaluate_edge_tuned_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:04:01.776541Z","iopub.execute_input":"2025-11-30T10:04:01.776846Z","iopub.status.idle":"2025-11-30T10:04:07.583392Z","shell.execute_reply.started":"2025-11-30T10:04:01.776825Z","shell.execute_reply":"2025-11-30T10:04:07.582516Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING EDGE-TUNED CLASSIFIER\n============================================================\nLoading model from: /kaggle/working/bond_classifier_v3\nUsing weights file: pytorch_model_edge.bin\nUsing device: cuda\n✓ Model loaded successfully!\n\n============================================================\nTESTING QUERIES (EDGE-TUNED MODEL)\n============================================================\n\n============================================================\nQuery: Should I shift my SDL holdings into shorter PSU bonds or just hold?\n============================================================\nIntent: switch_bonds\nConfidence: 0.499\n\n============================================================\nQuery: I want to reduce rate risk but I don’t want my yield to fall. What should I do?\n============================================================\nIntent: reduce_duration\nConfidence: 0.613\n\n============================================================\nQuery: Is it better to exit long-duration bonds and move into AA corporates?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.196\n\n============================================================\nQuery: Would switching from NTPC 2035 to REC 2030 improve my return?\n============================================================\nIntent: switch_bonds\nConfidence: 0.783\n\n============================================================\nQuery: Which reduces risk more: selling perpetuals or adding short-term G-Secs?\n============================================================\nIntent: reduce_duration\nConfidence: 0.269\n\n============================================================\nQuery: My yield looks weak lately… what should I change?\n============================================================\nIntent: switch_bonds\nConfidence: 0.391\n\n============================================================\nQuery: These long papers are stressing me out—what’s the safest move?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.439\n\n============================================================\nQuery: My portfolio feels too boring. Suggest something with more kick.\n============================================================\nIntent: switch_bonds\nConfidence: 0.788\n\n============================================================\nQuery: The curve is flattening; should I reposition?\n============================================================\nIntent: reduce_duration\nConfidence: 0.414\n\n============================================================\nQuery: My advisor said I’m too exposed. What adjustments should I consider?\n============================================================\nIntent: portfolio_analysis\nConfidence: 0.457\n\n============================================================\nQuery: Recommend high-yield PSU bonds and also check if any of my holdings should be sold.\n============================================================\nIntent: sell_recommendation\nConfidence: 0.740\n\n============================================================\nQuery: Reduce my duration and give alternatives with at least 7.5% yield.\n============================================================\nIntent: reduce_duration\nConfidence: 0.424\n\n============================================================\nQuery: Analyze my portfolio and tell me which low-yield bonds I should switch.\n============================================================\nIntent: switch_bonds\nConfidence: 0.875\n\n============================================================\nQuery: Before suggesting buys, what’s your outlook on corporate spreads?\n============================================================\nIntent: market_outlook\nConfidence: 0.810\n\n============================================================\nQuery: If I shift to shorter bonds, how will my yield be impacted?\n============================================================\nIntent: forecast_prices\nConfidence: 0.340\n\n============================================================\nQuery: Is NTPC 2033 attractive at current spreads?\n============================================================\nIntent: credit_analysis\nConfidence: 0.376\n\n============================================================\nQuery: Are AA PSU bonds offering a good entry point?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.650\n\n============================================================\nQuery: Is this a good time to accumulate SDLs?\n============================================================\nIntent: market_outlook\nConfidence: 0.708\n\n============================================================\nQuery: Should I start adding exposure to long maturities?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.527\n\n============================================================\nQuery: Are there better alternatives to ICICI 2029 with similar risk?\n============================================================\nIntent: switch_bonds\nConfidence: 0.933\n\n============================================================\nQuery: The market seems jumpy; how do I protect my portfolio?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.637\n\n============================================================\nQuery: If RBI hikes unexpectedly, which holdings will get hit the most?\n============================================================\nIntent: forecast_prices\nConfidence: 0.285\n\n============================================================\nQuery: Rate volatility worries me—how do I reduce the impact?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.919\n\n============================================================\nQuery: How can I stabilize P&L swings in my portfolio?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.545\n\n============================================================\nQuery: Should I rebalance sectors before the credit cycle weakens?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.196\n\n============================================================\nQuery: Duration too high?\n============================================================\nIntent: reduce_duration\nConfidence: 0.407\n\n============================================================\nQuery: Better yield ideas?\n============================================================\nIntent: increase_yield\nConfidence: 0.829\n\n============================================================\nQuery: Switch or hold?\n============================================================\nIntent: switch_bonds\nConfidence: 0.777\n\n============================================================\nQuery: Cut risk?\n============================================================\nIntent: hedge_volatility\nConfidence: 0.643\n\n============================================================\nQuery: Add PSU?\n============================================================\nIntent: barbell_strategy\nConfidence: 0.235\n\n============================================================\nQuery: Cut duration but don’t let yield drop below 7.6%.\n============================================================\nIntent: reduce_duration\nConfidence: 0.767\n\n============================================================\nQuery: I want high yield but without taking credit risk.\n============================================================\nIntent: increase_yield\nConfidence: 0.681\n\n============================================================\nQuery: Reduce risk but avoid selling anything.\n============================================================\nIntent: hedge_volatility\nConfidence: 0.305\n\n============================================================\nQuery: Switch out of low-yield bonds but keep duration same.\n============================================================\nIntent: switch_bonds\nConfidence: 0.965\n\n============================================================\nQuery: Increase return without increasing duration or credit risk.\n============================================================\nIntent: increase_yield\nConfidence: 0.615\n\n============================================================\nQuery: My duration increased after the last purchases. I’m worried about hikes. Suggest adjustments that don’t hurt yield.\n============================================================\nIntent: reduce_duration\nConfidence: 0.623\n\n============================================================\nQuery: My portfolio is mostly PSU and financials. Seems concentrated. Should I diversify into private corporates?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.875\n\n============================================================\nQuery: I sold some long-term bonds last month. Now thinking of adding 3–5 year AA corporates. Any ideas?\n============================================================\nIntent: buy_recommendation\nConfidence: 0.372\n\n============================================================\nQuery: I expect inflation to cool. Should I increase duration a bit?\n============================================================\nIntent: reduce_duration\nConfidence: 0.311\n\n============================================================\nQuery: Markets feel stable. Should I rotate sectors or focus on yield first?\n============================================================\nIntent: sector_rebalance\nConfidence: 0.447\n\n============================================================\nQuery: How is the credit quality of PFC right now?\n============================================================\nIntent: credit_analysis\nConfidence: 0.953\n\n============================================================\nQuery: Is REC fundamentally strong enough for long-term holding?\n============================================================\nIntent: credit_analysis\nConfidence: 0.361\n\n============================================================\nQuery: What’s the default risk on NTPC?\n============================================================\nIntent: credit_analysis\nConfidence: 0.914\n\n============================================================\nQuery: Should I worry about credit spreads widening?\n============================================================\nIntent: market_outlook\nConfidence: 0.674\n\n============================================================\nQuery: Are AA- names safe in this environment?\n============================================================\nIntent: credit_analysis\nConfidence: 0.748\n\n============================================================\nQuery: Where do you see G-Sec yields in six months?\n============================================================\nIntent: forecast_prices\nConfidence: 0.695\n\n============================================================\nQuery: If rates fall by 50 bps, what happens to long-duration PSU bonds?\n============================================================\nIntent: forecast_prices\nConfidence: 0.391\n\n============================================================\nQuery: Will corporate spreads tighten this year?\n============================================================\nIntent: market_outlook\nConfidence: 0.409\n\n============================================================\nQuery: Predict the movement of the 10-year benchmark.\n============================================================\nIntent: forecast_prices\nConfidence: 0.926\n\n============================================================\nQuery: How will a Fed cut affect Indian bond yields?\n============================================================\nIntent: market_outlook\nConfidence: 0.629\n\n============================================================\nEDGE-TUNED MODEL EVALUATION COMPLETE!\n============================================================\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\nimport time\nfrom pathlib import Path\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n# --- Load the trained model ---\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Same architecture as training: DeBERTa-v3-small + multi-task heads.\"\"\"\n    \n    def __init__(self, base_model: str = 'microsoft/deberta-v3-small', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # Heads: intent + sectors + rating + duration + constraints\n        self.intent_head = nn.Linear(feature_size, 13)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\nclass BondClassifier:\n    def __init__(self, model_path: str, base_model: str = \"microsoft/deberta-v3-small\"):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        # Load model\n        self.model = ProductionBondClassifier(base_model=base_model)\n        state_dict = torch.load(f\"{model_path}/pytorch_model_edge.bin\", map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n\n        self.intent_names = [\n            'buy_recommendation', 'sell_recommendation', 'portfolio_analysis',\n            'reduce_duration', 'increase_yield', 'hedge_volatility',\n            'sector_rebalance', 'barbell_strategy', 'switch_bonds',\n            'explain_recommendation', 'market_outlook', 'credit_analysis',\n            'forecast_prices'\n        ]\n    \n    def classify(self, query: str):\n        # Tokenize\n        enc = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        input_ids = enc[\"input_ids\"].to(self.device)\n        attention_mask = enc[\"attention_mask\"].to(self.device)\n        \n        # Run model\n        with torch.no_grad():\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            intent_logits = outputs['intent_logits']\n            intent_probs = F.softmax(intent_logits, dim=-1)\n            \n        # Get the predicted intent\n        intent_idx = intent_probs.argmax().item()\n        intent = self.intent_names[intent_idx]\n        confidence = intent_probs.max().item()\n        \n        return intent, confidence\n\n# --- Initialize the classifier ---\nMODEL_PATH = '/kaggle/working/bond_classifier_v3'  # Path to your trained model directory\nclassifier = BondClassifier(MODEL_PATH)\n\n# --- Input your query here ---\nquery = input(\"Enter your bond-related query: \")\n\n# --- Start timing the classification process ---\nstart_time = time.time()\n\n# --- Get the predicted intent ---\nintent, confidence = classifier.classify(query)\n\n# --- End timing the classification process ---\nend_time = time.time()\n\n# --- Output the results ---\nprint(f\"Predicted Intent: {intent}\")\nprint(f\"Confidence: {confidence:.3f}\")\n\n# --- Output the time taken ---\nprint(f\"Time taken for classification: {end_time - start_time:.4f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:43:06.151347Z","iopub.execute_input":"2025-11-30T10:43:06.151657Z","iopub.status.idle":"2025-11-30T10:43:18.568604Z","shell.execute_reply.started":"2025-11-30T10:43:06.151633Z","shell.execute_reply":"2025-11-30T10:43:18.567802Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your bond-related query:      \"If the Fed unexpectedly hikes interest rates, which of my bonds will likely be hit the hardest in terms of duration and credit quality?\",\n"},{"name":"stdout","text":"Predicted Intent: credit_analysis\nConfidence: 0.771\nTime taken for classification: 0.0160 seconds\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"# Non-bond related query","metadata":{}},{"cell_type":"code","source":"##DO for non-bonds and bonds\n##Bonds - Intent classification\n##Non bonds - Web search/General LLM \n\n#######\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}