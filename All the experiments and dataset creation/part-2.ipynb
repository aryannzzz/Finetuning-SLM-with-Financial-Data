{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13931846,"sourceType":"datasetVersion","datasetId":8878304},{"sourceId":13934300,"sourceType":"datasetVersion","datasetId":8878517}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:18:07.881670Z","iopub.execute_input":"2025-11-30T16:18:07.881844Z","iopub.status.idle":"2025-11-30T16:18:09.681763Z","shell.execute_reply.started":"2025-11-30T16:18:07.881828Z","shell.execute_reply":"2025-11-30T16:18:09.681103Z"}},"outputs":[{"name":"stdout","text":"/kaggle/lib/kaggle/gcp.py\n/kaggle/input/synthetic-gemini-created-dataset-for-finteuning/val.jsonl\n/kaggle/input/synthetic-gemini-created-dataset-for-finteuning/test.jsonl\n/kaggle/input/synthetic-gemini-created-dataset-for-finteuning/train.jsonl\n/kaggle/input/synthetic-gemini-created-dataset-for-finteuning/llm_intent_dataset.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/train_final.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/test_final.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/equity_edge_cases.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/val_final.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/bond_edge_cases.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/general_non_bond_edge_cases.jsonl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===== GEMINI SETUP CELL =====\n!pip install -q google-genai\n\nimport os\n\n# Paste your key here (or use Kaggle secrets and set env there)\nos.environ[\"GEMINI_API_KEY\"] = \"AIzaSyD-9r0N-YGJfL_hZR2elzwpc6m4f6PPb2Y\"\n\nprint(\"GEMINI_API_KEY set:\", \"GEMINI_API_KEY\" in os.environ)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:14:36.500193Z","iopub.execute_input":"2025-11-30T14:14:36.500435Z","iopub.status.idle":"2025-11-30T14:14:40.109922Z","shell.execute_reply.started":"2025-11-30T14:14:36.500418Z","shell.execute_reply":"2025-11-30T14:14:40.108975Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"GEMINI_API_KEY set: True\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import os\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\n\nfrom sklearn.model_selection import train_test_split\nfrom google import genai\n\n# ============================================================\n# CONFIG\n# ============================================================\nDATA_DIR = Path(\"/kaggle/input/synthetic-gemini-created-dataset-for-finteuning\")\nOUTPUT_DIR = Path(\"/kaggle/working/extended_intent_dataset\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nNON_BOND_CONFIG = {\n    \"model\": \"gemini-2.0-flash\",\n    \"samples_per_intent\": 400,   # per non-bond intent (tune if needed)\n    \"max_per_call\": 40,          # how many examples per Gemini call\n    \"augmentation_factor\": 0.4,  # 40% extra augmented rows on combined data\n    \"train_frac\": 0.7,\n    \"val_frac\": 0.15,            # test_frac = 1 - train_frac - val_frac\n}\n\nassert os.environ.get(\"GEMINI_API_KEY\"), \"Set GEMINI_API_KEY in Kaggle before running this cell.\"\n\n# ============================================================\n# UTILS: LOAD EXISTING BOND DATA\n# ============================================================\n\ndef load_jsonl(path: Path) -> List[Dict[str, Any]]:\n    data = []\n    if not path.exists():\n        print(f\"⚠ File not found, skipping: {path}\")\n        return data\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                data.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return data\n\n\nbond_train = load_jsonl(DATA_DIR / \"train.jsonl\")\nbond_val   = load_jsonl(DATA_DIR / \"val.jsonl\")\nbond_test  = load_jsonl(DATA_DIR / \"test.jsonl\")\nbond_llm   = load_jsonl(DATA_DIR / \"llm_intent_dataset.jsonl\")\n\nbond_all = bond_train + bond_val + bond_test + bond_llm\n\n# Deduplicate on (text, intent)\nseen = set()\ndeduped_bond: List[Dict[str, Any]] = []\nfor row in bond_all:\n    key = (row.get(\"text\", \"\").strip(), row.get(\"intent\"))\n    if key in seen:\n        continue\n    seen.add(key)\n    deduped_bond.append(row)\n\nprint(\"=\" * 60)\nprint(f\"Loaded bond data: {len(bond_all)} rows -> {len(deduped_bond)} unique rows\")\nprint(\"=\" * 60)\n\n\n# ============================================================\n# SIMPLE AUGMENTER (same spirit as original DataAugmenter)\n# ============================================================\n\nclass SimpleAugmenter:\n    @staticmethod\n    def augment_dataset(dataset: List[Dict[str, Any]], factor: float = 0.4) -> List[Dict[str, Any]]:\n        num_to_augment = int(len(dataset) * factor)\n        if num_to_augment <= 0:\n            return dataset\n        samples = random.sample(dataset, min(num_to_augment, len(dataset)))\n        augmented = []\n        for sample in samples:\n            aug_type = random.choice([\"synonym\", \"insertion\", \"deletion\"])\n            if aug_type == \"synonym\":\n                augmented.append(SimpleAugmenter._synonym_replacement(sample))\n            elif aug_type == \"insertion\":\n                augmented.append(SimpleAugmenter._random_insertion(sample))\n            else:\n                augmented.append(SimpleAugmenter._random_deletion(sample))\n        print(f\"✓ Augmented {len(augmented)} extra rows\")\n        return dataset + augmented\n\n    @staticmethod\n    def _synonym_replacement(sample: Dict[str, Any]) -> Dict[str, Any]:\n        synonyms = {\n            \"find\": [\"locate\", \"search for\", \"look for\"],\n            \"show\": [\"display\", \"list\", \"present\"],\n            \"get\": [\"fetch\", \"retrieve\"],\n            \"explain\": [\"describe\", \"clarify\", \"break down\"],\n        }\n        words = sample[\"text\"].split()\n        for i, w in enumerate(words):\n            low = w.lower().strip(\".,!?\")\n            if low in synonyms and random.random() < 0.3:\n                words[i] = random.choice(synonyms[low])\n        new_sample = sample.copy()\n        new_sample[\"text\"] = \" \".join(words)\n        return new_sample\n\n    @staticmethod\n    def _random_insertion(sample: Dict[str, Any]) -> Dict[str, Any]:\n        words = sample[\"text\"].split()\n        if len(words) > 3:\n            pos = random.randint(0, len(words))\n            words.insert(pos, random.choice([\"please\", \"kindly\", \"also\"]))\n        new_sample = sample.copy()\n        new_sample[\"text\"] = \" \".join(words)\n        return new_sample\n\n    @staticmethod\n    def _random_deletion(sample: Dict[str, Any]) -> Dict[str, Any]:\n        words = sample[\"text\"].split()\n        words = [w for w in words if w.lower() not in [\"please\", \"kindly\"] or random.random() > 0.5]\n        new_sample = sample.copy()\n        new_sample[\"text\"] = \" \".join(words) if words else sample[\"text\"]\n        return new_sample\n\n\n# ============================================================\n# GEMINI: NON-BOND DATA GENERATOR\n# ============================================================\n\nclass NonBondLLMDataGenerator:\n    \"\"\"\n    Generate labelled non-bond queries for two new intents:\n      - non_bond_search : things you want a search / retrieval backend to handle\n      - non_bond_llm    : things better answered by an LLM directly\n    \"\"\"\n    def __init__(self, model_name: str, samples_per_intent: int, max_per_call: int = 40):\n        self.client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n        self.model_name = model_name\n        self.samples_per_intent = samples_per_intent\n        self.max_per_call = max_per_call\n\n        self.intent_descriptions = {\n            \"non_bond_search\": (\n                \"Queries that are clearly NOT about bonds or fixed income. \"\n                \"They ask to look up or fetch external information: news, \"\n                \"documents, data, web pages, PDFs, regulations, etc. \"\n                \"Examples verbs: 'search', 'find', 'show me', 'look up', 'get me', 'download'.\"\n            ),\n            \"non_bond_llm\": (\n                \"Queries that are clearly NOT about bonds or fixed income. \"\n                \"They ask the assistant to reason, explain, draft, summarize, compare, \"\n                \"or brainstorm. They are best answered directly by an LLM instead of search. \"\n                \"Examples verbs: 'explain', 'summarize', 'compare', 'rewrite', 'draft', 'brainstorm'.\"\n            ),\n        }\n\n        self.forbidden_keywords = [\n            \"bond\", \"bonds\", \"g-sec\", \"gsec\", \"debenture\", \"coupon rate\",\n            \"yield\", \"duration\", \"sovereign bond\", \"corporate bond\", \"fixed income\"\n        ]\n\n    def generate_dataset(self) -> List[Dict[str, Any]]:\n        all_rows: List[Dict[str, Any]] = []\n        print(\"=\" * 60)\n        print(\"GENERATING NON-BOND DATA WITH GEMINI\")\n        print(\"=\" * 60)\n\n        for intent_name in [\"non_bond_search\", \"non_bond_llm\"]:\n            needed = self.samples_per_intent\n            print(f\"\\n→ Intent: {intent_name}  target={needed}\")\n            while needed > 0:\n                batch_size = min(self.max_per_call, needed)\n                batch = self._generate_batch_for_intent(intent_name, batch_size)\n                all_rows.extend(batch)\n                needed -= len(batch)\n                print(f\"  Collected {self.samples_per_intent - needed}/{self.samples_per_intent} for {intent_name}\")\n\n        print(f\"\\n✓ Total non-bond rows: {len(all_rows)}\")\n        return all_rows\n\n    def _generate_batch_for_intent(self, intent_name: str, batch_size: int) -> List[Dict[str, Any]]:\n        description = self.intent_descriptions[intent_name]\n\n        system_msg = (\n            \"You are generating synthetic training data for an intent classifier. \"\n            \"Your job is to create realistic user queries that are NOT about bonds or fixed income.\"\n        )\n\n        user_msg = f\"\"\"\nGenerate {batch_size} diverse user queries whose PRIMARY intent is: \"{intent_name}\".\n\nIntent description:\n{description}\n\nImportant constraints:\n- The queries MUST NOT be about bonds, fixed income, yields, coupons, debentures, or interest-rate products.\n- They can be about anything else: equity markets, crypto, general knowledge, tech, sports, travel, etc.\n- Mix short (3-8 words) and longer (1-3 sentences) queries.\n- Vary user persona: casual, professional, student, etc.\n\nReturn the output in JSON Lines format (one JSON object per line, no array, no extra text).\n\nEach JSON object MUST have exactly these keys:\n- \"text\": string, the user query\n- \"intent\": string, MUST be exactly \"{intent_name}\"\n- \"sectors\": array of strings, MUST be an empty array [] for non-bond queries\n- \"rating\": null\n- \"duration\": string, use \"medium\"\n- \"constraints\": object with boolean fields, all false:\n  {{\n    \"preserve_yield\": false,\n    \"maintain_liquidity\": false,\n    \"avoid_downgrades\": false,\n    \"sector_diversity\": false,\n    \"rating_above_aa\": false\n  }}\n\"\"\"\n\n        full_prompt = system_msg + \"\\n\\n\" + user_msg\n\n        resp = self.client.models.generate_content(\n            model=self.model_name,\n            contents=full_prompt,\n        )\n\n        raw = resp.text or \"\"\n        return self._parse_jsonl(raw, intent_name)\n\n    def _parse_jsonl(self, raw: str, intent_name: str) -> List[Dict[str, Any]]:\n        rows: List[Dict[str, Any]] = []\n        for line in raw.splitlines():\n            line = line.strip()\n            if not line or line.startswith(\"```\"):\n                continue\n            if line.startswith(\"-\"):\n                line = line.lstrip(\"-\").strip()\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            sample = self._normalize_record(obj, intent_name)\n            if sample is not None:\n                rows.append(sample)\n        return rows\n\n    def _normalize_record(self, obj: Dict[str, Any], intent_name: str) -> Optional[Dict[str, Any]]:\n        text = str(obj.get(\"text\", \"\")).strip()\n        if not text:\n            return None\n\n        # Filter out anything that still looks bond-ish\n        lower = text.lower()\n        if any(kw in lower for kw in self.forbidden_keywords):\n            return None\n\n        intent = obj.get(\"intent\", intent_name)\n        if intent != intent_name:\n            intent = intent_name\n\n        # Force schema consistent with bond dataset\n        sample = {\n            \"text\": text,\n            \"intent\": intent,\n            \"sectors\": [],           # always empty for non-bond\n            \"rating\": None,          # always null\n            \"duration\": \"medium\",    # keep it simple\n            \"constraints\": {\n                \"preserve_yield\": False,\n                \"maintain_liquidity\": False,\n                \"avoid_downgrades\": False,\n                \"sector_diversity\": False,\n                \"rating_above_aa\": False,\n            },\n        }\n        return sample\n\n\n# ============================================================\n# RUN GENERATION + MERGE + AUGMENT + SPLIT\n# ============================================================\n\nnonbond_gen = NonBondLLMDataGenerator(\n    model_name=NON_BOND_CONFIG[\"model\"],\n    samples_per_intent=NON_BOND_CONFIG[\"samples_per_intent\"],\n    max_per_call=NON_BOND_CONFIG[\"max_per_call\"],\n)\n\nnonbond_data = nonbond_gen.generate_dataset()\nprint(f\"\\nNon-bond label counts:\")\nfrom collections import Counter\nprint(Counter(row[\"intent\"] for row in nonbond_data))\n\n# Merge with bond data\ncombined = deduped_bond + nonbond_data\nprint(f\"\\nCombined dataset size before augmentation: {len(combined)}\")\n\n# Augment the combined dataset\ncombined_aug = SimpleAugmenter.augment_dataset(\n    combined,\n    factor=NON_BOND_CONFIG[\"augmentation_factor\"],\n)\nprint(f\"Combined dataset size AFTER augmentation: {len(combined_aug)}\")\n\n# ------------------------------------------------------------\n# Final stratified split by intent into train/val/test\n# ------------------------------------------------------------\nintents = [row[\"intent\"] for row in combined_aug]\n\ntrain_tmp, test_data = train_test_split(\n    combined_aug,\n    test_size=1.0 - (NON_BOND_CONFIG[\"train_frac\"] + NON_BOND_CONFIG[\"val_frac\"]),\n    stratify=intents,\n    random_state=42,\n)\n\nintents_tmp = [row[\"intent\"] for row in train_tmp]\nval_size = NON_BOND_CONFIG[\"val_frac\"] / (NON_BOND_CONFIG[\"train_frac\"] + NON_BOND_CONFIG[\"val_frac\"])\n\ntrain_data, val_data = train_test_split(\n    train_tmp,\n    test_size=val_size,\n    stratify=intents_tmp,\n    random_state=42,\n)\n\nprint(\"\\nFinal split sizes (all intents: bond + non-bond)\")\nprint(f\"Train: {len(train_data)}\")\nprint(f\"Val:   {len(val_data)}\")\nprint(f\"Test:  {len(test_data)}\")\n\n# Save out new JSONL files\ndef save_jsonl(path: Path, rows: List[Dict[str, Any]]):\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nsave_jsonl(OUTPUT_DIR / \"train_extended.jsonl\", train_data)\nsave_jsonl(OUTPUT_DIR / \"val_extended.jsonl\",   val_data)\nsave_jsonl(OUTPUT_DIR / \"test_extended.jsonl\",  test_data)\n\nprint(f\"\\n✓ Saved extended datasets (with non-bond intents) to: {OUTPUT_DIR}\")\nprint(\"  - train_extended.jsonl\")\nprint(\"  - val_extended.jsonl\")\nprint(\"  - test_extended.jsonl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:51:33.036430Z","iopub.execute_input":"2025-11-30T12:51:33.036773Z","iopub.status.idle":"2025-11-30T12:56:43.225812Z","shell.execute_reply.started":"2025-11-30T12:51:33.036737Z","shell.execute_reply":"2025-11-30T12:56:43.224258Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLoaded bond data: 22100 rows -> 5067 unique rows\n============================================================\n============================================================\nGENERATING NON-BOND DATA WITH GEMINI\n============================================================\n\n→ Intent: non_bond_search  target=400\n  Collected 40/400 for non_bond_search\n  Collected 80/400 for non_bond_search\n  Collected 120/400 for non_bond_search\n  Collected 160/400 for non_bond_search\n  Collected 199/400 for non_bond_search\n  Collected 238/400 for non_bond_search\n  Collected 278/400 for non_bond_search\n  Collected 317/400 for non_bond_search\n  Collected 356/400 for non_bond_search\n  Collected 396/400 for non_bond_search\n  Collected 400/400 for non_bond_search\n\n→ Intent: non_bond_llm  target=400\n  Collected 40/400 for non_bond_llm\n  Collected 80/400 for non_bond_llm\n  Collected 120/400 for non_bond_llm\n  Collected 160/400 for non_bond_llm\n  Collected 200/400 for non_bond_llm\n  Collected 239/400 for non_bond_llm\n  Collected 279/400 for non_bond_llm\n  Collected 318/400 for non_bond_llm\n  Collected 357/400 for non_bond_llm\n  Collected 397/400 for non_bond_llm\n  Collected 400/400 for non_bond_llm\n\n✓ Total non-bond rows: 800\n\nNon-bond label counts:\nCounter({'non_bond_search': 400, 'non_bond_llm': 400})\n\nCombined dataset size before augmentation: 5867\n✓ Augmented 2346 extra rows\nCombined dataset size AFTER augmentation: 8213\n\nFinal split sizes (all intents: bond + non-bond)\nTrain: 5749\nVal:   1232\nTest:  1232\n\n✓ Saved extended datasets (with non-bond intents) to: /kaggle/working/extended_intent_dataset\n  - train_extended.jsonl\n  - val_extended.jsonl\n  - test_extended.jsonl\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q transformers accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:19:09.364661Z","iopub.execute_input":"2025-11-30T13:19:09.365027Z","iopub.status.idle":"2025-11-30T13:20:39.962180Z","shell.execute_reply.started":"2025-11-30T13:19:09.364998Z","shell.execute_reply":"2025-11-30T13:20:39.961044Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Training on extended (bond + non-bond) dataset\n======================================================================\n\nAssumes these files exist (your newly created dataset):\n\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/train_extended.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/val_extended.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/test_extended.jsonl\n\"\"\"\n\n# ==================== INSTALL DEPENDENCIES ====================\nimport sys\nimport subprocess\n\nprint(\"=\" * 60)\nprint(\"INSTALLING DEPENDENCIES\")\nprint(\"=\" * 60)\n\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"transformers\", \"accelerate\", \"scikit-learn\"\n])\n\nprint(\"✓ Dependencies installed!\\n\")\n\n\n# ==================== IMPORTS ====================\nimport os\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    get_cosine_schedule_with_warmup\n)\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n\n# ==================== GPU CHECK ====================\nprint(\"=\" * 60)\nprint(\"GPU CHECK\")\nprint(\"=\" * 60)\n\nif torch.cuda.is_available():\n    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"✓ CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"⚠ WARNING: No GPU detected! Training will be very slow.\")\n\nprint()\n\n\n# ==================== CONFIGURATION ====================\nCONFIG = {\n    'base_model': 'microsoft/deberta-v3-small',\n\n    # ---- extended dataset paths ----\n    'dataset_dir': '/kaggle/input/bonds-query-classifier-finetuning-slm-dataset',\n    'train_file': 'train_extended.jsonl',\n    'val_file':   'val_extended.jsonl',\n    'test_file':  'test_extended.jsonl',\n\n    'batch_size': 32,\n    'num_epochs': 12,          # you can bump to 8–10 once it trains fine\n    'learning_rate': 2e-5,\n    'warmup_ratio': 0.1,\n    'max_length': 128,\n    'output_dir': '/kaggle/working/bond_classifier_v3',\n    'seed': 42,\n}\n\n# Set seeds (same as old pipeline)\nrandom.seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\ntorch.manual_seed(CONFIG['seed'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CONFIG['seed'])\n\n\n# ==================== PYTORCH DATASET (NOW 15 INTENTS) ====================\n\nclass BondQueryDataset(Dataset):\n    \"\"\"PyTorch Dataset (bond + non-bond router)\"\"\"\n    \n    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # 13 original intents + 2 new router intents\n        self.intent_to_id = {\n            'buy_recommendation': 0,\n            'sell_recommendation': 1,\n            'portfolio_analysis': 2,\n            'reduce_duration': 3,\n            'increase_yield': 4,\n            'hedge_volatility': 5,\n            'sector_rebalance': 6,\n            'barbell_strategy': 7,\n            'switch_bonds': 8,\n            'explain_recommendation': 9,\n            'market_outlook': 10,\n            'credit_analysis': 11,\n            'forecast_prices': 12,\n            'non_bond_search': 13,  # NEW\n            'non_bond_llm': 14,     # NEW\n        }\n        \n        self.sector_to_id = {\n            'Sovereign': 0, 'PSU Energy': 1, 'Financial': 2,\n            'Corporate': 3, 'Infrastructure': 4, 'NBFC': 5, 'Banking': 6\n        }\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        encoding = self.tokenizer(\n            sample['text'],\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        # ---- intent ----\n        intent_label = self.intent_to_id[sample['intent']]\n        \n        # ---- sectors (multi-label) ----\n        sector_labels = torch.zeros(len(self.sector_to_id))\n        for sector in sample.get('sectors', []):\n            if sector in self.sector_to_id:\n                sector_labels[self.sector_to_id[sector]] = 1\n        \n        # ---- rating ----\n        rating = sample.get('rating')\n        rating_map = {'AAA': 0, 'AA+': 1, 'AA': 2, 'A+': 3, 'A': 4, 'BBB': 5}\n        rating_label = rating_map.get(rating, 6)   # 6 = \"other / none\"\n        \n        # ---- duration ----\n        duration_map = {'short': 0, 'medium': 1, 'long': 2}\n        duration_label = duration_map.get(sample.get('duration', 'medium'), 1)\n        \n        # ---- constraints (5 binary flags) ----\n        constraints = sample.get('constraints', {})\n        constraint_labels = torch.tensor([\n            float(constraints.get('preserve_yield', False)),\n            float(constraints.get('maintain_liquidity', False)),\n            float(constraints.get('avoid_downgrades', False)),\n            float(constraints.get('sector_diversity', False)),\n            float(constraints.get('rating_above_aa', False))\n        ])\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'intent_label': torch.tensor(intent_label),\n            'sector_labels': sector_labels,\n            'rating_label': torch.tensor(rating_label),\n            'duration_label': torch.tensor(duration_label),\n            'constraint_labels': constraint_labels\n        }\n\n\n# ==================== MODEL (SAME ARCH, 15 INTENT OUTPUTS) ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Multi-task classifier\"\"\"\n    \n    def __init__(self, base_model: str = 'distilbert-base-uncased', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # ⬇️ changed from 13 → 15 intents\n        self.intent_head = nn.Linear(feature_size, 15)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== LOSS (UNCHANGED) ====================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n\n\nclass MultiTaskLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intent_loss_fn = FocalLoss(gamma=2.0)\n        self.sector_loss_fn = nn.BCEWithLogitsLoss()\n        self.rating_loss_fn = nn.CrossEntropyLoss()\n        self.duration_loss_fn = nn.CrossEntropyLoss()\n        self.constraint_loss_fn = nn.BCEWithLogitsLoss()\n    \n    def forward(self, outputs, labels):\n        intent_loss = self.intent_loss_fn(outputs['intent_logits'], labels['intent_label'])\n        sector_loss = self.sector_loss_fn(outputs['sector_logits'], labels['sector_labels'])\n        rating_loss = self.rating_loss_fn(outputs['rating_logits'], labels['rating_label'])\n        duration_loss = self.duration_loss_fn(outputs['duration_logits'], labels['duration_label'])\n        constraint_loss = self.constraint_loss_fn(outputs['constraint_logits'], labels['constraint_labels'])\n        \n        total = intent_loss + 0.5*sector_loss + 0.3*rating_loss + 0.3*duration_loss + 0.4*constraint_loss\n        \n        return {'total': total, 'intent': intent_loss}\n\n\n# ==================== TRAINER (UNCHANGED) ====================\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, device, output_dir):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.best_acc = 0.0\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            \n            labels = {k: v.to(self.device) for k, v in batch.items() \n                      if k not in ['input_ids', 'attention_mask']}\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(input_ids, attention_mask)\n            loss_dict = self.criterion(outputs, labels)\n            loss = loss_dict['total']\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            preds = outputs['intent_logits'].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels['intent_label'].cpu().numpy())\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{accuracy_score(all_labels[-len(preds):], preds):.3f}'\n            })\n        \n        return total_loss / len(self.train_loader)\n    \n    def evaluate(self):\n        self.model.eval()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc='Evaluating'):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = {k: v.to(self.device) for k, v in batch.items() \n                          if k not in ['input_ids', 'attention_mask']}\n                \n                outputs = self.model(input_ids, attention_mask)\n                loss_dict = self.criterion(outputs, labels)\n                total_loss += loss_dict['total'].item()\n                \n                preds = outputs['intent_logits'].argmax(dim=-1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels['intent_label'].cpu().numpy())\n        \n        return {\n            'loss': total_loss / len(self.val_loader),\n            'accuracy': accuracy_score(all_labels, all_preds),\n            'f1_macro': f1_score(all_labels, all_preds, average='macro')\n        }\n    \n    def save_checkpoint(self, epoch, metrics):\n        if metrics['accuracy'] > self.best_acc:\n            self.best_acc = metrics['accuracy']\n            torch.save(self.model.state_dict(), self.output_dir / 'pytorch_model.bin')\n            print(f\"✓ New best model saved (acc: {metrics['accuracy']:.4f})\")\n    \n    def train(self, num_epochs):\n        print(\"=\" * 60)\n        print(\"TRAINING\")\n        print(\"=\" * 60)\n        \n        for epoch in range(1, num_epochs + 1):\n            train_loss = self.train_epoch(epoch)\n            print(f\"\\nEpoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n            \n            val_metrics = self.evaluate()\n            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Val F1: {val_metrics['f1_macro']:.4f}\\n\")\n            \n            self.save_checkpoint(epoch, val_metrics)\n        \n        print(f\"Training complete! Best accuracy: {self.best_acc:.4f}\\n\")\n\n\n# ==================== JSONL LOADER ====================\n\ndef load_jsonl(path: str) -> List[Dict[str, Any]]:\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                rows.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return rows\n\n\n# ==================== MAIN TRAINING FUNCTION ====================\n\ndef train_model():\n    \"\"\"Train on pre-built extended dataset (bond + non-bond).\"\"\"\n    \n    dataset_dir = CONFIG['dataset_dir']\n    train_path = os.path.join(dataset_dir, CONFIG['train_file'])\n    val_path   = os.path.join(dataset_dir, CONFIG['val_file'])\n    test_path  = os.path.join(dataset_dir, CONFIG['test_file'])\n\n    print(\"=\" * 60)\n    print(\"LOADING EXTENDED DATASET\")\n    print(\"=\" * 60)\n    train_data = load_jsonl(train_path)\n    val_data   = load_jsonl(val_path)\n    test_data  = load_jsonl(test_path)\n\n    print(f\"Train rows: {len(train_data)}\")\n    print(f\"Val rows:   {len(val_data)}\")\n    print(f\"Test rows:  {len(test_data)}\\n\")\n\n    # ---- Tokenizer & Datasets ----\n    print(\"=\" * 60)\n    print(\"LOADING MODEL & TOKENIZER\")\n    print(\"=\" * 60)\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG['base_model'])\n    \n    train_dataset = BondQueryDataset(train_data, tokenizer, CONFIG['max_length'])\n    val_dataset   = BondQueryDataset(val_data,   tokenizer, CONFIG['max_length'])\n    test_dataset  = BondQueryDataset(test_data,  tokenizer, CONFIG['max_length'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader   = DataLoader(val_dataset,   batch_size=CONFIG['batch_size'])\n    test_loader  = DataLoader(test_dataset,  batch_size=CONFIG['batch_size'])\n    \n    # ---- Model ----\n    model = ProductionBondClassifier(CONFIG['base_model'])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    print(f\"✓ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n    print(f\"✓ Device: {device}\\n\")\n    \n    # ---- Optimizer & Scheduler ----\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n    num_training_steps = len(train_loader) * CONFIG['num_epochs']\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * CONFIG['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    criterion = MultiTaskLoss()\n    \n    # ---- Train ----\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        criterion=criterion,\n        device=device,\n        output_dir=CONFIG['output_dir'],\n    )\n    trainer.train(CONFIG['num_epochs'])\n    \n    # ---- Save tokenizer ----\n    tokenizer.save_pretrained(CONFIG['output_dir'])\n    \n    # ---- Final test ----\n    print(\"=\" * 60)\n    print(\"FINAL TEST\")\n    print(\"=\" * 60)\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc='Testing'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask)\n            preds = outputs['intent_logits'].argmax(dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['intent_label'].numpy())\n    \n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    print(f\"\\n✓ Test Accuracy: {test_acc:.4f}\")\n    print(f\"✓ Test F1 Macro: {test_f1:.4f}\\n\")\n    \n    print(\"=\" * 60)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"\\n✓ Model saved to: {CONFIG['output_dir']}\")\n    print(f\"✓ Files: pytorch_model.bin, tokenizer files\")\n    print(\"\\nTo download:\")\n    print(\"  1. Go to Output tab\")\n    print(\"  2. Download 'bond_classifier_v3' folder\")\n    print(\"  3. Use locally with your inference notebook\")\n\n\n# ==================== RUN ====================\n\nif __name__ == '__main__':\n    train_model()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:27:19.340391Z","iopub.execute_input":"2025-11-30T13:27:19.340712Z","iopub.status.idle":"2025-11-30T13:44:50.956008Z","shell.execute_reply.started":"2025-11-30T13:27:19.340689Z","shell.execute_reply":"2025-11-30T13:44:50.955122Z"}},"outputs":[{"name":"stdout","text":"============================================================\nINSTALLING DEPENDENCIES\n============================================================\n✓ Dependencies installed!\n\n============================================================\nGPU CHECK\n============================================================\n✓ GPU detected: Tesla T4\n✓ GPU memory: 15.83 GB\n✓ CUDA version: 12.4\n\n============================================================\nLOADING EXTENDED DATASET\n============================================================\nTrain rows: 5749\nVal rows:   1232\nTest rows:  1232\n\n============================================================\nLOADING MODEL & TOKENIZER\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ Model loaded: 142,206,757 parameters\n✓ Device: cuda\n\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84367d5e9b1d4522a7b1675d86cd6e93"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/12 - Train Loss: 3.4867\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae1aa31f3bfb44bfa5e6f55347473053"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 1.9907\nVal Accuracy: 0.6266\nVal F1: 0.5090\n\n✓ New best model saved (acc: 0.6266)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feeab502b0744fada8f80aef0754838b"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2/12 - Train Loss: 1.2776\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c997edfb884b6bad9f092067240d36"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.7625\nVal Accuracy: 0.9886\nVal F1: 0.9900\n\n✓ New best model saved (acc: 0.9886)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a529c26c104113897322124fba5e02"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3/12 - Train Loss: 0.7386\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5fbfa400b7422dadcf907f6154e102"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.6087\nVal Accuracy: 0.9927\nVal F1: 0.9921\n\n✓ New best model saved (acc: 0.9927)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26aed500a9dc4af5a0a635e32ad1f089"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4/12 - Train Loss: 0.5986\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6971894ad99845bcbc93eee70cb8b158"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5192\nVal Accuracy: 0.9951\nVal F1: 0.9950\n\n✓ New best model saved (acc: 0.9951)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eee731d7c9b4a759f02cb3afcd29795"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 5/12 - Train Loss: 0.5137\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5c530bd877b473ab684bf10a1cc8316"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.4520\nVal Accuracy: 0.9968\nVal F1: 0.9965\n\n✓ New best model saved (acc: 0.9968)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add324ad2e6347fda2b9c9c351692128"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 6/12 - Train Loss: 0.4510\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a22bfdcb51449ac851ef9c91199cc35"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3984\nVal Accuracy: 0.9943\nVal F1: 0.9941\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8ef8c6859e4b98b7fd2fa4fa452095"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 7/12 - Train Loss: 0.4046\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e047b5c04f947bea65c105f0ee1791d"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3597\nVal Accuracy: 0.9951\nVal F1: 0.9949\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1810a09e2ad48c48dd10d46c0f84d04"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 8/12 - Train Loss: 0.3745\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6d00001e344ed7a5f5f47fac1c44e0"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3411\nVal Accuracy: 0.9951\nVal F1: 0.9949\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fdbc6efeeb04e73aa6a4690c731ee86"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 9/12 - Train Loss: 0.3549\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a490aceb6a864c09aa973c6a1704db9f"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3323\nVal Accuracy: 0.9943\nVal F1: 0.9943\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2806230446ae4233aa59703b3069cd49"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 10/12 - Train Loss: 0.3434\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50cef18340948ceb9bf1a961b0dec96"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3241\nVal Accuracy: 0.9943\nVal F1: 0.9943\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c9bb75e3fc4fe29e88edf9a17452df"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 11/12 - Train Loss: 0.3376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d75178d96f43c49b79ff5a92275c5a"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3219\nVal Accuracy: 0.9951\nVal F1: 0.9951\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5f5c3b3e44426ab630058e66141327"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 12/12 - Train Loss: 0.3354\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbb77d13b4d4e439b2c792ea90b4e6c"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3209\nVal Accuracy: 0.9951\nVal F1: 0.9951\n\nTraining complete! Best accuracy: 0.9968\n\n============================================================\nFINAL TEST\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad28af4444d542b9bd8a518d85f0d4dc"}},"metadata":{}},{"name":"stdout","text":"\n✓ Test Accuracy: 0.9959\n✓ Test F1 Macro: 0.9960\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n\n✓ Model saved to: /kaggle/working/bond_classifier_v3\n✓ Files: pytorch_model.bin, tokenizer files\n\nTo download:\n  1. Go to Output tab\n  2. Download 'bond_classifier_v3' folder\n  3. Use locally with your inference notebook\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport torch.nn.functional as F\nimport time\nimport torch.nn as nn\n\n# --- Model architecture (must match training) ---\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Same architecture as training: DeBERTa-v3-small + multi-task heads.\"\"\"\n    \n    def __init__(self, base_model: str = 'microsoft/deberta-v3-small', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # Heads: intent + sectors + rating + duration + constraints\n        # ⬇️ 15 intents now (13 bond + 2 non-bond)\n        self.intent_head = nn.Linear(feature_size, 15)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\nclass BondClassifier:\n    def __init__(self, model_path: str, base_model: str = \"microsoft/deberta-v3-small\"):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Load tokenizer saved during training\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        # Load model + weights\n        self.model = ProductionBondClassifier(base_model=base_model)\n        # NOTE: training script saved as 'pytorch_model.bin'\n        state_dict = torch.load(f\"{model_path}/pytorch_model.bin\", map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n\n        # This order MUST match BondQueryDataset.intent_to_id used in training\n        self.intent_names = [\n            'buy_recommendation',     # 0\n            'sell_recommendation',    # 1\n            'portfolio_analysis',     # 2\n            'reduce_duration',        # 3\n            'increase_yield',         # 4\n            'hedge_volatility',       # 5\n            'sector_rebalance',       # 6\n            'barbell_strategy',       # 7\n            'switch_bonds',           # 8\n            'explain_recommendation', # 9\n            'market_outlook',         # 10\n            'credit_analysis',        # 11\n            'forecast_prices',        # 12\n            'non_bond_search',        # 13 (NEW)\n            'non_bond_llm',           # 14 (NEW)\n        ]\n    \n    def _route_from_intent(self, intent: str) -> str:\n        \"\"\"\n        Simple router decision:\n        - 'non_bond_search' -> 'search'\n        - 'non_bond_llm'    -> 'llm'\n        - everything else   -> 'bond'\n        \"\"\"\n        if intent == 'non_bond_search':\n            return 'search'\n        elif intent == 'non_bond_llm':\n            return 'llm'\n        else:\n            return 'bond'\n    \n    def classify(self, query: str):\n        # Tokenize\n        enc = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        input_ids = enc[\"input_ids\"].to(self.device)\n        attention_mask = enc[\"attention_mask\"].to(self.device)\n        \n        # Run model\n        with torch.no_grad():\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            intent_logits = outputs['intent_logits']\n            intent_probs = F.softmax(intent_logits, dim=-1)\n            \n        # Get the predicted intent\n        intent_idx = intent_probs.argmax(dim=-1).item()\n        intent = self.intent_names[intent_idx]\n        confidence = intent_probs.max().item()\n        \n        route = self._route_from_intent(intent)\n        \n        return intent, confidence, route\n\n\n# --- Initialize the classifier ---\nMODEL_PATH = '/kaggle/working/bond_classifier_v3'  # Path to your trained model directory\nclassifier = BondClassifier(MODEL_PATH)\n\n# --- Input your query here ---\nquery = input(\"Enter your query: \")\n\nstart_time = time.time()\nintent, confidence, route = classifier.classify(query)\nend_time = time.time()\n\nprint(f\"Predicted Intent: {intent}\")\nprint(f\"Confidence: {confidence:.3f}\")\nprint(f\"Router decision: {route}  (bond/search/llm)\")\nprint(f\"Time taken for classification: {end_time - start_time:.4f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:59:34.556207Z","iopub.execute_input":"2025-11-30T13:59:34.556784Z","iopub.status.idle":"2025-11-30T13:59:52.243467Z","shell.execute_reply.started":"2025-11-30T13:59:34.556758Z","shell.execute_reply":"2025-11-30T13:59:52.242649Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your query:  which stocks should i buy\n"},{"name":"stdout","text":"Predicted Intent: buy_recommendation\nConfidence: 0.595\nRouter decision: bond  (bond/search/llm)\nTime taken for classification: 0.0141 seconds\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\nfrom collections import Counter\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, get_cosine_schedule_with_warmup\nfrom google import genai\n\n# ------------------ EDGE-STAGE CONFIG ------------------ #\nEDGE_CONFIG = {\n    \"train_path\": \"/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/train_extended.jsonl\",\n    \"val_path\":   \"/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/val_extended.jsonl\",\n    \"test_path\":  \"/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/test_extended.jsonl\",\n\n    # Stage-1 model dir (where pytorch_model.bin + tokenizer are)\n    \"base_model_dir\": \"/kaggle/working/bond_classifier_v3\",\n\n    # New dir for the edge-tuned model\n    \"output_dir\": \"/kaggle/working/bond_classifier_v3_edge\",\n\n    # Training hyperparams for stage-2\n    \"batch_size\": 32,\n    \"num_epochs\": 3,          # small number, this is just refinement\n    \"learning_rate\": 1e-5,    # a bit lower for continued fine-tuning\n    \"warmup_ratio\": 0.1,\n    \"max_length\": 128,\n\n    # Gemini edge-case generation\n    \"edge_model_name\": \"gemini-2.0-flash\",\n    \"edge_samples_per_intent\": 200,   # per non-bond intent\n    \"max_per_call\": 40,\n    \"seed\": 42,\n}\n\n# Make sure output dir exists\nPath(EDGE_CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n\n# Re-seed\nrandom.seed(EDGE_CONFIG[\"seed\"])\ntorch.manual_seed(EDGE_CONFIG[\"seed\"])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(EDGE_CONFIG[\"seed\"])\n\ndef load_jsonl(path: str | Path):\n    path = Path(path)\n    data = []\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                data.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:13:45.477777Z","iopub.execute_input":"2025-11-30T14:13:45.478574Z","iopub.status.idle":"2025-11-30T14:13:48.467708Z","shell.execute_reply.started":"2025-11-30T14:13:45.478547Z","shell.execute_reply":"2025-11-30T14:13:48.467116Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class NonBondEquityEdgeGenerator:\n    \"\"\"\n    Generate 'hard' non-bond queries that mention stocks / equities / cashflow / P&L,\n    explicitly labeled as:\n      - non_bond_search  (needs web / retrieval)\n      - non_bond_llm     (needs reasoning / drafting / explanation)\n    Schema matches the extended dataset (sectors = [], rating = null, etc.)\n    \"\"\"\n\n    def __init__(self, model_name: str, samples_per_intent: int, max_per_call: int = 40):\n        api_key = os.environ.get(\"GEMINI_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\"GEMINI_API_KEY not set in environment.\")\n        self.client = genai.Client(api_key=api_key)\n\n        self.model_name = model_name\n        self.samples_per_intent = samples_per_intent\n        self.max_per_call = max_per_call\n\n        # Things that should be present (equity world)\n        self.equity_keywords = [\n            \"stock\", \"stocks\", \"equity\", \"equities\", \"share\", \"shares\",\n            \"cash flow\", \"cashflow\", \"income statement\", \"balance sheet\",\n            \"p&l\", \"profit and loss\", \"eps\", \"earnings\", \"dividend\",\n            \"ipo\", \"market cap\", \"price to earnings\", \"p/e\"\n        ]\n        # Things that MUST NOT appear (bond / fixed-income terms)\n        self.forbidden_keywords = [\n            \"bond\", \"bonds\", \"debenture\", \"g-sec\", \"gsec\",\n            \"coupon\", \"fixed income\", \"yield curve\", \"sovereign bond\"\n        ]\n\n        self.intent_descriptions = {\n            \"non_bond_search\": (\n                \"Queries about stocks/equities/company financials that clearly ask to \"\n                \"search / fetch external information (news, filings, PDFs, data).\"\n            ),\n            \"non_bond_llm\": (\n                \"Queries about stocks/equities/company financials that are best answered \"\n                \"by reasoning / explanation / summarization / drafting, without explicit search.\"\n            ),\n        }\n\n    def generate_dataset(self):\n        all_rows = []\n        print(\"=\" * 60)\n        print(\"GENERATING EQUITY EDGE-CASE NON-BOND DATA (Gemini)\")\n        print(\"=\" * 60)\n\n        for intent_name in [\"non_bond_search\", \"non_bond_llm\"]:\n            needed = self.samples_per_intent\n            print(f\"\\n→ Intent: {intent_name}  target={needed}\")\n            while needed > 0:\n                batch_size = min(self.max_per_call, needed * 2)  # a bit extra to survive filtering\n                batch = self._generate_batch_for_intent(intent_name, batch_size)\n                all_rows.extend(batch)\n                needed -= len(batch)\n                print(f\"  Collected {self.samples_per_intent - needed}/{self.samples_per_intent} for {intent_name}\")\n\n        print(f\"\\n✓ Total equity edge-case rows: {len(all_rows)}\")\n        print(\"Label distribution:\", Counter(r[\"intent\"] for r in all_rows))\n        return all_rows\n\n    def _generate_batch_for_intent(self, intent_name: str, batch_size: int):\n        description = self.intent_descriptions[intent_name]\n\n        system_msg = (\n            \"You are generating synthetic training data for an intent classifier. \"\n            \"Create realistic user queries that are about STOCKS / EQUITIES / COMPANY FINANCIALS, \"\n            \"and NOT about bonds or fixed income. Output JSON Lines.\"\n        )\n\n        user_msg = f\"\"\"\nGenerate {batch_size} diverse user queries whose PRIMARY intent is: \"{intent_name}\".\n\nIntent description:\n{description}\n\nHard constraints:\n- Queries MUST be about stocks, equities, company fundamentals, earnings, P&L, cash flow,\n  financial statements, valuations, etc.\n- Queries MUST NOT be about bonds, debentures, sovereign debt, coupons, G-Secs or fixed income.\n- Make some explicitly data-fetch/search-like, others explanation/analysis-like as appropriate.\n\nFor \"non_bond_search\":\n- The user should clearly request search / fetch / lookup, e.g. \"search\", \"find\", \"look up\",\n  \"download\", \"get me the latest ...\".\n\nFor \"non_bond_llm\":\n- The user should clearly ask for explanation / comparison / summarization / drafting / brainstorming\n  where an LLM can answer from knowledge & reasoning (without explicit web search being required).\n\nReturn EXACTLY one JSON object per line (JSONL), no array, no commentary.\n\nEach JSON object MUST have:\n- \"text\": string, the user query\n- \"intent\": string, MUST be exactly \"{intent_name}\"\n- \"sectors\": empty array []\n- \"rating\": null\n- \"duration\": \"medium\"\n- \"constraints\": object with:\n  {{\n    \"preserve_yield\": false,\n    \"maintain_liquidity\": false,\n    \"avoid_downgrades\": false,\n    \"sector_diversity\": false,\n    \"rating_above_aa\": false\n  }}\n\"\"\"\n\n        full_prompt = system_msg + \"\\n\\n\" + user_msg\n        resp = self.client.models.generate_content(\n            model=self.model_name,\n            contents=full_prompt,\n        )\n        raw = resp.text or \"\"\n        return self._parse_jsonl(raw, intent_name)\n\n    def _parse_jsonl(self, raw: str, intent_name: str):\n        rows = []\n        for line in raw.splitlines():\n            line = line.strip()\n            if not line or line.startswith(\"```\"):\n                continue\n            if line.startswith(\"-\"):\n                line = line.lstrip(\"-\").strip()\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            sample = self._normalize_record(obj, intent_name)\n            if sample is not None:\n                rows.append(sample)\n        return rows\n\n    def _normalize_record(self, obj, intent_name: str):\n        text = str(obj.get(\"text\", \"\")).strip()\n        if not text:\n            return None\n\n        lower = text.lower()\n\n        # Must contain at least one equity keyword\n        if not any(kw in lower for kw in self.equity_keywords):\n            return None\n\n        # Must NOT contain any bond keyword\n        if any(kw in lower for kw in self.forbidden_keywords):\n            return None\n\n        # Force schema\n        sample = {\n            \"text\": text,\n            \"intent\": intent_name,\n            \"sectors\": [],\n            \"rating\": None,\n            \"duration\": \"medium\",\n            \"constraints\": {\n                \"preserve_yield\": False,\n                \"maintain_liquidity\": False,\n                \"avoid_downgrades\": False,\n                \"sector_diversity\": False,\n                \"rating_above_aa\": False,\n            },\n        }\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:13:51.869455Z","iopub.execute_input":"2025-11-30T14:13:51.870109Z","iopub.status.idle":"2025-11-30T14:13:51.885619Z","shell.execute_reply.started":"2025-11-30T14:13:51.870083Z","shell.execute_reply":"2025-11-30T14:13:51.884842Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"edge_gen = NonBondEquityEdgeGenerator(\n    model_name=EDGE_CONFIG[\"edge_model_name\"],\n    samples_per_intent=EDGE_CONFIG[\"edge_samples_per_intent\"],\n    max_per_call=EDGE_CONFIG[\"max_per_call\"],\n)\n\nequity_edge_cases = edge_gen.generate_dataset()\n\n# (Optional) Save for inspection\nedge_path = Path(\"/kaggle/working/equity_edge_cases.jsonl\")\nwith edge_path.open(\"w\", encoding=\"utf-8\") as f:\n    for r in equity_edge_cases:\n        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Saved equity edge cases to {edge_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:14:43.346449Z","iopub.execute_input":"2025-11-30T14:14:43.346754Z","iopub.status.idle":"2025-11-30T14:19:41.821314Z","shell.execute_reply.started":"2025-11-30T14:14:43.346729Z","shell.execute_reply":"2025-11-30T14:19:41.820417Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING EQUITY EDGE-CASE NON-BOND DATA (Gemini)\n============================================================\n\n→ Intent: non_bond_search  target=200\n  Collected 17/200 for non_bond_search\n  Collected 34/200 for non_bond_search\n  Collected 53/200 for non_bond_search\n  Collected 77/200 for non_bond_search\n  Collected 94/200 for non_bond_search\n  Collected 115/200 for non_bond_search\n  Collected 135/200 for non_bond_search\n  Collected 155/200 for non_bond_search\n  Collected 176/200 for non_bond_search\n  Collected 205/200 for non_bond_search\n\n→ Intent: non_bond_llm  target=200\n  Collected 34/200 for non_bond_llm\n  Collected 58/200 for non_bond_llm\n  Collected 94/200 for non_bond_llm\n  Collected 126/200 for non_bond_llm\n  Collected 161/200 for non_bond_llm\n  Collected 196/200 for non_bond_llm\n  Collected 204/200 for non_bond_llm\n\n✓ Total equity edge-case rows: 409\nLabel distribution: Counter({'non_bond_search': 205, 'non_bond_llm': 204})\nSaved equity edge cases to /kaggle/working/equity_edge_cases.jsonl\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Must match training + inference order\nINTENT_LABELS = [\n    \"buy_recommendation\",\n    \"sell_recommendation\",\n    \"portfolio_analysis\",\n    \"reduce_duration\",\n    \"increase_yield\",\n    \"hedge_volatility\",\n    \"sector_rebalance\",\n    \"barbell_strategy\",\n    \"switch_bonds\",\n    \"explain_recommendation\",\n    \"market_outlook\",\n    \"credit_analysis\",\n    \"forecast_prices\",\n    \"non_bond_search\",\n    \"non_bond_llm\",\n]\n\nSECTOR_TO_ID = {\n    \"Sovereign\": 0,\n    \"PSU Energy\": 1,\n    \"Financial\": 2,\n    \"Corporate\": 3,\n    \"Infrastructure\": 4,\n    \"NBFC\": 5,\n    \"Banking\": 6,\n}\n\nRATING_TO_ID = {\n    \"AAA\": 0,\n    \"AA+\": 1,\n    \"AA\": 2,\n    \"A+\": 3,\n    \"A\": 4,\n    \"BBB\": 5,\n    # 6 = \"other\"/None\n}\n\nDURATION_TO_ID = {\n    \"short\": 0,\n    \"medium\": 1,\n    \"long\": 2,\n}\n\nclass ExtendedBondQueryDataset(Dataset):\n    def __init__(self, records, tokenizer, max_length: int = 128):\n        self.records = records\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        self.intent_to_id = {name: i for i, name in enumerate(INTENT_LABELS)}\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        sample = self.records[idx]\n\n        text = sample[\"text\"]\n        enc = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        intent = sample[\"intent\"]\n        intent_id = self.intent_to_id[intent]\n\n        # sectors (multi-label)\n        sectors = sample.get(\"sectors\", []) or []\n        sector_labels = torch.zeros(len(SECTOR_TO_ID))\n        for s in sectors:\n            if s in SECTOR_TO_ID:\n                sector_labels[SECTOR_TO_ID[s]] = 1.0\n\n        # rating (single class)\n        rating = sample.get(\"rating\")\n        rating_id = RATING_TO_ID.get(rating, 6)  # 6 = \"other\"/None\n\n        # duration\n        duration = str(sample.get(\"duration\", \"medium\")).lower()\n        duration_id = DURATION_TO_ID.get(duration, 1)\n\n        # constraints\n        constraints = sample.get(\"constraints\", {}) or {}\n        constraint_labels = torch.tensor([\n            float(constraints.get(\"preserve_yield\", False)),\n            float(constraints.get(\"maintain_liquidity\", False)),\n            float(constraints.get(\"avoid_downgrades\", False)),\n            float(constraints.get(\"sector_diversity\", False)),\n            float(constraints.get(\"rating_above_aa\", False)),\n        ])\n\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"intent_label\": torch.tensor(intent_id, dtype=torch.long),\n            \"sector_labels\": sector_labels,\n            \"rating_label\": torch.tensor(rating_id, dtype=torch.long),\n            \"duration_label\": torch.tensor(duration_id, dtype=torch.long),\n            \"constraint_labels\": constraint_labels,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:19:41.822445Z","iopub.execute_input":"2025-11-30T14:19:41.822675Z","iopub.status.idle":"2025-11-30T14:19:41.833462Z","shell.execute_reply.started":"2025-11-30T14:19:41.822657Z","shell.execute_reply":"2025-11-30T14:19:41.832698Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def continue_finetune_with_equity_edges():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Device:\", device)\n\n    # ---- 1) Load base splits ----\n    train_records = load_jsonl(EDGE_CONFIG[\"train_path\"])\n    val_records   = load_jsonl(EDGE_CONFIG[\"val_path\"])\n    test_records  = load_jsonl(EDGE_CONFIG[\"test_path\"])\n\n    print(f\"Base train size: {len(train_records)}\")\n    print(f\"Base val size:   {len(val_records)}\")\n    print(f\"Base test size:  {len(test_records)}\")\n\n    # ---- 2) Generate & append equity edge-cases (already created above) ----\n    global equity_edge_cases\n    if \"equity_edge_cases\" not in globals():\n        edge_gen = NonBondEquityEdgeGenerator(\n            model_name=EDGE_CONFIG[\"edge_model_name\"],\n            samples_per_intent=EDGE_CONFIG[\"edge_samples_per_intent\"],\n            max_per_call=EDGE_CONFIG[\"max_per_call\"],\n        )\n        equity_edge_cases = edge_gen.generate_dataset()\n\n    print(f\"Appending {len(equity_edge_cases)} equity edge-case samples to TRAIN only.\")\n    train_records_extended = train_records + equity_edge_cases\n\n    # ---- 3) Tokenizer ----\n    # Use tokenizer saved from stage-1 (or base model)\n    if Path(EDGE_CONFIG[\"base_model_dir\"]).exists():\n        tokenizer = AutoTokenizer.from_pretrained(EDGE_CONFIG[\"base_model_dir\"])\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model\"])\n\n    # ---- 4) Datasets & loaders ----\n    train_dataset = ExtendedBondQueryDataset(\n        train_records_extended, tokenizer, EDGE_CONFIG[\"max_length\"]\n    )\n    val_dataset   = ExtendedBondQueryDataset(\n        val_records, tokenizer, EDGE_CONFIG[\"max_length\"]\n    )\n    test_dataset  = ExtendedBondQueryDataset(\n        test_records, tokenizer, EDGE_CONFIG[\"max_length\"]\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=EDGE_CONFIG[\"batch_size\"],\n        shuffle=True,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=EDGE_CONFIG[\"batch_size\"],\n    )\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=EDGE_CONFIG[\"batch_size\"],\n    )\n\n    # ---- 5) Load model from previous checkpoint ----\n    from pathlib import Path as _Path\n\n    base_ckpt = _Path(EDGE_CONFIG[\"base_model_dir\"]) / \"pytorch_model.bin\"\n    if not base_ckpt.exists():\n        raise FileNotFoundError(f\"Base checkpoint not found: {base_ckpt}\")\n\n    model = ProductionBondClassifier(base_model=CONFIG[\"base_model\"])\n    state_dict = torch.load(base_ckpt, map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device)\n\n    print(f\"Loaded base model from {base_ckpt}\")\n\n    # ---- 6) Optimizer & scheduler ----\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=EDGE_CONFIG[\"learning_rate\"],\n        weight_decay=0.01,\n    )\n\n    num_training_steps = len(train_loader) * EDGE_CONFIG[\"num_epochs\"]\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * EDGE_CONFIG[\"warmup_ratio\"]),\n        num_training_steps=num_training_steps,\n    )\n\n    criterion = MultiTaskLoss()\n\n    # ---- 7) Trainer (reuse your existing Trainer class) ----\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        criterion=criterion,\n        device=device,\n        output_dir=EDGE_CONFIG[\"output_dir\"],\n    )\n\n    trainer.train(EDGE_CONFIG[\"num_epochs\"])\n\n    # ---- 8) Final test evaluation ----\n    print(\"=\" * 60)\n    print(\"FINAL TEST (after equity edge-case fine-tuning)\")\n    print(\"=\" * 60)\n\n    model.eval()\n    all_preds, all_labels = [], []\n\n    from sklearn.metrics import accuracy_score, f1_score\n    from tqdm.auto import tqdm as _tqdm\n\n    with torch.no_grad():\n        for batch in _tqdm(test_loader, desc=\"Testing\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = outputs[\"intent_logits\"].argmax(dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch[\"intent_label\"].numpy())\n\n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n\n    print(f\"\\n✓ Test Accuracy: {test_acc:.4f}\")\n    print(f\"✓ Test F1 Macro: {test_f1:.4f}\\n\")\n\n    # ---- 9) Save edge-tuned checkpoint ----\n    edge_ckpt = Path(EDGE_CONFIG[\"output_dir\"]) / \"pytorch_model.bin\"\n    edge_ckpt_edge_name = Path(EDGE_CONFIG[\"output_dir\"]) / \"pytorch_model_edge.bin\"\n\n    if edge_ckpt.exists():\n        import shutil\n        shutil.copy(edge_ckpt, edge_ckpt_edge_name)\n        print(f\"✓ Edge-tuned model saved to: {edge_ckpt}\")\n        print(f\"✓ Also copied to: {edge_ckpt_edge_name}\")\n    else:\n        print(\"⚠ Trainer did not save a checkpoint named 'pytorch_model.bin'?! Check Trainer.save_checkpoint logic.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:19:41.834325Z","iopub.execute_input":"2025-11-30T14:19:41.834545Z","iopub.status.idle":"2025-11-30T14:19:41.858528Z","shell.execute_reply.started":"2025-11-30T14:19:41.834529Z","shell.execute_reply":"2025-11-30T14:19:41.857784Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"continue_finetune_with_equity_edges()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:19:51.414070Z","iopub.execute_input":"2025-11-30T14:19:51.414367Z","iopub.status.idle":"2025-11-30T14:24:36.133483Z","shell.execute_reply.started":"2025-11-30T14:19:51.414347Z","shell.execute_reply":"2025-11-30T14:24:36.132829Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Device: cuda\nBase train size: 5749\nBase val size:   1232\nBase test size:  1232\nAppending 409 equity edge-case samples to TRAIN only.\nLoaded base model from /kaggle/working/bond_classifier_v3/pytorch_model.bin\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/193 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f768226a39e4c7f938bb99bd2149ecf"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/3 - Train Loss: 0.4444\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b167fd627e8a4e60a360e0f80dbe7ad2"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.4198\nVal Accuracy: 0.9943\nVal F1: 0.9940\n\n✓ New best model saved (acc: 0.9943)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/193 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"513582d11b484f86b0820768afbad59e"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2/3 - Train Loss: 0.4035\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc092e0c0bf47a4b4180f67638f65d9"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3909\nVal Accuracy: 0.9951\nVal F1: 0.9948\n\n✓ New best model saved (acc: 0.9951)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/193 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95ec677f3e147a6aa089e5c0b24e5db"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3/3 - Train Loss: 0.3849\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e348a5b454441dbf0bcb026b9e2a23"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.3850\nVal Accuracy: 0.9943\nVal F1: 0.9940\n\nTraining complete! Best accuracy: 0.9951\n\n============================================================\nFINAL TEST (after equity edge-case fine-tuning)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"099e52f8ebf643779c43b420404c4769"}},"metadata":{}},{"name":"stdout","text":"\n✓ Test Accuracy: 0.9943\n✓ Test F1 Macro: 0.9940\n\n✓ Edge-tuned model saved to: /kaggle/working/bond_classifier_v3_edge/pytorch_model.bin\n✓ Also copied to: /kaggle/working/bond_classifier_v3_edge/pytorch_model_edge.bin\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport torch.nn.functional as F\nimport time\nimport torch.nn as nn\n\n# ---------------------------------------------------------\n# CONFIG: choose which model to load\n# ---------------------------------------------------------\nUSE_EDGE_MODEL = True  # True = 15-intent edge model, False = original 13-intent model\n\nBASE_MODEL_DIR = \"/kaggle/working/bond_classifier_v3\"\nEDGE_MODEL_DIR = \"/kaggle/working/bond_classifier_v3_edge\"\nBASE_MODEL_NAME = \"microsoft/deberta-v3-small\"\nCHECKPOINT_NAME = \"pytorch_model.bin\"  # both training scripts save under this name\n\n\n# ---------------------------------------------------------\n# MODEL ARCHITECTURE (shared, but num_intents is configurable)\n# ---------------------------------------------------------\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"\n    Same backbone as training (DeBERTa-v3-small + multi-task heads),\n    but with configurable number of intent classes so we can support:\n      - 13 intents  (original model)\n      - 15 intents  (edge model: + non_bond_search, non_bond_llm)\n    \"\"\"\n    def __init__(\n        self,\n        base_model: str = BASE_MODEL_NAME,\n        dropout: float = 0.15,\n        num_intents: int = 13,\n    ):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # Heads: intent + sectors + rating + duration + constraints\n        self.intent_head = nn.Linear(feature_size, num_intents)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ---------------------------------------------------------\n# WRAPPER CLASS\n# ---------------------------------------------------------\nclass BondClassifier:\n    def __init__(self, use_edge_model: bool = True, base_model: str = BASE_MODEL_NAME):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.use_edge_model = use_edge_model\n        \n        # Choose which directory & label set to use\n        if self.use_edge_model:\n            self.model_path = EDGE_MODEL_DIR\n            self.num_intents = 15\n            # 13 bond intents + 2 non-bond intents\n            self.intent_names = [\n                'buy_recommendation',     # 0\n                'sell_recommendation',    # 1\n                'portfolio_analysis',     # 2\n                'reduce_duration',        # 3\n                'increase_yield',         # 4\n                'hedge_volatility',       # 5\n                'sector_rebalance',       # 6\n                'barbell_strategy',       # 7\n                'switch_bonds',           # 8\n                'explain_recommendation', # 9\n                'market_outlook',         # 10\n                'credit_analysis',        # 11\n                'forecast_prices',        # 12\n                'non_bond_search',        # 13\n                'non_bond_llm',           # 14\n            ]\n        else:\n            self.model_path = BASE_MODEL_DIR\n            self.num_intents = 13\n            # Original 13 bond intents\n            self.intent_names = [\n                'buy_recommendation',     # 0\n                'sell_recommendation',    # 1\n                'portfolio_analysis',     # 2\n                'reduce_duration',        # 3\n                'increase_yield',         # 4\n                'hedge_volatility',       # 5\n                'sector_rebalance',       # 6\n                'barbell_strategy',       # 7\n                'switch_bonds',           # 8\n                'explain_recommendation', # 9\n                'market_outlook',         # 10\n                'credit_analysis',        # 11\n                'forecast_prices',        # 12\n            ]\n        \n        # ---- Load tokenizer ----\n        # In training you used AutoTokenizer.from_pretrained(CONFIG['base_model']),\n        # so for inference we can safely do the same:\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n        \n        # ---- Load model + weights ----\n        self.model = ProductionBondClassifier(\n            base_model=base_model,\n            num_intents=self.num_intents,\n        )\n        ckpt_path = os.path.join(self.model_path, CHECKPOINT_NAME)\n        if not os.path.exists(ckpt_path):\n            raise FileNotFoundError(f\"Checkpoint not found at: {ckpt_path}\")\n        \n        state_dict = torch.load(ckpt_path, map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def _route_from_intent(self, intent: str) -> str:\n        \"\"\"\n        Simple router decision:\n        - 'non_bond_search' -> 'search'\n        - 'non_bond_llm'    -> 'llm'\n        - everything else   -> 'bond'\n        \n        For the original 13-intent model, you will never see non_bond_*,\n        so route will always be 'bond'.\n        \"\"\"\n        if intent == 'non_bond_search':\n            return 'search'\n        elif intent == 'non_bond_llm':\n            return 'llm'\n        else:\n            return 'bond'\n    \n    def classify(self, query: str):\n        # Tokenize\n        enc = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        input_ids = enc[\"input_ids\"].to(self.device)\n        attention_mask = enc[\"attention_mask\"].to(self.device)\n        \n        # Run model\n        with torch.no_grad():\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            intent_logits = outputs['intent_logits']\n            intent_probs = F.softmax(intent_logits, dim=-1)\n            \n        # Get the predicted intent\n        intent_idx = intent_probs.argmax(dim=-1).item()\n        intent = self.intent_names[intent_idx]\n        confidence = intent_probs.max().item()\n        \n        route = self._route_from_intent(intent)\n        \n        return intent, confidence, route\n\n\n# ---------------------------------------------------------\n# USAGE\n# ---------------------------------------------------------\nif __name__ == \"__main__\":\n    # Toggle USE_EDGE_MODEL at the top to switch between:\n    # - original 13-intent model\n    # - 15-intent edge model with non-bond routing\n    classifier = BondClassifier(use_edge_model=USE_EDGE_MODEL)\n    \n    query = input(\"Enter your query: \")\n    \n    start_time = time.time()\n    intent, confidence, route = classifier.classify(query)\n    end_time = time.time()\n    \n    print(f\"\\nModel: {'EDGE (15 intents)' if USE_EDGE_MODEL else 'BASE (13 intents)'}\")\n    print(f\"Predicted Intent: {intent}\")\n    print(f\"Confidence: {confidence:.3f}\")\n    print(f\"Router decision: {route}  (bond/search/llm)\")\n    print(f\"Time taken for classification: {end_time - start_time:.4f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:37:50.147773Z","iopub.execute_input":"2025-11-30T14:37:50.148415Z","iopub.status.idle":"2025-11-30T14:37:59.076604Z","shell.execute_reply.started":"2025-11-30T14:37:50.148391Z","shell.execute_reply":"2025-11-30T14:37:59.075803Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your query:  which stocks should i buy\n"},{"name":"stdout","text":"\nModel: EDGE (15 intents)\nPredicted Intent: buy_recommendation\nConfidence: 0.428\nRouter decision: bond  (bond/search/llm)\nTime taken for classification: 0.0145 seconds\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom collections import Counter\n\nfrom google import genai\nfrom tqdm.auto import tqdm\n\n# ============================================================\n# GENERAL NON-BOND EDGE CASE CONFIG\n# ============================================================\nGENERAL_NON_BOND_CONFIG = {\n    \"model\": \"gemini-2.0-flash\",\n    \"samples_per_intent\": 500,      # 500 non_bond_search + 500 non_bond_llm = 1000 rows\n    \"max_per_call\": 50,             # 50 per request -> 10 calls per intent -> 20 calls total\n    \"output_path\": Path(\"/kaggle/working/general_non_bond_edge_cases.jsonl\"),\n}\n\nassert os.environ.get(\"GEMINI_API_KEY\"), \"Set GEMINI_API_KEY in Kaggle before running this cell.\"\n\n\n# ============================================================\n# GENERAL NON-BOND EDGE GENERATOR\n# ============================================================\n\nclass GeneralNonBondEdgeGenerator:\n    \"\"\"\n    Generate HARD / GENERAL non-bond edge cases, complementary to your existing\n    equity_edge_cases.jsonl and extended bond datasets.\n\n    Intents:\n      - non_bond_search : clearly needs external data / retrieval\n      - non_bond_llm    : primarily needs LLM reasoning / generation\n    \"\"\"\n\n    def __init__(self, model_name: str, samples_per_intent: int, max_per_call: int = 50):\n        self.client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n        self.model_name = model_name\n        self.samples_per_intent = samples_per_intent\n        self.max_per_call = max_per_call\n\n    # ---------------- CORE PUBLIC API ----------------\n\n    def generate_dataset(self) -> List[Dict[str, Any]]:\n        print(\"=\" * 60)\n        print(\"GENERATING GENERAL NON-BOND EDGE CASE DATA (Gemini)\")\n        print(\"=\" * 60)\n\n        all_rows: List[Dict[str, Any]] = []\n\n        # 1) non_bond_search\n        print(\"\\n→ Generating for intent: non_bond_search\")\n        all_rows.extend(\n            self._generate_for_intent(\n                intent_name=\"non_bond_search\",\n                total_needed=self.samples_per_intent,\n            )\n        )\n\n        # 2) non_bond_llm\n        print(\"\\n→ Generating for intent: non_bond_llm\")\n        all_rows.extend(\n            self._generate_for_intent(\n                intent_name=\"non_bond_llm\",\n                total_needed=self.samples_per_intent,\n            )\n        )\n\n        print(f\"\\n✓ Total general non-bond rows: {len(all_rows)}\")\n        return all_rows\n\n    # ---------------- PROMPT BUILDING ----------------\n\n    def _general_edge_scenarios_block(self) -> str:\n        \"\"\"\n        Big-ass description of all the non-bond edge-case themes.\n        We DELIBERATELY de-emphasize pure bond stuff, and also avoid\n        duplicating only equity cases (you already have those).\n        \"\"\"\n        return \"\"\"\nYou are generating HARD / GENERAL non-bond edge cases for a router in a bond assistant.\n\nThe model must learn that the following are NOT bond-specific tasks, even if they mention\nrates, interest, duration, yield, or \"bond\" in a non-finance sense.\n\nSpread examples across these themes (roughly evenly):\n\n1) Banking & Retail Credit (non-bond)\n   - Home loans, car loans, personal loans, credit cards, overdrafts, BNPL, EMIs,\n     education loans, loan refinancing, etc.\n   - Example: \"Should I prepay my home loan or invest extra money in an index fund?\"\n   - Example: \"Compare SBI and HDFC credit card reward programs for frequent flyers\"\n   - Example: \"Refinance my 12% car loan to 9%, is it worth it?\"\n\n2) Deposits & Cash Management\n   - Savings accounts, fixed deposits (FDs), recurring deposits (RDs), liquid funds,\n     sweep accounts, money parked in current accounts.\n   - Example: \"Find top 5 bank FDs above 7.5% for 2 years\"\n   - Example: \"Is a liquid fund better than savings account for my emergency corpus?\"\n\n3) Personal Finance & Planning\n   - Budgeting, emergency funds, retirement planning (EPF, NPS, 401k equivalents),\n     insurance (life, health, term, ULIPs), tax saving choices.\n   - Example: \"How much emergency fund should I keep as a salaried employee in India?\"\n   - Example: \"Compare ELSS vs PPF vs NPS for tax-saving and long-term returns\"\n   - Example: \"What health insurance riders should I consider for my parents?\"\n\n4) Corporate Finance & Business Decisions (non-bond focus)\n   - CAPEX decisions, payback, NPV, IRR, working capital management, pricing, unit economics,\n     project appraisal, capital structure with minimal bond focus.\n   - Example: \"Calculate NPV and IRR for this factory project with given cashflows\"\n   - Example: \"Explain working capital cycle for an FMCG distributor\"\n   - Example: \"How does issuing ESOPs dilute existing shareholders?\"\n\n5) Accounting & Reporting\n   - Income statement, balance sheet, cashflow statement, ratios, audit issues,\n     revenue recognition, lease accounting, provisions.\n   - Example: \"Why can net profit be positive while operating cashflow is negative?\"\n   - Example: \"Explain difference between EBITDA, EBIT, and EBT using examples\"\n   - Example: \"How are leases treated under Ind AS / IFRS 16?\"\n\n6) Macroeconomics, FX, Commodities, Crypto\n   - Inflation, GDP, currency exchange, oil prices, gold, crypto, but not bond portfolios\n     as the main topic.\n   - Example: \"How does rupee depreciation impact import-heavy companies?\"\n   - Example: \"Explain halvings in Bitcoin and their historical impact on price\"\n   - Example: \"What happens to emerging market currencies when US Fed hikes rates?\"\n\n7) General ChatGPT-style Non-Finance Tasks\n   - Coding, data science, math, essays, productivity, career advice, travel itineraries,\n     email drafting, etc.\n   - Example: \"Write a Python script to compute XIRR for irregular cashflows\"\n   - Example: \"Plan a 4-day trip in Kerala with a budget of 25k\"\n   - Example: \"Draft a professional email to negotiate a higher salary offer\"\n\n8) Non-Financial Uses of the Word \"bond\"\n   - Chemical bonds, social bonds, emotional bonding, James Bond, adhesives, materials.\n   - Example: \"Explain covalent vs ionic bonds with daily life analogies\"\n   - Example: \"Rank James Bond actors by critical acclaim\"\n   - Example: \"How to build a stronger emotional bond with my team at work?\"\n\n9) Mixed / Ambiguous Queries where bonds are NOT the primary focus\n   - Multi-asset portfolios where equities, gold, real estate dominate and bonds are\n     a side note.\n   - Example: \"I have 60% in equity mutual funds, 20% in gold, 10% in FDs, 10% in RBI bonds,\n               is my allocation too risky?\"\n   - Example: \"Design a barbell between growth stocks and high-quality FDs\"\n   - Here, ensure the main question is broader than just 'optimize bonds'.\n\nCRITICAL RULE:\n- The PRIMARY topic must NOT be a pure fixed-income bond portfolio task.\n- It's okay if the user mentions 'interest rate', 'yield', or even 'bond' casually,\n  but the assistant should NOT treat this as a core bond-investing query.\n\"\"\"\n\n    def _intent_hint_block(self, intent_name: str) -> str:\n        if intent_name == \"non_bond_search\":\n            return \"\"\"\nFor this batch, the \\\"intent\\\" is \\\"non_bond_search\\\".\n\nqueries MUST:\n- clearly ask to LOOK UP or FETCH external information:\n  * live or historical prices, charts, rates\n  * bank product details, loan offers, FD rates\n  * PDFs, statements, filings, FAQs, support pages, regulations\n  * lists and screeners (e.g., top funds, best FDs above X%, etc.)\n- Use verbs like: \"find\", \"search\", \"look up\", \"show me\", \"get\", \"download\", \"list\",\n  \"screen for\", \"pull data for\", \"fetch\".\n\"\"\"\n        else:\n            return \"\"\"\nFor this batch, the \\\"intent\\\" is \\\"non_bond_llm\\\".\n\nqueries MUST:\n- primarily ask for REASONING, EXPLANATION, or GENERATION, not raw data fetch:\n  * explain, summarize, compare, analyze, interpret, critique\n  * calculate NPV/IRR on given numbers\n  * draft / rewrite emails or documents\n  * provide frameworks, pros/cons, structured advice\n- Use verbs like: \"explain\", \"summarize\", \"compare\", \"analyze\", \"walk me through\",\n  \"draft\", \"rewrite\", \"brainstorm\", \"advise\", \"help me understand\".\n\"\"\"\n\n    # ---------------- GENERATION LOOPS ----------------\n\n    def _generate_for_intent(self, intent_name: str, total_needed: int) -> List[Dict[str, Any]]:\n        rows: List[Dict[str, Any]] = []\n        remaining = total_needed\n\n        print(f\"  Target samples for {intent_name}: {total_needed}\")\n        pbar = tqdm(total=total_needed, desc=f\"Generating {intent_name}\")\n\n        while remaining > 0:\n            batch_size = min(self.max_per_call, remaining)\n            batch = self._generate_batch_for_intent(intent_name, batch_size)\n            rows.extend(batch)\n            got = len(batch)\n            remaining -= got\n            pbar.update(got)\n\n        pbar.close()\n        print(f\"  ✓ Collected {len(rows)} samples for {intent_name}\")\n        return rows\n\n    def _generate_batch_for_intent(self, intent_name: str, batch_size: int) -> List[Dict[str, Any]]:\n        system_msg = (\n            \"You are generating synthetic training data for an intent classifier used \"\n            \"by a bond-investing assistant.\\n\"\n            \"The goal is to produce non-bond edge cases that should be routed AWAY from \"\n            \"the bond-specific engine and towards either: search or a general LLM.\"\n        )\n\n        user_msg = f\"\"\"\nGenerate {batch_size} diverse user queries whose PRIMARY intent is: \"{intent_name}\".\n\n{self._intent_hint_block(intent_name)}\n\n{self._general_edge_scenarios_block()}\n\nGlobal constraints:\n- HARD RULE: Do NOT make the main task \"optimize a bond portfolio\", \"find bonds\", or\n  \"recommend which bond to buy/sell\". Those are handled elsewhere.\n- It's okay to mention bonds in passing, but the core question must be about:\n  banking, loans, deposits, accounting, macro, personal finance, generic tasks, or\n  non-financial 'bond'.\n- Vary user persona: retail, HNI, student, small business owner, CFO, startup founder,\n  non-finance person, etc.\n- Vary style and length: short prompts (4–8 words) and longer 1–3 sentence prompts.\n\nReturn the output STRICTLY in JSON Lines format:\n- One JSON object per line\n- No surrounding array\n- No backticks or code fences\n- No commentary\n\nEach JSON object MUST have exactly these keys:\n- \"text\": string, the user query\n- \"intent\": string, MUST be exactly \"{intent_name}\"\n- \"sectors\": array of strings, MUST be an empty array [] for these non-bond queries\n- \"rating\": null\n- \"duration\": string, ALWAYS \"medium\" for these non-bond queries\n- \"constraints\": object with boolean fields, ALL false:\n  {{\n    \"preserve_yield\": false,\n    \"maintain_liquidity\": false,\n    \"avoid_downgrades\": false,\n    \"sector_diversity\": false,\n    \"rating_above_aa\": false\n  }}\n\"\"\"\n\n        full_prompt = system_msg + \"\\n\\n\" + user_msg\n\n        resp = self.client.models.generate_content(\n            model=self.model_name,\n            contents=full_prompt,\n        )\n\n        raw = resp.text or \"\"\n        return self._parse_jsonl(raw, intent_name)\n\n    # ---------------- PARSING & NORMALIZATION ----------------\n\n    def _parse_jsonl(self, raw: str, intent_name: str) -> List[Dict[str, Any]]:\n        rows: List[Dict[str, Any]] = []\n        for line in raw.splitlines():\n            line = line.strip()\n            if not line or line.startswith(\"```\"):\n                continue\n            if line.startswith(\"-\"):\n                line = line.lstrip(\"-\").strip()\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            sample = self._normalize_record(obj, intent_name)\n            if sample is not None:\n                rows.append(sample)\n        return rows\n\n    def _normalize_record(self, obj: Dict[str, Any], intent_name: str) -> Optional[Dict[str, Any]]:\n        text = str(obj.get(\"text\", \"\")).strip()\n        if not text:\n            return None\n\n        # We override everything to match your schema exactly\n        sample = {\n            \"text\": text,\n            \"intent\": intent_name,\n            \"sectors\": [],          # always empty (non-bond)\n            \"rating\": None,         # always null\n            \"duration\": \"medium\",   # fixed\n            \"constraints\": {\n                \"preserve_yield\": False,\n                \"maintain_liquidity\": False,\n                \"avoid_downgrades\": False,\n                \"sector_diversity\": False,\n                \"rating_above_aa\": False,\n            },\n        }\n        return sample\n\n\n# ============================================================\n# RUN GENERATION + SAVE\n# ============================================================\n\ngen_nonbond = GeneralNonBondEdgeGenerator(\n    model_name=GENERAL_NON_BOND_CONFIG[\"model\"],\n    samples_per_intent=GENERAL_NON_BOND_CONFIG[\"samples_per_intent\"],\n    max_per_call=GENERAL_NON_BOND_CONFIG[\"max_per_call\"],\n)\n\ngeneral_edge_data = gen_nonbond.generate_dataset()\n\nprint(\"\\nLabel counts in general non-bond edge dataset:\")\nprint(Counter(row[\"intent\"] for row in general_edge_data))\n\nout_path = GENERAL_NON_BOND_CONFIG[\"output_path\"]\nout_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith out_path.open(\"w\", encoding=\"utf-8\") as f:\n    for row in general_edge_data:\n        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n\nprint(f\"\\n✓ Saved general non-bond edge dataset to: {out_path}\")\nprint(f\"  Total rows: {len(general_edge_data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:53:11.420301Z","iopub.execute_input":"2025-11-30T14:53:11.420615Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING GENERAL NON-BOND EDGE CASE DATA (Gemini)\n============================================================\n\n→ Generating for intent: non_bond_search\n  Target samples for non_bond_search: 500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating non_bond_search:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d1229fb31e493fb070d39a79e658fb"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 500 samples for non_bond_search\n\n→ Generating for intent: non_bond_llm\n  Target samples for non_bond_llm: 500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating non_bond_llm:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7865a9f2395345fa8781f090af15a1a0"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom collections import Counter\n\nfrom google import genai\nfrom tqdm.auto import tqdm\n\n# ============================================================\n# EXTRA BOND EDGE-CASE CONFIG\n# ============================================================\nBOND_EDGE_CONFIG = {\n    \"model\": \"gemini-2.0-flash\",\n    \"samples_per_intent\": 180,       # per bond intent (13 intents -> 2340 rows)\n    \"max_per_call\": 60,              # 3 calls/intent -> 39 calls total\n    \"output_path\": Path(\"/kaggle/working/bond_edge_cases.jsonl\"),\n}\n\nassert os.environ.get(\"GEMINI_API_KEY\"), \"Set GEMINI_API_KEY in Kaggle before running this cell.\"\n\n\n# ============================================================\n# EDGE-CASE BOND INTENT GENERATOR (GEMINI)\n# ============================================================\n\nclass BondEdgeLLMDataGenerator:\n    \"\"\"\n    Generate *extra* synthetic bond queries for all 13 bond intents.\n\n    Goal:\n    - Harder, more realistic, messy edge cases\n    - Multi-constraint queries, combined objectives, follow-ups, ambiguous wording\n    - Still CLEAN, labelled, and consistent with your 13-intent schema.\n\n    Output schema (per row):\n      - text: str\n      - intent: one of the 13 bond intents\n      - sectors: list[str] subset of\n            [\"Sovereign\", \"PSU Energy\", \"Financial\", \"Corporate\",\n             \"Infrastructure\", \"NBFC\", \"Banking\"]\n      - rating: null or one of [\"AAA\",\"AA+\",\"AA\",\"A+\",\"A\",\"BBB\"]\n      - duration: \"short\" | \"medium\" | \"long\"\n      - constraints: dict with 5 boolean keys\n    \"\"\"\n\n    BOND_INTENT_NAMES = [\n        \"buy_recommendation\",\n        \"sell_recommendation\",\n        \"portfolio_analysis\",\n        \"reduce_duration\",\n        \"increase_yield\",\n        \"hedge_volatility\",\n        \"sector_rebalance\",\n        \"barbell_strategy\",\n        \"switch_bonds\",\n        \"explain_recommendation\",\n        \"market_outlook\",\n        \"credit_analysis\",\n        \"forecast_prices\",\n    ]\n\n    def __init__(self, model_name: str, samples_per_intent: int, max_per_call: int = 60):\n        api_key = os.environ.get(\"GEMINI_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\"GEMINI_API_KEY not set in environment\")\n\n        self.client = genai.Client(api_key=api_key)\n        self.model_name = model_name\n        self.samples_per_intent = samples_per_intent\n        self.max_per_call = max_per_call\n\n        self.allowed_sectors = [\n            \"Sovereign\", \"PSU Energy\", \"Financial\",\n            \"Corporate\", \"Infrastructure\", \"NBFC\", \"Banking\"\n        ]\n        self.allowed_ratings = [\"AAA\", \"AA+\", \"AA\", \"A+\", \"A\", \"BBB\"]\n        self.allowed_durations = [\"short\", \"medium\", \"long\"]\n\n        # Base descriptions (reusing your intent semantics)\n        self.intent_descriptions = {\n            \"buy_recommendation\": \"User wants recommendations of which bonds to BUY, often with preferences about sector, rating, duration, yield, liquidity, or risk tolerance.\",\n            \"sell_recommendation\": \"User wants to know what to SELL or whether to exit certain bonds or positions.\",\n            \"portfolio_analysis\": \"User wants analysis or diagnosis of their current bond portfolio: exposures, duration, sector mix, risks, diversification.\",\n            \"reduce_duration\": \"User wants to reduce INTEREST-RATE risk/duration of their bond portfolio while staying invested.\",\n            \"increase_yield\": \"User wants to increase portfolio YIELD, often trading off some quality/liquidity or duration.\",\n            \"hedge_volatility\": \"User wants to hedge or reduce the impact of bond price volatility or interest-rate moves.\",\n            \"sector_rebalance\": \"User wants to rebalance SECTOR allocation or reduce concentration in particular sectors/issuers.\",\n            \"barbell_strategy\": \"User wants a BARBELL strategy (mix of short + long duration bonds, sometimes with constraints).\",\n            \"switch_bonds\": \"User wants to switch from one BOND or issuer to another similar but better bond.\",\n            \"explain_recommendation\": \"User wants EXPLANATION / rationale behind some earlier recommendation or trade idea.\",\n            \"market_outlook\": \"User wants OUTLOOK on bond markets, yields, spreads, central bank policy, etc.\",\n            \"credit_analysis\": \"User wants analysis of CREDIT quality, default risk, downgrade risk or rating outlook.\",\n            \"forecast_prices\": \"User wants explicit FORECASTS of future bond prices, yields, or total returns.\",\n        }\n\n    # ---------------- PUBLIC API ----------------\n\n    def generate_dataset(self) -> List[Dict[str, Any]]:\n        print(\"=\" * 60)\n        print(\"GENERATING EXTRA BOND EDGE-CASE DATA (Gemini)\")\n        print(\"=\" * 60)\n\n        all_samples: List[Dict[str, Any]] = []\n\n        for intent_name in self.BOND_INTENT_NAMES:\n            print(f\"\\n→ Intent: {intent_name} (target {self.samples_per_intent})\")\n            intent_samples = self._generate_for_intent(intent_name, self.samples_per_intent)\n            all_samples.extend(intent_samples)\n\n        print(f\"\\n✓ Generated {len(all_samples)} extra bond edge-case samples\\n\")\n        return all_samples\n\n    # ---------------- PROMPT HELPERS ----------------\n\n    def _edge_case_hint_block(self, intent_name: str) -> str:\n        \"\"\"\n        Extra guidance per intent to push Gemini towards edge-case scenarios.\n        \"\"\"\n        if intent_name == \"buy_recommendation\":\n            return \"\"\"\nEdge-case patterns:\n- Conflicting constraints: high yield AND AAA-only AND < 2-year maturity\n- Liquidity vs yield trade-offs\n- Tax constraints (tax-free vs taxable)\n- Lot-size / ticket size constraints (e.g., minimum 10 lakh)\n- Very specific sectors/issuers, concentration concerns, ESG filters\n\"\"\"\n        if intent_name == \"sell_recommendation\":\n            return \"\"\"\nEdge-case patterns:\n- Deciding whether to exit after downgrade, spread widening, or macro scare\n- Partial exit vs full exit decisions\n- Tax-loss harvesting reasons\n- Multiple bonds in question, but primary decision around 1-2 names\n\"\"\"\n        if intent_name == \"portfolio_analysis\":\n            return \"\"\"\nEdge-case patterns:\n- Ugly portfolios: concentrated in 1–2 issuers, weird duration profiles\n- Mismatch between stated risk tolerance and current holdings\n- Asking for scenario analysis (rate hikes/cuts, spread shocks)\n- Holdings described in text, not in perfect tabular form\n\"\"\"\n        if intent_name == \"reduce_duration\":\n            return \"\"\"\nEdge-case patterns:\n- Need to cut duration without sacrificing yield too much\n- Want to maintain sector or credit quality while reducing duration\n- Already barbelled portfolios that need duration trimming on one side\n\"\"\"\n        if intent_name == \"increase_yield\":\n            return \"\"\"\nEdge-case patterns:\n- User chasing yield but with specific max drawdown or rating floor\n- Asking about callable/perpetual structures vs plain vanilla\n- Tension between liquidity and higher yield\n\"\"\"\n        if intent_name == \"hedge_volatility\":\n            return \"\"\"\nEdge-case patterns:\n- Hedge using barbell, cash, or inverse interest-rate exposures\n- Portfolio with large MTM swings after policy shocks\n- Desire to hedge only a sub-portfolio or certain maturity bucket\n\"\"\"\n        if intent_name == \"sector_rebalance\":\n            return \"\"\"\nEdge-case patterns:\n- Overweight PSU Energy, NBFC, or one corporate group\n- Need to bring sectors within target bands\n- Sector views changing due to regulation, ESG, or macro themes\n\"\"\"\n        if intent_name == \"barbell_strategy\":\n            return \"\"\"\nEdge-case patterns:\n- Specific short/long buckets (e.g., <1 year and >10 year)\n- Target average duration & yield for the overall barbell\n- Barbell plus constraint on issuer or sector concentration\n\"\"\"\n        if intent_name == \"switch_bonds\":\n            return \"\"\"\nEdge-case patterns:\n- Switch from downgraded or illiquid bonds into better quality/liquidity\n- Same issuer switch (old series to new series)\n- Switch to capture curve rolldown or spread compression\n\"\"\"\n        if intent_name == \"explain_recommendation\":\n            return \"\"\"\nEdge-case patterns:\n- User quoting a past recommendation and asking 'why'\n- Wanting explanation in simple language for senior management / board\n- Disagreeing with the recommendation and asking for justification\n\"\"\"\n        if intent_name == \"market_outlook\":\n            return \"\"\"\nEdge-case patterns:\n- Multi-scenario outlooks: soft landing, hard landing, stagflation\n- Views on specific parts of the curve (2Y vs 10Y)\n- Outlook conditional on upcoming policy events or inflation prints\n\"\"\"\n        if intent_name == \"credit_analysis\":\n            return \"\"\"\nEdge-case patterns:\n- Complex capital structures (AT1, perpetuals, sub debt)\n- Parent-subsidiary support, implicit guarantees\n- Early warning signals, covenant breaches, rating watch\n\"\"\"\n        if intent_name == \"forecast_prices\":\n            return \"\"\"\nEdge-case patterns:\n- Explicit price/return forecast over 3–12 months\n- Total return including carry and rolldown, not just price moves\n- Asking for forecast under 2–3 different rate paths\n\"\"\"\n        return \"\"\n\n    # ---------------- GENERATION LOOPS ----------------\n\n    def _generate_for_intent(self, intent_name: str, total_needed: int) -> List[Dict[str, Any]]:\n        samples: List[Dict[str, Any]] = []\n        remaining = total_needed\n\n        pbar = tqdm(total=total_needed, desc=f\"Generating {intent_name}\")\n        while remaining > 0:\n            batch_size = min(self.max_per_call, remaining)\n            batch = self._generate_batch_for_intent(intent_name, batch_size)\n            samples.extend(batch)\n            got = len(batch)\n            remaining -= got\n            pbar.update(got)\n        pbar.close()\n\n        print(f\"  ✓ Collected {len(samples)} samples for {intent_name}\")\n        return samples\n\n    def _generate_batch_for_intent(self, intent_name: str, batch_size: int) -> List[Dict[str, Any]]:\n        base_desc = self.intent_descriptions[intent_name]\n        edge_hint = self._edge_case_hint_block(intent_name)\n\n        system_msg = (\n            \"You are generating synthetic TRAINING DATA for a supervised bond-intent classifier.\\n\"\n            \"You MUST produce realistic, messy, institutional-grade bond queries.\\n\"\n            \"The examples should be HARDER / EDGE-CASE variants, not simple textbook prompts.\"\n        )\n\n        user_msg = f\"\"\"\nGenerate {batch_size} diverse user queries whose PRIMARY intent is exactly: \"{intent_name}\".\n\nIntent semantics:\n{base_desc}\n\nEdge-case guidance:\n{edge_hint}\n\nGlobal rules:\n- The queries MUST BE about bonds / fixed-income investing, portfolios, or markets.\n- Do NOT generate generic equity-only or generic personal-finance prompts here.\n- Explicitly use bond jargon: spreads, duration, YTM, callable, perpetual, roll-down, MTM, etc. where natural.\n- Vary:\n  * investor type (retail, HNI, treasury, CIO, risk officer, family office),\n  * tone (short commands, long descriptive questions, follow-ups),\n  * complexity (single objective vs multi-constraint trade-offs).\n\nLabeling rules:\n- The PRIMARY task must match the intent \"{intent_name}\".\n- Even if the query touches multiple ideas, decide the dominant intention.\n\nOutput FORMAT:\nReturn the output STRICTLY in JSON Lines format:\n- One JSON object per line\n- No surrounding array\n- No commentary, no backticks\n\nEach JSON object MUST have exactly these keys:\n- \"text\": string, the user query\n- \"intent\": string, MUST be exactly \"{intent_name}\"\n- \"sectors\": array of strings, each must be one of:\n  [\"Sovereign\", \"PSU Energy\", \"Financial\", \"Corporate\", \"Infrastructure\", \"NBFC\", \"Banking\"]\n  (can be empty if sectors are not clearly specified)\n- \"rating\": null or one of [\"AAA\",\"AA+\",\"AA\",\"A+\",\"A\",\"BBB\"]\n- \"duration\": string, one of [\"short\",\"medium\",\"long\"]\n- \"constraints\": object with EXACTLY these boolean fields:\n  {{\n    \"preserve_yield\": <true/false>,\n    \"maintain_liquidity\": <true/false>,\n    \"avoid_downgrades\": <true/false>,\n    \"sector_diversity\": <true/false>,\n    \"rating_above_aa\": <true/false>\n  }}\n\nConstraints defaults:\n- If unsure, use null for \"rating\".\n- If not mentioned, use \"medium\" for duration.\n- All constraint flags default to false unless explicitly implied.\n\"\"\"\n\n        full_prompt = system_msg + \"\\n\\n\" + user_msg\n\n        resp = self.client.models.generate_content(\n            model=self.model_name,\n            contents=full_prompt,\n        )\n\n        raw = resp.text or \"\"\n        return self._parse_jsonl(raw, intent_name)\n\n    # ---------------- PARSING & NORMALIZATION ----------------\n\n    def _parse_jsonl(self, raw: str, intent_name: str) -> List[Dict[str, Any]]:\n        rows: List[Dict[str, Any]] = []\n        for line in raw.splitlines():\n            line = line.strip()\n            if not line or line.startswith(\"```\"):\n                continue\n            if line.startswith(\"-\"):\n                line = line.lstrip(\"-\").strip()\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            sample = self._normalize_record(obj, intent_name)\n            if sample is not None:\n                rows.append(sample)\n        return rows\n\n    def _normalize_record(self, obj: Dict[str, Any], intent_name: str) -> Optional[Dict[str, Any]]:\n        text = str(obj.get(\"text\", \"\")).strip()\n        if not text:\n            return None\n\n        # Force canonical intent\n        intent = obj.get(\"intent\", intent_name)\n        if intent != intent_name:\n            intent = intent_name\n\n        # Sectors\n        sectors_raw = obj.get(\"sectors\") or []\n        sectors: List[str] = []\n        if isinstance(sectors_raw, list):\n            for s in sectors_raw:\n                ss = str(s).strip()\n                if ss in self.allowed_sectors and ss not in sectors:\n                    sectors.append(ss)\n\n        # Rating\n        rating_raw = obj.get(\"rating\")\n        if rating_raw is None:\n            rating_norm = None\n        else:\n            r = str(rating_raw).strip()\n            rating_norm = r if r in self.allowed_ratings else None\n\n        # Duration\n        duration_raw = str(obj.get(\"duration\", \"medium\")).strip().lower()\n        duration = duration_raw if duration_raw in self.allowed_durations else \"medium\"\n\n        # Constraints\n        cons_raw = obj.get(\"constraints\") or {}\n        constraints = {\n            \"preserve_yield\": bool(cons_raw.get(\"preserve_yield\", False)),\n            \"maintain_liquidity\": bool(cons_raw.get(\"maintain_liquidity\", False)),\n            \"avoid_downgrades\": bool(cons_raw.get(\"avoid_downgrades\", False)),\n            \"sector_diversity\": bool(cons_raw.get(\"sector_diversity\", False)),\n            \"rating_above_aa\": bool(cons_raw.get(\"rating_above_aa\", False)),\n        }\n\n        # If rating is AAA/AA+/AA, it's reasonable to flip rating_above_aa\n        if rating_norm in (\"AAA\", \"AA+\", \"AA\"):\n            constraints[\"rating_above_aa\"] = True\n\n        return {\n            \"text\": text,\n            \"intent\": intent,\n            \"sectors\": sectors,\n            \"rating\": rating_norm,\n            \"duration\": duration,\n            \"constraints\": constraints,\n        }\n\n\n# ============================================================\n# RUN GENERATION + SAVE BOND EDGE DATA\n# ============================================================\n\nbond_edge_gen = BondEdgeLLMDataGenerator(\n    model_name=BOND_EDGE_CONFIG[\"model\"],\n    samples_per_intent=BOND_EDGE_CONFIG[\"samples_per_intent\"],\n    max_per_call=BOND_EDGE_CONFIG[\"max_per_call\"],\n)\n\nbond_edge_data = bond_edge_gen.generate_dataset()\n\nprint(\"Intent counts in new bond edge dataset:\")\nprint(Counter(row[\"intent\"] for row in bond_edge_data))\n\nout_path_bond = BOND_EDGE_CONFIG[\"output_path\"]\nout_path_bond.parent.mkdir(parents=True, exist_ok=True)\n\nwith out_path_bond.open(\"w\", encoding=\"utf-8\") as f:\n    for row in bond_edge_data:\n        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n\nprint(f\"\\n✓ Saved extra bond edge data to: {out_path_bond}\")\nprint(f\"  Total rows: {len(bond_edge_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T15:10:03.790020Z","iopub.execute_input":"2025-11-30T15:10:03.790590Z","iopub.status.idle":"2025-11-30T15:31:11.790176Z","shell.execute_reply.started":"2025-11-30T15:10:03.790564Z","shell.execute_reply":"2025-11-30T15:31:11.789325Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGENERATING EXTRA BOND EDGE-CASE DATA (Gemini)\n============================================================\n\n→ Intent: buy_recommendation (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating buy_recommendation:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a332717818714b53963ee5ed24435521"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 182 samples for buy_recommendation\n\n→ Intent: sell_recommendation (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating sell_recommendation:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73e546f414a4105b2105856b3c6db3b"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for sell_recommendation\n\n→ Intent: portfolio_analysis (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating portfolio_analysis:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d5d130282734f4c83908346f4782b3b"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 181 samples for portfolio_analysis\n\n→ Intent: reduce_duration (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating reduce_duration:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f02d62df3510410dae58b82c8750e81d"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for reduce_duration\n\n→ Intent: increase_yield (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating increase_yield:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b6768474d5425b8e5b2fa9f55977af"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for increase_yield\n\n→ Intent: hedge_volatility (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating hedge_volatility:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"308cb0e0f9db4bd687c417e2b5b6fad7"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for hedge_volatility\n\n→ Intent: sector_rebalance (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating sector_rebalance:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ec0746bd9d4d5b9128e9f4e781d8c8"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for sector_rebalance\n\n→ Intent: barbell_strategy (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating barbell_strategy:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6674a509325a43b3b0638f0a404ddddc"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for barbell_strategy\n\n→ Intent: switch_bonds (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating switch_bonds:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6df8bc71d84a63ad1ee6c8cac47d25"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for switch_bonds\n\n→ Intent: explain_recommendation (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating explain_recommendation:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247637c7409b4cb492cb2bdc7ff3b63d"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for explain_recommendation\n\n→ Intent: market_outlook (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating market_outlook:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38583659bf874d1aabb042521a26a155"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for market_outlook\n\n→ Intent: credit_analysis (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating credit_analysis:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c629f675052f4dc1a4d6633d3e45e38c"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for credit_analysis\n\n→ Intent: forecast_prices (target 180)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating forecast_prices:   0%|          | 0/180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f027e365a747451e8dca6df925f6dff1"}},"metadata":{}},{"name":"stdout","text":"  ✓ Collected 180 samples for forecast_prices\n\n✓ Generated 2343 extra bond edge-case samples\n\nIntent counts in new bond edge dataset:\nCounter({'buy_recommendation': 182, 'portfolio_analysis': 181, 'sell_recommendation': 180, 'reduce_duration': 180, 'increase_yield': 180, 'hedge_volatility': 180, 'sector_rebalance': 180, 'barbell_strategy': 180, 'switch_bonds': 180, 'explain_recommendation': 180, 'market_outlook': 180, 'credit_analysis': 180, 'forecast_prices': 180})\n\n✓ Saved extra bond edge data to: /kaggle/working/bond_edge_cases.jsonl\n  Total rows: 2343\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Tuple\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\n\n# ============================================================\n# PATHS: ALL 6 FILES\n# ============================================================\n\nBASE_DATA_DIR = Path(\"/kaggle/input/bonds-query-classifier-finetuning-slm-dataset\")\n\nTRAIN_EXT = BASE_DATA_DIR / \"train_extended.jsonl\"\nVAL_EXT   = BASE_DATA_DIR / \"val_extended.jsonl\"\nTEST_EXT  = BASE_DATA_DIR / \"test_extended.jsonl\"\n\n# Adjust these if your earlier generators used different paths\nEQUITY_EDGE_PATH       = Path(\"/kaggle/working/equity_edge_cases.jsonl\")\nGENERAL_NONBOND_PATH   = Path(\"/kaggle/working/general_non_bond_edge_cases.jsonl\")\nBOND_EDGE_PATH         = Path(\"/kaggle/working/bond_edge_cases.jsonl\")\n\nFINAL_OUT_DIR = Path(\"/kaggle/working/final_intent_dataset\")\nFINAL_OUT_DIR.mkdir(parents=True, exist_ok=True)\n\n\ndef load_jsonl(path: Path) -> List[Dict[str, Any]]:\n    data: List[Dict[str, Any]] = []\n    if not path.exists():\n        print(f\"⚠ File not found: {path}\")\n        return data\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                data.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return data\n\n\ndef save_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\n\n# ============================================================\n# LOAD ALL 6 DATASETS\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"LOADING ALL 6 DATA FILES\")\nprint(\"=\" * 60)\n\ntrain_base = load_jsonl(TRAIN_EXT)\nval_base   = load_jsonl(VAL_EXT)\ntest_base  = load_jsonl(TEST_EXT)\n\nequity_edge      = load_jsonl(EQUITY_EDGE_PATH)\ngeneral_nonbond  = load_jsonl(GENERAL_NONBOND_PATH)\nbond_edge        = load_jsonl(BOND_EDGE_PATH)\n\nprint(f\"train_extended: {len(train_base)}\")\nprint(f\"val_extended:   {len(val_base)}\")\nprint(f\"test_extended:  {len(test_base)}\")\nprint(f\"equity_edge:    {len(equity_edge)}\")\nprint(f\"general_nonbond_edge: {len(general_nonbond)}\")\nprint(f\"bond_edge:      {len(bond_edge)}\")\n\n# Combine everything\nall_data_raw = train_base + val_base + test_base + equity_edge + general_nonbond + bond_edge\nprint(f\"\\nCombined raw size (before dedupe): {len(all_data_raw)}\")\n\n\n# ============================================================\n# DEDUPLICATE BY (text, intent)\n# ============================================================\n\nseen_keys = set()\nall_data: List[Dict[str, Any]] = []\nfor row in all_data_raw:\n    text = str(row.get(\"text\", \"\")).strip()\n    intent = row.get(\"intent\")\n    if not text or intent is None:\n        continue\n    key = (text, intent)\n    if key in seen_keys:\n        continue\n    seen_keys.add(key)\n    all_data.append(row)\n\nprint(f\"✓ After dedupe: {len(all_data)} rows\")\n\n\n# Quick sanity: intent distribution\nintent_counts = Counter(r[\"intent\"] for r in all_data if \"intent\" in r)\nprint(\"\\nIntent distribution (all_data):\")\nfor k, v in sorted(intent_counts.items()):\n    print(f\"  {k:20s} : {v}\")\n\n\n# ============================================================\n# FINAL STRATIFIED SPLIT (train / val / test)\n# ============================================================\n\nTRAIN_FRAC = 0.7\nVAL_FRAC   = 0.15  # test = 0.15\n\nintents = [r[\"intent\"] for r in all_data]\n\ntrain_all, test_final = train_test_split(\n    all_data,\n    test_size=1.0 - (TRAIN_FRAC + VAL_FRAC),\n    stratify=intents,\n    random_state=42,\n)\n\nintents_train_all = [r[\"intent\"] for r in train_all]\nval_size = VAL_FRAC / (TRAIN_FRAC + VAL_FRAC)\n\ntrain_final, val_final = train_test_split(\n    train_all,\n    test_size=val_size,\n    stratify=intents_train_all,\n    random_state=42,\n)\n\nprint(\"\\nFinal split sizes:\")\nprint(f\"  train_final: {len(train_final)}\")\nprint(f\"  val_final:   {len(val_final)}\")\nprint(f\"  test_final:  {len(test_final)}\")\n\n\n# Save\ntrain_out_path = FINAL_OUT_DIR / \"train_final.jsonl\"\nval_out_path   = FINAL_OUT_DIR / \"val_final.jsonl\"\ntest_out_path  = FINAL_OUT_DIR / \"test_final.jsonl\"\n\nsave_jsonl(train_out_path, train_final)\nsave_jsonl(val_out_path,   val_final)\nsave_jsonl(test_out_path,  test_final)\n\nprint(\"\\n✓ Saved final combined splits to:\")\nprint(f\"  {train_out_path}\")\nprint(f\"  {val_out_path}\")\nprint(f\"  {test_out_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T15:32:43.892501Z","iopub.execute_input":"2025-11-30T15:32:43.892807Z","iopub.status.idle":"2025-11-30T15:32:44.093576Z","shell.execute_reply.started":"2025-11-30T15:32:43.892784Z","shell.execute_reply":"2025-11-30T15:32:44.092827Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING ALL 6 DATA FILES\n============================================================\ntrain_extended: 5749\nval_extended:   1232\ntest_extended:  1232\nequity_edge:    409\ngeneral_nonbond_edge: 1000\nbond_edge:      2343\n\nCombined raw size (before dedupe): 11965\n✓ After dedupe: 10243 rows\n\nIntent distribution (all_data):\n  barbell_strategy     : 552\n  buy_recommendation   : 1021\n  credit_analysis      : 563\n  explain_recommendation : 503\n  forecast_prices      : 638\n  hedge_volatility     : 529\n  increase_yield       : 760\n  market_outlook       : 571\n  non_bond_llm         : 1017\n  non_bond_search      : 1102\n  portfolio_analysis   : 535\n  reduce_duration      : 578\n  sector_rebalance     : 721\n  sell_recommendation  : 564\n  switch_bonds         : 589\n\nFinal split sizes:\n  train_final: 7169\n  val_final:   1537\n  test_final:  1537\n\n✓ Saved final combined splits to:\n  /kaggle/working/final_intent_dataset/train_final.jsonl\n  /kaggle/working/final_intent_dataset/val_final.jsonl\n  /kaggle/working/final_intent_dataset/test_final.jsonl\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"\"\"\"\nBond Query Classifier - Training on FINAL master dataset (bond + non-bond + edge cases)\n=======================================================================================\n\nAssumes these files exist:\n\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/train_final.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/val_final.jsonl\n/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/test_final.jsonl\n\"\"\"\n\n# ==================== INSTALL DEPENDENCIES ====================\nimport sys\nimport subprocess\n\nprint(\"=\" * 60)\nprint(\"INSTALLING DEPENDENCIES\")\nprint(\"=\" * 60)\n\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"transformers\", \"accelerate\", \"scikit-learn\"\n])\n\nprint(\"✓ Dependencies installed!\\n\")\n\n\n# ==================== IMPORTS ====================\nimport os\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    get_cosine_schedule_with_warmup\n)\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n\n# ==================== GPU CHECK ====================\nprint(\"=\" * 60)\nprint(\"GPU CHECK\")\nprint(\"=\" * 60)\n\nif torch.cuda.is_available():\n    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n    print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"✓ CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"⚠ WARNING: No GPU detected! Training will be very slow.\")\n\nprint()\n\n\n# ==================== CONFIGURATION ====================\nCONFIG = {\n    'base_model': 'microsoft/deberta-v3-small',\n\n    # ---- FINAL dataset paths ----\n    'dataset_dir': '/kaggle/input/bonds-query-classifier-finetuning-slm-dataset',\n    'train_file': 'train_final.jsonl',\n    'val_file':   'val_final.jsonl',\n    'test_file':  'test_final.jsonl',\n\n    'batch_size': 32,\n    'num_epochs': 10,\n    'learning_rate': 2e-5,\n    'warmup_ratio': 0.1,\n    'max_length': 128,\n    'output_dir': '/kaggle/working/bond_classifier_v3_final',\n    'seed': 42,\n}\n\n# Set seeds\nrandom.seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\ntorch.manual_seed(CONFIG['seed'])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CONFIG['seed'])\n\n\n# ==================== PYTORCH DATASET (15 INTENTS) ====================\n\nclass BondQueryDataset(Dataset):\n    \"\"\"PyTorch Dataset (bond + non-bond router)\"\"\"\n    \n    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # 13 original intents + 2 non-bond router intents\n        self.intent_to_id = {\n            'buy_recommendation': 0,\n            'sell_recommendation': 1,\n            'portfolio_analysis': 2,\n            'reduce_duration': 3,\n            'increase_yield': 4,\n            'hedge_volatility': 5,\n            'sector_rebalance': 6,\n            'barbell_strategy': 7,\n            'switch_bonds': 8,\n            'explain_recommendation': 9,\n            'market_outlook': 10,\n            'credit_analysis': 11,\n            'forecast_prices': 12,\n            'non_bond_search': 13,\n            'non_bond_llm': 14,\n        }\n        \n        self.sector_to_id = {\n            'Sovereign': 0, 'PSU Energy': 1, 'Financial': 2,\n            'Corporate': 3, 'Infrastructure': 4, 'NBFC': 5, 'Banking': 6\n        }\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        encoding = self.tokenizer(\n            sample['text'],\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        # ---- intent ----\n        intent_label = self.intent_to_id[sample['intent']]\n        \n        # ---- sectors (multi-label) ----\n        sector_labels = torch.zeros(len(self.sector_to_id))\n        for sector in sample.get('sectors', []):\n            if sector in self.sector_to_id:\n                sector_labels[self.sector_to_id[sector]] = 1\n        \n        # ---- rating ----\n        rating = sample.get('rating')\n        rating_map = {'AAA': 0, 'AA+': 1, 'AA': 2, 'A+': 3, 'A': 4, 'BBB': 5}\n        rating_label = rating_map.get(rating, 6)   # 6 = \"none / other\"\n        \n        # ---- duration ----\n        duration_map = {'short': 0, 'medium': 1, 'long': 2}\n        duration_label = duration_map.get(sample.get('duration', 'medium'), 1)\n        \n        # ---- constraints (5 binary flags) ----\n        constraints = sample.get('constraints', {})\n        constraint_labels = torch.tensor([\n            float(constraints.get('preserve_yield', False)),\n            float(constraints.get('maintain_liquidity', False)),\n            float(constraints.get('avoid_downgrades', False)),\n            float(constraints.get('sector_diversity', False)),\n            float(constraints.get('rating_above_aa', False))\n        ])\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'intent_label': torch.tensor(intent_label),\n            'sector_labels': sector_labels,\n            'rating_label': torch.tensor(rating_label),\n            'duration_label': torch.tensor(duration_label),\n            'constraint_labels': constraint_labels\n        }\n\n\n# ==================== MODEL (SAME ARCH, 15-CLASS INTENT HEAD) ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"Multi-task classifier: intent + sectors + rating + duration + constraints\"\"\"\n    \n    def __init__(self, base_model: str = 'microsoft/deberta-v3-small', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # 15 intent classes (13 bond + 2 non-bond)\n        self.intent_head = nn.Linear(feature_size, 15)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== LOSS ====================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n\n\nclass MultiTaskLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.intent_loss_fn = FocalLoss(gamma=2.0)\n        self.sector_loss_fn = nn.BCEWithLogitsLoss()\n        self.rating_loss_fn = nn.CrossEntropyLoss()\n        self.duration_loss_fn = nn.CrossEntropyLoss()\n        self.constraint_loss_fn = nn.BCEWithLogitsLoss()\n    \n    def forward(self, outputs, labels):\n        intent_loss = self.intent_loss_fn(outputs['intent_logits'], labels['intent_label'])\n        sector_loss = self.sector_loss_fn(outputs['sector_logits'], labels['sector_labels'])\n        rating_loss = self.rating_loss_fn(outputs['rating_logits'], labels['rating_label'])\n        duration_loss = self.duration_loss_fn(outputs['duration_logits'], labels['duration_label'])\n        constraint_loss = self.constraint_loss_fn(outputs['constraint_logits'], labels['constraint_labels'])\n        \n        total = intent_loss + 0.5 * sector_loss + 0.3 * rating_loss + 0.3 * duration_loss + 0.4 * constraint_loss\n        \n        return {'total': total, 'intent': intent_loss}\n\n\n# ==================== TRAINER ====================\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, criterion, device, output_dir):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.best_acc = 0.0\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            \n            labels = {k: v.to(self.device) for k, v in batch.items()\n                      if k not in ['input_ids', 'attention_mask']}\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(input_ids, attention_mask)\n            loss_dict = self.criterion(outputs, labels)\n            loss = loss_dict['total']\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            \n            total_loss += loss.item()\n            preds = outputs['intent_logits'].argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels['intent_label'].cpu().numpy())\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{accuracy_score(all_labels[-len(preds):], preds):.3f}'\n            })\n        \n        return total_loss / len(self.train_loader)\n    \n    def evaluate(self):\n        self.model.eval()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_loader, desc='Evaluating'):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = {k: v.to(self.device) for k, v in batch.items()\n                          if k not in ['input_ids', 'attention_mask']}\n                \n                outputs = self.model(input_ids, attention_mask)\n                loss_dict = self.criterion(outputs, labels)\n                total_loss += loss_dict['total'].item()\n                \n                preds = outputs['intent_logits'].argmax(dim=-1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels['intent_label'].cpu().numpy())\n        \n        return {\n            'loss': total_loss / len(self.val_loader),\n            'accuracy': accuracy_score(all_labels, all_preds),\n            'f1_macro': f1_score(all_labels, all_preds, average='macro')\n        }\n    \n    def save_checkpoint(self, epoch, metrics):\n        if metrics['accuracy'] > self.best_acc:\n            self.best_acc = metrics['accuracy']\n            torch.save(self.model.state_dict(), self.output_dir / 'pytorch_model.bin')\n            print(f\"✓ New best model saved (acc: {metrics['accuracy']:.4f})\")\n    \n    def train(self, num_epochs):\n        print(\"=\" * 60)\n        print(\"TRAINING\")\n        print(\"=\" * 60)\n        \n        for epoch in range(1, num_epochs + 1):\n            train_loss = self.train_epoch(epoch)\n            print(f\"\\nEpoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n            \n            val_metrics = self.evaluate()\n            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Val F1: {val_metrics['f1_macro']:.4f}\\n\")\n            \n            self.save_checkpoint(epoch, val_metrics)\n        \n        print(f\"Training complete! Best accuracy: {self.best_acc:.4f}\\n\")\n\n\n# ==================== JSONL LOADER + OPTIONAL CLEANING ====================\n\ndef load_jsonl(path: str) -> List[Dict[str, Any]]:\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                rows.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return rows\n\n\ndef clean_nonbond_rows(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Optional tiny cleanup:\n    - Drop non_bond_* rows whose text still clearly mentions bonds/g-secs.\n    If you don't want this, just return rows unchanged.\n    \"\"\"\n    cleaned = []\n    dropped = 0\n    bondish = (\"bond\", \"bonds\", \"g-sec\", \"gsec\", \"debenture\", \"coupon\", \"yield\")\n    for r in rows:\n        intent = r.get(\"intent\", \"\")\n        text = (r.get(\"text\") or \"\").lower()\n        if intent in (\"non_bond_search\", \"non_bond_llm\") and any(k in text for k in bondish):\n            dropped += 1\n            continue\n        cleaned.append(r)\n    if dropped > 0:\n        print(f\"  ➜ Dropped {dropped} noisy non-bond rows mentioning bonds\")\n    return cleaned\n\n\n# ==================== MAIN TRAINING FUNCTION ====================\n\ndef train_model():\n    \"\"\"Train on FINAL master dataset (bond + non-bond + edge cases).\"\"\"\n    \n    dataset_dir = CONFIG['dataset_dir']\n    train_path = os.path.join(dataset_dir, CONFIG['train_file'])\n    val_path   = os.path.join(dataset_dir, CONFIG['val_file'])\n    test_path  = os.path.join(dataset_dir, CONFIG['test_file'])\n\n    print(\"=\" * 60)\n    print(\"LOADING FINAL DATASET\")\n    print(\"=\" * 60)\n    train_data = load_jsonl(train_path)\n    val_data   = load_jsonl(val_path)\n    test_data  = load_jsonl(test_path)\n\n    print(f\"Raw Train rows: {len(train_data)}\")\n    print(f\"Raw Val rows:   {len(val_data)}\")\n    print(f\"Raw Test rows:  {len(test_data)}\")\n\n    # --- Optional: small clean-up for non-bond rows ---\n    train_data = clean_nonbond_rows(train_data)\n    val_data   = clean_nonbond_rows(val_data)\n    test_data  = clean_nonbond_rows(test_data)\n\n    print(f\"Clean Train rows: {len(train_data)}\")\n    print(f\"Clean Val rows:   {len(val_data)}\")\n    print(f\"Clean Test rows:  {len(test_data)}\\n\")\n\n    # ---- Tokenizer & Datasets ----\n    print(\"=\" * 60)\n    print(\"LOADING MODEL & TOKENIZER\")\n    print(\"=\" * 60)\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG['base_model'])\n    \n    train_dataset = BondQueryDataset(train_data, tokenizer, CONFIG['max_length'])\n    val_dataset   = BondQueryDataset(val_data,   tokenizer, CONFIG['max_length'])\n    test_dataset  = BondQueryDataset(test_data,  tokenizer, CONFIG['max_length'])\n    \n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    val_loader   = DataLoader(val_dataset,   batch_size=CONFIG['batch_size'])\n    test_loader  = DataLoader(test_dataset,  batch_size=CONFIG['batch_size'])\n    \n    # ---- Model ----\n    model = ProductionBondClassifier(CONFIG['base_model'])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    print(f\"✓ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n    print(f\"✓ Device: {device}\\n\")\n    \n    # ---- Optimizer & Scheduler ----\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n    num_training_steps = len(train_loader) * CONFIG['num_epochs']\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * CONFIG['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    criterion = MultiTaskLoss()\n    \n    # ---- Train ----\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        criterion=criterion,\n        device=device,\n        output_dir=CONFIG['output_dir'],\n    )\n    trainer.train(CONFIG['num_epochs'])\n    \n    # ---- Save tokenizer ----\n    tokenizer.save_pretrained(CONFIG['output_dir'])\n    \n    # ---- Final test ----\n    print(\"=\" * 60)\n    print(\"FINAL TEST\")\n    print(\"=\" * 60)\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc='Testing'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask)\n            preds = outputs['intent_logits'].argmax(dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['intent_label'].numpy())\n    \n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    print(f\"\\n✓ Test Accuracy: {test_acc:.4f}\")\n    print(f\"✓ Test F1 Macro: {test_f1:.4f}\\n\")\n    \n    print(\"=\" * 60)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"\\n✓ Model saved to: {CONFIG['output_dir']}\")\n    print(f\"✓ Files: pytorch_model.bin, tokenizer files\")\n    print(\"\\nTo download:\")\n    print(\"  1. Go to Output tab\")\n    print(\"  2. Download 'bond_classifier_v3_final' folder\")\n    print(\"  3. Use locally with your inference notebook\")\n\n\n# ==================== RUN ====================\n\nif __name__ == '__main__':\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:29:48.670777Z","iopub.execute_input":"2025-11-30T16:29:48.671078Z","iopub.status.idle":"2025-11-30T16:50:43.901461Z","shell.execute_reply.started":"2025-11-30T16:29:48.671056Z","shell.execute_reply":"2025-11-30T16:50:43.900818Z"}},"outputs":[{"name":"stdout","text":"============================================================\nINSTALLING DEPENDENCIES\n============================================================\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 113.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 91.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 45.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 8.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 33.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 14.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 8.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 87.6 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"✓ Dependencies installed!\n\n============================================================\nGPU CHECK\n============================================================\n✓ GPU detected: Tesla T4\n✓ GPU memory: 15.83 GB\n✓ CUDA version: 12.4\n\n============================================================\nLOADING FINAL DATASET\n============================================================\nRaw Train rows: 7169\nRaw Val rows:   1537\nRaw Test rows:  1537\n  ➜ Dropped 67 noisy non-bond rows mentioning bonds\n  ➜ Dropped 12 noisy non-bond rows mentioning bonds\n  ➜ Dropped 16 noisy non-bond rows mentioning bonds\nClean Train rows: 7102\nClean Val rows:   1525\nClean Test rows:  1521\n\n============================================================\nLOADING MODEL & TOKENIZER\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2274eb632a6408cb6a12753457f56a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46e1b104f7146788d223847c2243f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e89891072e943bc8ccd6c18755dde48"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-30 16:31:18.856630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764520279.062789      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764520279.127261      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea44770c9ce4282868717e2aa39cd49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e854c43c584657b2ffeef5d68a3f99"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded: 142,206,757 parameters\n✓ Device: cuda\n\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b959be36e84e758975989525b7f188"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/10 - Train Loss: 3.2768\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0b0f42249a4584905cbd4ccd6bde04"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 1.4890\nVal Accuracy: 0.7508\nVal F1: 0.7226\n\n✓ New best model saved (acc: 0.7508)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e39a68673b742d0990cbe46d9cdc5d2"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2/10 - Train Loss: 1.1423\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0064750c7c45c4a7464298e1bf0c3f"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.7941\nVal Accuracy: 0.9390\nVal F1: 0.9477\n\n✓ New best model saved (acc: 0.9390)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f097038b3dfb44738dd2642c8f7ae734"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3/10 - Train Loss: 0.7708\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81df9e79df9d4887bbcac57a1f3217ee"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.6658\nVal Accuracy: 0.9430\nVal F1: 0.9500\n\n✓ New best model saved (acc: 0.9430)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd3d10f05ee45a681e6ce29fe16f1b0"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4/10 - Train Loss: 0.6376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd6916b20cc4bf3b7b5aeb1317af13d"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5868\nVal Accuracy: 0.9554\nVal F1: 0.9611\n\n✓ New best model saved (acc: 0.9554)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af3845ea48343ff9c49e508828ebb5e"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 5/10 - Train Loss: 0.5607\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ad9babee8942b185860a6e5176e658"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5520\nVal Accuracy: 0.9489\nVal F1: 0.9562\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1610b916ad22453a9c3a508a3da55582"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 6/10 - Train Loss: 0.5147\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d80febabc848fba20b6004ff446d3e"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5266\nVal Accuracy: 0.9600\nVal F1: 0.9652\n\n✓ New best model saved (acc: 0.9600)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af81266a79de4f3aa0e71b4d75981ba3"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 7/10 - Train Loss: 0.4842\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e98316e21c141a8b7993a87d3a28719"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5111\nVal Accuracy: 0.9554\nVal F1: 0.9615\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b90c2aadc949a98bdfc5a9a8ed3628"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 8/10 - Train Loss: 0.4671\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c99455d90fc4451dbe96a534415217c2"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.5007\nVal Accuracy: 0.9554\nVal F1: 0.9622\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b517a1a90f6a450794439807b604ab3e"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 9/10 - Train Loss: 0.4537\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd312af88f941d2ba505987381ba789"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.4958\nVal Accuracy: 0.9561\nVal F1: 0.9626\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60f840635c64dfa8f687c2bbb2f474f"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 10/10 - Train Loss: 0.4499\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96317d1c35b34c43bbc281cd663d91af"}},"metadata":{}},{"name":"stdout","text":"Val Loss: 0.4954\nVal Accuracy: 0.9541\nVal F1: 0.9609\n\nTraining complete! Best accuracy: 0.9600\n\n============================================================\nFINAL TEST\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cb9924f1cf40d49e96a422289adc9e"}},"metadata":{}},{"name":"stdout","text":"\n✓ Test Accuracy: 0.9619\n✓ Test F1 Macro: 0.9660\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n\n✓ Model saved to: /kaggle/working/bond_classifier_v3_final\n✓ Files: pytorch_model.bin, tokenizer files\n\nTo download:\n  1. Go to Output tab\n  2. Download 'bond_classifier_v3_final' folder\n  3. Use locally with your inference notebook\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==================== EVAL: BOND + NON-BOND ROUTER MODEL ====================\nimport json\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\n\n# ==================== MODEL (MUST MATCH TRAINING) ====================\n\nclass ProductionBondClassifier(nn.Module):\n    \"\"\"DeBERTa-v3-small + multi-task heads, 15 intents (13 bond + 2 non-bond).\"\"\"\n    \n    def __init__(self, base_model: str = 'microsoft/deberta-v3-small', dropout: float = 0.15):\n        super().__init__()\n        \n        self.bert = AutoModel.from_pretrained(base_model)\n        hidden_size = self.bert.config.hidden_size\n        \n        self.feature_layer = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.Dropout(dropout),\n            nn.GELU()\n        )\n        \n        feature_size = hidden_size // 2\n        \n        # 15 intents: 13 bond + 2 router\n        self.intent_head = nn.Linear(feature_size, 15)\n        self.sector_head = nn.Linear(feature_size, 7)\n        self.rating_head = nn.Linear(feature_size, 7)\n        self.duration_head = nn.Linear(feature_size, 3)\n        self.constraint_head = nn.Linear(feature_size, 5)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        features = self.feature_layer(cls_output)\n        features = self.dropout(features)\n        \n        return {\n            'intent_logits': self.intent_head(features),\n            'sector_logits': self.sector_head(features),\n            'rating_logits': self.rating_head(features),\n            'duration_logits': self.duration_head(features),\n            'constraint_logits': self.constraint_head(features),\n        }\n\n\n# ==================== CLASSIFIER WRAPPER ====================\n\nclass BondClassifier:\n    def __init__(self, model_path: str, base_model: str = \"microsoft/deberta-v3-small\"):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # ✅ Load tokenizer from base model (NOT from local dir to avoid HFValidationError)\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n        \n        # Load model + weights from local folder\n        self.model = ProductionBondClassifier(base_model=base_model)\n        state_dict = torch.load(Path(model_path) / \"pytorch_model.bin\", map_location=self.device)\n        self.model.load_state_dict(state_dict)\n        self.model.to(self.device)\n        self.model.eval()\n\n        # MUST match BondQueryDataset.intent_to_id used during training\n        self.intent_names = [\n            'buy_recommendation',     # 0\n            'sell_recommendation',    # 1\n            'portfolio_analysis',     # 2\n            'reduce_duration',        # 3\n            'increase_yield',         # 4\n            'hedge_volatility',       # 5\n            'sector_rebalance',       # 6\n            'barbell_strategy',       # 7\n            'switch_bonds',           # 8\n            'explain_recommendation', # 9\n            'market_outlook',         # 10\n            'credit_analysis',        # 11\n            'forecast_prices',        # 12\n            'non_bond_search',        # 13\n            'non_bond_llm',           # 14\n        ]\n        self.intent_to_id = {name: i for i, name in enumerate(self.intent_names)}\n    \n    def _route_from_intent(self, intent: str) -> str:\n        \"\"\"\n        Router decision:\n        - 'non_bond_search' -> 'search'\n        - 'non_bond_llm'    -> 'llm'\n        - everything else   -> 'bond'\n        \"\"\"\n        if intent == 'non_bond_search':\n            return 'search'\n        elif intent == 'non_bond_llm':\n            return 'llm'\n        else:\n            return 'bond'\n    \n    def classify(self, query: str):\n        enc = self.tokenizer(\n            query,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        input_ids = enc[\"input_ids\"].to(self.device)\n        attention_mask = enc[\"attention_mask\"].to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            intent_logits = outputs['intent_logits']\n            intent_probs = F.softmax(intent_logits, dim=-1)\n        \n        intent_idx = intent_probs.argmax(dim=-1).item()\n        intent = self.intent_names[intent_idx]\n        confidence = intent_probs.max().item()\n        route = self._route_from_intent(intent)\n        \n        return intent, confidence, route\n\n\n# ==================== JSONL LOADER ====================\n\ndef load_jsonl(path: str):\n    rows = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                rows.append(json.loads(line))\n            except json.JSONDecodeError:\n                continue\n    return rows\n\n\n# ==================== EVALUATION ON TEST JSONL ====================\n\ndef evaluate_on_jsonl(classifier: BondClassifier, jsonl_path: str):\n    data = load_jsonl(jsonl_path)\n    print(f\"Loaded {len(data)} test rows from {jsonl_path}\")\n    \n    y_true_idx = []\n    y_pred_idx = []\n    routes = []\n\n    start = time.time()\n    for row in data:\n        text = row[\"text\"]\n        true_intent_str = row[\"intent\"]\n\n        pred_intent, conf, route = classifier.classify(text)\n        routes.append(route)\n\n        # Map both true & pred to indices\n        if true_intent_str not in classifier.intent_to_id:\n            # If some weird label sneaks in, skip that row\n            continue\n        y_true_idx.append(classifier.intent_to_id[true_intent_str])\n        y_pred_idx.append(classifier.intent_to_id[pred_intent])\n    \n    elapsed = time.time() - start\n    print(f\"\\nInference time on test set: {elapsed:.2f}s \"\n          f\"({len(y_true_idx) / max(elapsed,1e-6):.1f} samples/sec)\")\n    \n    # Overall metrics\n    acc = accuracy_score(y_true_idx, y_pred_idx)\n    f1_macro = f1_score(y_true_idx, y_pred_idx, average=\"macro\")\n    print(f\"\\n=== Overall Intent Metrics ===\")\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Macro F1: {f1_macro:.4f}\")\n    \n    # Per-intent report\n    print(\"\\n=== Per-intent Classification Report ===\")\n    print(classification_report(\n        y_true_idx, \n        y_pred_idx, \n        target_names=classifier.intent_names, \n        digits=4\n    ))\n    \n    # Router stats\n    from collections import Counter\n    print(\"\\n=== Router Decision Distribution (pred side) ===\")\n    print(Counter(routes))\n\n\n# ==================== RUN EVAL + INTERACTIVE TEST ====================\n\nif __name__ == \"__main__\":\n    MODEL_PATH = \"/kaggle/working/bond_classifier_v3_final\"  # folder with pytorch_model.bin\n    TEST_JSONL = \"/kaggle/input/bonds-query-classifier-finetuning-slm-dataset/test_final.jsonl\"\n    \n    classifier = BondClassifier(MODEL_PATH)\n    \n    # 1) Full evaluation on your final test set\n    evaluate_on_jsonl(classifier, TEST_JSONL)\n    \n    # 2) Quick manual query check\n    # print(\"\\n================ Manual sanity check ================\")\n    # while True:\n    #     q = input(\"\\nEnter a query (or 'q' to quit): \").strip()\n    #     if q.lower() in {\"q\", \"quit\", \"exit\"}:\n    #         break\n    #     t0 = time.time()\n    #     intent, conf, route = classifier.classify(q)\n    #     dt = time.time() - t0\n    #     print(f\"Predicted Intent : {intent}\")\n    #     print(f\"Confidence       : {conf:.3f}\")\n    #     print(f\"Router decision  : {route}  (bond/search/llm)\")\n    #     print(f\"Latency          : {dt*1000:.1f} ms\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:01:05.734931Z","iopub.execute_input":"2025-11-30T17:01:05.735529Z","iopub.status.idle":"2025-11-30T17:04:25.877243Z","shell.execute_reply.started":"2025-11-30T17:01:05.735502Z","shell.execute_reply":"2025-11-30T17:04:25.876202Z"}},"outputs":[{"name":"stdout","text":"Loaded 1537 test rows from /kaggle/input/bonds-query-classifier-finetuning-slm-dataset/test_final.jsonl\n\nInference time on test set: 18.15s (84.7 samples/sec)\n\n=== Overall Intent Metrics ===\nAccuracy: 0.9623\nMacro F1: 0.9676\n\n=== Per-intent Classification Report ===\n                        precision    recall  f1-score   support\n\n    buy_recommendation     0.9733    0.9542    0.9637       153\n   sell_recommendation     0.9884    1.0000    0.9942        85\n    portfolio_analysis     0.9747    0.9625    0.9686        80\n       reduce_duration     0.9765    0.9540    0.9651        87\n        increase_yield     0.9652    0.9737    0.9694       114\n      hedge_volatility     0.9351    0.9114    0.9231        79\n      sector_rebalance     0.9817    0.9907    0.9862       108\n      barbell_strategy     0.9765    1.0000    0.9881        83\n          switch_bonds     0.9667    0.9886    0.9775        88\nexplain_recommendation     1.0000    1.0000    1.0000        75\n        market_outlook     0.9770    0.9884    0.9827        86\n       credit_analysis     0.9767    0.9882    0.9825        85\n       forecast_prices     1.0000    0.9896    0.9948        96\n       non_bond_search     0.9792    0.8545    0.9126       165\n          non_bond_llm     0.8506    0.9673    0.9052       153\n\n              accuracy                         0.9623      1537\n             macro avg     0.9681    0.9682    0.9676      1537\n          weighted avg     0.9640    0.9623    0.9622      1537\n\n\n=== Router Decision Distribution (pred side) ===\nCounter({'bond': 1219, 'llm': 174, 'search': 144})\n\n================ Manual sanity check ================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  Show me stocks increase\n"},{"name":"stdout","text":"Predicted Intent : non_bond_search\nConfidence       : 0.867\nRouter decision  : search  (bond/search/llm)\nLatency          : 13.8 ms\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3760539610.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n================ Manual sanity check ================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter a query (or 'q' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"while True:\n        q = input(\"\\nEnter a query (or 'q' to quit): \").strip()\n        if q.lower() in {\"q\", \"quit\", \"exit\"}:\n            break\n        t0 = time.time()\n        intent, conf, route = classifier.classify(q)\n        dt = time.time() - t0\n        print(f\"Predicted Intent : {intent}\")\n        print(f\"Confidence       : {conf:.3f}\")\n        print(f\"Router decision  : {route}  (bond/search/llm)\")\n        print(f\"Latency          : {dt*1000:.1f} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:05:55.053929Z","iopub.execute_input":"2025-11-30T17:05:55.054625Z","iopub.status.idle":"2025-11-30T17:08:05.116975Z","shell.execute_reply.started":"2025-11-30T17:05:55.054599Z","shell.execute_reply":"2025-11-30T17:08:05.116167Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  Aryan jain bonds\n"},{"name":"stdout","text":"Predicted Intent : buy_recommendation\nConfidence       : 0.668\nRouter decision  : bond  (bond/search/llm)\nLatency          : 14.4 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  Hi how are u\n"},{"name":"stdout","text":"Predicted Intent : non_bond_search\nConfidence       : 0.316\nRouter decision  : search  (bond/search/llm)\nLatency          : 12.6 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  fuck y\n"},{"name":"stdout","text":"Predicted Intent : non_bond_llm\nConfidence       : 0.456\nRouter decision  : llm  (bond/search/llm)\nLatency          : 13.5 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  what s up\n"},{"name":"stdout","text":"Predicted Intent : non_bond_search\nConfidence       : 0.410\nRouter decision  : search  (bond/search/llm)\nLatency          : 14.0 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  can you help me increase my stock\n"},{"name":"stdout","text":"Predicted Intent : increase_yield\nConfidence       : 0.620\nRouter decision  : bond  (bond/search/llm)\nLatency          : 13.0 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  can you help me increase my stocks\n"},{"name":"stdout","text":"Predicted Intent : increase_yield\nConfidence       : 0.734\nRouter decision  : bond  (bond/search/llm)\nLatency          : 13.2 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  is stock in my portfolio\n"},{"name":"stdout","text":"Predicted Intent : non_bond_search\nConfidence       : 0.681\nRouter decision  : search  (bond/search/llm)\nLatency          : 12.9 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  can i see my stocks\n"},{"name":"stdout","text":"Predicted Intent : non_bond_search\nConfidence       : 0.767\nRouter decision  : search  (bond/search/llm)\nLatency          : 12.4 ms\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a query (or 'q' to quit):  q\n"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}